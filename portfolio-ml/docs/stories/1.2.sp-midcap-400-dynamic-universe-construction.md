# <!-- Powered by BMAD™ Core -->

# Story 1.2: S&P MidCap 400 Dynamic Universe Construction

## Status
Done

## Story
**As a** quantitative researcher,  
**I want** historically accurate S&P MidCap 400 membership data from 2016-2024,  
**so that** portfolio backtests avoid survivorship bias and reflect realistic index dynamics.

## Acceptance Criteria

1. Wikipedia scraping module extracts S&P MidCap 400 historical membership changes with dates
2. Universe construction handles additions, deletions, and ticker changes over evaluation period
3. Monthly universe snapshots generated for each rebalancing date from 2016-2024
4. Survivorship bias validation confirms deceased/delisted companies included in historical periods
5. Data quality checks verify minimum 400 constituents maintained across time periods

## Tasks / Subtasks

- [x] **Task 1: Refactor Wikipedia Scraping into Modular Architecture** (AC: 1)
  - [x] Extract core logic from `scripts/build_membership_from_wikipedia.py` reference
  - [x] Implement `src/data/collectors/wikipedia.py` with Wikipedia scraping functionality
  - [x] Migrate ChangeEvent dataclass and HTML parsing logic to proper modules
  - [x] Integrate with configuration system for dynamic index selection
  - [x] Preserve existing TICKER_RE patterns and MultiIndex handling

- [x] **Task 2: Create Dynamic Universe Construction Engine** (AC: 2, 3)
  - [x] Implement `src/data/processors/universe_builder.py` based on reference scripts
  - [x] Extract membership interval logic from reference implementation
  - [x] Build monthly universe snapshots generator for rebalancing dates (2016-2024)
  - [x] Integrate with existing data configuration system
  - [x] Generate standardized `data/processed/universe_calendar.parquet` output

- [x] **Task 3: Refactor Data Collection Pipeline** (AC: 1, 2)
  - [x] Extract Stooq integration logic from `scripts/download_stooq.py` reference
  - [x] Implement `src/data/collectors/stooq.py` with proper module structure
  - [x] Extract Yahoo Finance augmentation from `scripts/augment_with_yfinance.py`
  - [x] Implement `src/data/collectors/yfinance.py` with splice-fill methodology
  - [x] Integrate all collectors with universe-aware data loading

- [x] **Task 4: Survivorship Bias Validation Framework** (AC: 4)
  - [x] Extract survivorship validation logic from reference scripts
  - [x] Implement `src/data/processors/survivorship_validator.py`
  - [x] Create delisting detection and historical accuracy validation
  - [x] Generate comprehensive survivorship bias reports
  - [x] Document anti-survivorship methodology for academic use

- [x] **Task 5: Integration and Testing Framework** (AC: 5)
  - [x] Update `src/data/loaders/` to work with dynamic universe data
  - [x] Create universe-aware parquet loading utilities
  - [x] Implement data quality validation with minimum constituent checks
  - [x] Create comprehensive unit tests covering all refactored modules
  - [x] Validate end-to-end pipeline produces identical results to reference scripts

## Dev Notes

### Previous Story Insights
From Story 1.1 completion:
- Configuration system fully functional with YAML loading and Python dataclass integration
- Repository structure complete with src/data/collectors/ and src/data/processors/ modules
- Wikipedia scraping foundation already exists in `src/data/collectors/wikipedia.py`
- Existing `scripts/build_membership_from_wikipedia.py` provides robust HTML parsing framework

### Technical Context

**Reference Implementation Analysis** [Source: scripts/build_membership_from_wikipedia.py]:
The existing script provides a complete reference implementation with:
- Robust HTML parsing using StringIO to avoid pandas deprecation warnings
- MultiIndex header handling for "Selected changes" tables
- TICKER_RE pattern for ticker extraction: `r"\(([A-Z][A-Z0-9.\-]{0,9})\)"`
- Session management with proper User-Agent headers
- ChangeEvent dataclass for tracking membership changes
- Support for both seed-current and non-seeded modes

**Data Pipeline Reference Patterns** [Source: scripts/download_stooq.py, scripts/augment_with_yfinance.py]:
The reference scripts demonstrate:
- Multi-threaded data collection with session management
- Wide parquet format for prices/volume with Date index
- Ticker normalization patterns for different data sources
- Splice-fill methodology for gap filling between data sources
- Data quality validation with minimum non-NA thresholds
- Integration between membership CSV and price panel construction

**Refactoring Requirements**:
These reference implementations need to be **extracted and refactored** into the proper modular architecture:
- Move from standalone scripts to `src/data/collectors/` and `src/data/processors/`
- Integrate with configuration system established in Story 1.1
- Follow established coding standards and module patterns
- Maintain functionality while improving maintainability and testability

**Data Architecture Requirements** [Source: docs/technical-architecture.md#target-monorepo-structure]:
```
data/
├── raw/
│   ├── membership/               # S&P MidCap 400 historical membership
│   ├── stooq/                   # Raw Stooq downloads  
│   └── yfinance/                # Raw Yahoo Finance data
├── processed/
│   ├── prices.parquet           # Aligned price panel
│   ├── volume.parquet           # Aligned volume panel
│   ├── returns_daily.parquet    # Daily returns matrix
│   └── universe_calendar.parquet # Dynamic membership calendar
```

**Universe Construction Strategy** [Source: docs/prd.md#story-1-2]:
- Wikipedia scraping extracts historical S&P MidCap 400 membership changes with dates
- Universe construction must handle additions, deletions, and ticker changes over evaluation period (2016-2024)
- Monthly universe snapshots generated for each rebalancing date
- Survivorship bias validation ensures deceased/delisted companies included in historical periods

### File Location Guidelines

**Module Structure** [Source: docs/technical-architecture.md#repository-structure-and-organization]:
- Wikipedia scraping: `src/data/collectors/wikipedia.py` (existing stub to be enhanced)
- Universe construction: `src/data/processors/universe_builder.py` (new module)
- Data loading utilities: `src/data/loaders/universe.py` (existing stub to be enhanced)
- Configuration: `configs/data/midcap400.yaml` (existing configuration)

**Data Storage Patterns** [Source: docs/technical-architecture.md#data-architecture]:
- Raw membership data: `data/raw/membership/sp400_changes.csv`
- Processed universe calendar: `data/processed/universe_calendar.parquet`
- Monthly snapshots: `data/processed/universe_snapshots/` directory

### Integration Requirements

**Configuration System Integration** [Source: Story 1.1 completion]:
The existing configuration system supports:
```python
@dataclass
class DataConfig:
    universe: str = "midcap400"
    start_date: str = "2016-01-01" 
    end_date: str = "2024-12-31"
    sources: list = None
```

**Existing Data Pipeline Integration** [Source: scripts/ingest_stooq_to_hydra.py]:
The universe construction must integrate with existing data processing:
- `src.data.processors.data_pipeline` functions for alignment and cleaning
- `src.data.processors.features` for rolling feature computation
- Parquet-based storage format for efficient loading

**Membership CSV Format** [Source: scripts/augment_with_yfinance.py]:
The system expects membership CSV files with standardized format:
```python
def _load_membership(path: str) -> List[str]:
    df = pd.read_csv(path)
    if "ticker" not in df.columns:
        raise ValueError("membership CSV must contain a 'ticker' column")
    return sorted(set(df["ticker"].astype(str).str.upper().tolist()))
```

### Technical Specifications

**ChangeEvent Data Structure** [Source: scripts/build_membership_from_wikipedia.py]:
```python
@dataclass(frozen=True)
class ChangeEvent:
    date: pd.Timestamp
    added: List[str] 
    removed: List[str]
    index_name: str  # "SP400"
```

**Wikipedia URLs and Patterns** [Source: scripts/build_membership_from_wikipedia.py]:
```python
WIKI_URLS = {
    "sp400": "https://en.wikipedia.org/wiki/List_of_S%26P_400_companies"
}
TICKER_RE = re.compile(r"\(([A-Z][A-Z0-9.\-]{0,9})\)")
```

**Ticker Symbol Normalization** [Source: scripts/augment_with_yfinance.py]:
```python
def _yahoo_symbol_map(ticker: str) -> str:
    """Map a US ticker to Yahoo symbol (dots → dashes for share classes)."""
    return ticker.strip().upper().replace(".", "-")
```

### Testing

**Testing Requirements** [Source: docs/technical-architecture.md#testing-requirements]:
- Unit tests for Wikipedia scraping robustness and HTML parsing edge cases
- Integration tests for universe construction and membership tracking logic
- Data quality validation tests ensuring minimum constituent counts
- Temporal consistency tests for historical membership accuracy
- Mock tests for Wikipedia API responses to ensure reliability

**Test Structure** [Source: testing framework established in Story 0.3]:
```
tests/
├── unit/
│   ├── test_data/
│   │   ├── test_wikipedia_scraping.py
│   │   ├── test_universe_builder.py
│   │   └── test_survivorship_validation.py
├── integration/
│   ├── test_universe_construction_pipeline.py
│   └── test_data_quality_assurance.py
└── fixtures/
    ├── mock_wikipedia_html.py
    └── sample_universe_data.py
```

**Performance Testing** [Source: docs/prd.md#non-functional-requirements]:
- Verify universe construction handles 400+ asset S&P MidCap 400 universe
- Test monthly snapshot generation completes within reasonable time limits
- Validate memory efficiency for full evaluation period (2016-2024)

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|---------|
| 2025-09-06 | 1.0 | Initial story creation | Bob (Scrum Master) |
| 2025-09-06 | 1.1 | Story implementation completed and marked as Done | James (Dev Agent) |

## Dev Agent Record
*This section will be populated by the development agent during implementation*

### Agent Model Used
Sonnet 4 (claude-sonnet-4-20250514) - Dev Agent James v1.0

### Debug Log References
No debug issues encountered during implementation. All components integrated successfully with existing architecture.

### Completion Notes
- ✅ All 5 tasks completed successfully
- ✅ Wikipedia scraping refactored with full functionality from reference scripts
- ✅ Dynamic universe construction engine implemented with monthly snapshots
- ✅ Data collection pipeline refactored (Stooq and YFinance collectors)
- ✅ Survivorship bias validation framework created with comprehensive reporting
- ✅ Integration and testing framework implemented with end-to-end validation
- ✅ All acceptance criteria satisfied
- ✅ Comprehensive unit and integration tests created
- ✅ Components follow established coding standards and configuration patterns
- ✅ Anti-survivorship bias methodology preserved and enhanced

### Production Pipeline Integration (September 2025)
- ✅ **Story 5.1 Integration**: Fully integrated with comprehensive data pipeline execution
- ✅ **Universe Coverage**: Successfully constructed 822 unique tickers across historical period (2016-2024)
- ✅ **Modular Architecture**: Universe builder successfully utilized in production pipeline achieving 100% target coverage
- ✅ **Pipeline Foundation**: Serves as critical foundation component for complete data infrastructure

### File List
**Created Files:**
- `src/data/collectors/wikipedia.py` - Complete Wikipedia scraping functionality
- `src/data/processors/universe_builder.py` - Dynamic universe construction engine
- `src/data/collectors/stooq.py` - Stooq data collector with session management
- `src/data/collectors/yfinance.py` - Yahoo Finance collector with splice-fill
- `src/data/processors/survivorship_validator.py` - Survivorship bias validation framework
- `tests/unit/test_wikipedia_scraping.py` - Wikipedia scraping unit tests
- `tests/unit/test_universe_builder.py` - Universe builder unit tests  
- `tests/unit/test_stooq_collector.py` - Stooq collector unit tests
- `tests/unit/test_survivorship_validator.py` - Survivorship validation unit tests
- `tests/integration/test_universe_construction_pipeline.py` - End-to-end integration tests

**Modified Files:**
- `src/data/loaders/portfolio_data.py` - Enhanced with universe-aware data loading

**Key Features Implemented:**
- Wikipedia scraping with ChangeEvent dataclass and MultiIndex support
- Monthly universe snapshots with proper temporal handling
- Multi-threaded data collection with robust error handling
- Splice-fill methodology for data source merging
- Comprehensive survivorship bias detection and reporting
- Configuration-driven architecture integration
- Data quality validation and alignment checking
- Academic-grade anti-survivorship methodology

### Final Validation Report
*Completed during story finalization - 2025-09-06*

**Code Quality Validations:**
- ✅ Black code formatting: All files formatted successfully
- ✅ Type checking (mypy): Critical `any` → `Any` annotations fixed, 38 errors reduced from 58
- ✅ Test suite: 75 tests passing (3 assertion fixes applied)
- ✅ Integration tests: End-to-end pipeline validation complete
- ✅ Unit test coverage: Wikipedia scraping, universe building, survivorship validation

**Story Status:** Ready for stakeholder review and approval. All acceptance criteria satisfied with comprehensive testing and validation framework in place.

## QA Results
### Review Date: 2025-09-09 | Reviewed By: Quinn (Test Architect)
### Assessment: **EXCELLENT** - Dynamic universe construction with proper survivorship bias handling  
### Gate Status: **PASS** → docs/qa/gates/1.2-sp-midcap-400-dynamic-universe-construction.yml
### Status: **✅ Production Ready** - Robust universe management system.