"""
Integration layer for model training pipeline and backtest execution.

This module provides seamless integration between the rolling backtest engine
and existing model training components including checkpoint management,
validation engines, and performance analytics.
"""

from __future__ import annotations

import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Any

import numpy as np
import pandas as pd

from src.evaluation.metrics.returns import PerformanceAnalytics
from src.evaluation.validation.rolling_validation import RollingValidationEngine, RollSplit
from src.models.base import PortfolioModel

logger = logging.getLogger(__name__)


@dataclass
class ModelCheckpointInfo:
    """Information about model checkpoint."""

    checkpoint_path: Path
    model_name: str
    split_info: dict[str, Any]
    training_metrics: dict[str, float]
    validation_metrics: dict[str, float]
    created_at: pd.Timestamp
    model_config: dict[str, Any] = field(default_factory=dict)


@dataclass
class ValidationResult:
    """Results from model validation."""

    model_name: str
    split_index: int
    validation_period: tuple[pd.Timestamp, pd.Timestamp]
    metrics: dict[str, float]
    predictions: pd.Series | None = None
    validation_success: bool = True
    error_message: str | None = None


@dataclass
class IntegrationConfig:
    """Configuration for model integration."""

    # Checkpoint management
    enable_checkpointing: bool = True
    checkpoint_dir: Path | None = None
    checkpoint_frequency: int = 1  # Every N splits

    # Validation integration
    enable_validation_metrics: bool = True
    validation_metrics: list[str] = field(default_factory=lambda: ["sharpe_ratio", "total_return", "volatility"])

    # Performance analytics
    enable_performance_attribution: bool = True
    benchmark_ticker: str = "SPY"
    risk_free_rate: float = 0.02

    # Model compatibility
    supported_model_types: list[str] = field(default_factory=lambda: ["HRP", "LSTM", "GAT"])
    enable_model_ensemble: bool = False
    ensemble_weights: dict[str, float] = field(default_factory=dict)


class ModelTrainingIntegrator:
    """
    Integration layer between rolling backtest and model training pipeline.

    This integrator provides:
    - Seamless integration with existing ModelCheckpointManager
    - Connection to RollingValidationEngine validation protocols
    - Compatibility with all model types (HRP, LSTM, GAT)
    - Performance analytics integration
    - Model ensemble capabilities
    """

    def __init__(self, config: IntegrationConfig):
        """Initialize model training integrator."""
        self.config = config

        # Initialize components
        self.performance_analytics = PerformanceAnalytics()

        # Initialize checkpoint registry
        self.checkpoint_registry: dict[str, list[ModelCheckpointInfo]] = {}

        # Initialize validation cache
        self.validation_cache: dict[str, ValidationResult] = {}

        # Model compatibility registry
        self.model_adapters: dict[str, callable] = {
            "HRP": self._adapt_hrp_model,
            "LSTM": self._adapt_lstm_model,
            "GAT": self._adapt_gat_model,
        }

        # Setup checkpoint directory
        if config.enable_checkpointing and config.checkpoint_dir:
            config.checkpoint_dir.mkdir(parents=True, exist_ok=True)

    def integrate_with_rolling_validation(
        self,
        rolling_engine: RollingValidationEngine,
        models: dict[str, PortfolioModel],
        data: dict[str, pd.DataFrame],
    ) -> dict[str, Any]:
        """
        Integrate backtest execution with rolling validation engine.

        Args:
            rolling_engine: Existing rolling validation engine
            models: Portfolio models to integrate
            data: Market data for validation

        Returns:
            Integration results and validation metrics
        """
        logger.info("Integrating with rolling validation engine")

        integration_results = {
            "models_integrated": 0,
            "validation_results": {},
            "checkpoint_info": {},
            "performance_metrics": {},
        }

        # Generate rolling windows using existing engine
        sample_timestamps = list(data["returns"].index)
        splits = rolling_engine.generate_rolling_windows(sample_timestamps)

        if not splits:
            raise ValueError("No rolling windows generated by validation engine")

        logger.info(f"Generated {len(splits)} rolling windows for validation")

        # Integrate each model with validation pipeline
        for model_name, model in models.items():
            try:
                model_results = self._integrate_single_model(
                    model=model,
                    model_name=model_name,
                    splits=splits,
                    data=data,
                    rolling_engine=rolling_engine,
                )

                integration_results["validation_results"][model_name] = model_results
                integration_results["models_integrated"] += 1

            except Exception as e:
                logger.error(f"Failed to integrate model {model_name}: {e}")
                integration_results["validation_results"][model_name] = {"error": str(e)}

        # Generate ensemble results if configured
        if self.config.enable_model_ensemble and len(models) > 1:
            ensemble_results = self._create_model_ensemble(
                models, splits, data, integration_results["validation_results"]
            )
            integration_results["ensemble_results"] = ensemble_results

        # Calculate integrated performance metrics
        integration_results["performance_metrics"] = self._calculate_integrated_performance(
            integration_results["validation_results"]
        )

        return integration_results

    def _integrate_single_model(
        self,
        model: PortfolioModel,
        model_name: str,
        splits: list[RollSplit],
        data: dict[str, pd.DataFrame],
        rolling_engine: RollingValidationEngine,
    ) -> dict[str, Any]:
        """Integrate single model with validation pipeline."""

        model_results = {
            "model_type": self._detect_model_type(model),
            "splits_processed": 0,
            "validation_results": [],
            "checkpoint_paths": [],
            "overall_metrics": {},
        }

        # Detect and adapt model if necessary
        adapted_model = self._adapt_model_for_integration(model, model_name)

        # Process each rolling split
        for i, split in enumerate(splits):
            try:
                # Prepare split data with temporal guards
                train_data, val_data, test_data = rolling_engine.enforce_temporal_guard(
                    split, data["returns"], data["returns"], data["returns"]
                )

                # Train model on split
                training_result = self._train_model_on_split(
                    adapted_model, model_name, split, train_data, val_data
                )

                # Validate model performance
                validation_result = self._validate_model_on_split(
                    adapted_model, model_name, split, val_data, test_data, i
                )

                model_results["validation_results"].append(validation_result)

                # Save checkpoint if configured
                if (self.config.enable_checkpointing and
                    i % self.config.checkpoint_frequency == 0):

                    checkpoint_info = self._save_model_checkpoint(
                        adapted_model, model_name, split, training_result, validation_result
                    )
                    model_results["checkpoint_paths"].append(checkpoint_info.checkpoint_path)

                model_results["splits_processed"] += 1

            except Exception as e:
                logger.error(f"Error processing split {i} for model {model_name}: {e}")
                continue

        # Calculate overall model performance
        if model_results["validation_results"]:
            model_results["overall_metrics"] = self._aggregate_validation_metrics(
                model_results["validation_results"]
            )

        return model_results

    def _detect_model_type(self, model: PortfolioModel) -> str:
        """Detect model type from model class."""

        model_class_name = model.__class__.__name__.upper()

        for model_type in self.config.supported_model_types:
            if model_type in model_class_name:
                return model_type

        return "UNKNOWN"

    def _adapt_model_for_integration(self, model: PortfolioModel, model_name: str) -> PortfolioModel:
        """Adapt model for integration with backtest pipeline."""

        model_type = self._detect_model_type(model)

        if model_type in self.model_adapters:
            return self.model_adapters[model_type](model, model_name)

        logger.warning(f"No adapter available for model type {model_type}, using as-is")
        return model

    def _adapt_hrp_model(self, model: PortfolioModel, model_name: str) -> PortfolioModel:
        """Adapt HRP model for integration."""
        # HRP models typically don't need special adaptation
        return model

    def _adapt_lstm_model(self, model: PortfolioModel, model_name: str) -> PortfolioModel:
        """Adapt LSTM model for integration."""
        # LSTM models may need sequence length adjustments
        return model

    def _adapt_gat_model(self, model: PortfolioModel, model_name: str) -> PortfolioModel:
        """Adapt GAT model for integration."""
        # GAT models may need graph structure adaptations
        return model

    def _train_model_on_split(
        self,
        model: PortfolioModel,
        model_name: str,
        split: RollSplit,
        train_data: pd.DataFrame,
        val_data: pd.DataFrame | None,
    ) -> dict[str, Any]:
        """Train model on specific split."""

        training_start = pd.Timestamp.now()

        try:
            # Prepare universe
            universe = train_data.columns.tolist()

            # Train model
            model.fit(
                returns=train_data,
                universe=universe,
                fit_period=(split.train_period.start_date, split.train_period.end_date),
            )

            training_time = (pd.Timestamp.now() - training_start).total_seconds()

            return {
                "success": True,
                "training_time": training_time,
                "training_samples": len(train_data),
                "training_assets": len(universe),
                "split_info": {
                    "train_start": split.train_period.start_date,
                    "train_end": split.train_period.end_date,
                },
            }

        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "training_time": (pd.Timestamp.now() - training_start).total_seconds(),
            }

    def _validate_model_on_split(
        self,
        model: PortfolioModel,
        model_name: str,
        split: RollSplit,
        val_data: pd.DataFrame | None,
        test_data: pd.DataFrame | None,
        split_index: int,
    ) -> ValidationResult:
        """Validate model performance on split."""

        try:
            # Use test data for validation if validation data is not available
            validation_data = val_data if val_data is not None and not val_data.empty else test_data

            if validation_data is None or validation_data.empty:
                return ValidationResult(
                    model_name=model_name,
                    split_index=split_index,
                    validation_period=(split.validation_period.start_date, split.validation_period.end_date),
                    metrics={},
                    validation_success=False,
                    error_message="No validation data available",
                )

            # Generate predictions
            predictions = model.predict_weights(
                date=split.validation_period.start_date,
                universe=validation_data.columns.tolist(),
            )

            if predictions.empty:
                return ValidationResult(
                    model_name=model_name,
                    split_index=split_index,
                    validation_period=(split.validation_period.start_date, split.validation_period.end_date),
                    metrics={},
                    validation_success=False,
                    error_message="Model generated empty predictions",
                )

            # Calculate portfolio returns
            aligned_data = validation_data.loc[:, predictions.index.intersection(validation_data.columns)]
            aligned_weights = predictions.reindex(aligned_data.columns, fill_value=0)
            portfolio_returns = (aligned_data * aligned_weights).sum(axis=1)

            # Calculate validation metrics
            metrics = {}
            if len(portfolio_returns) > 1:
                metrics = self.performance_analytics.calculate_portfolio_metrics(portfolio_returns)

            return ValidationResult(
                model_name=model_name,
                split_index=split_index,
                validation_period=(split.validation_period.start_date, split.validation_period.end_date),
                metrics=metrics,
                predictions=predictions,
                validation_success=True,
            )

        except Exception as e:
            return ValidationResult(
                model_name=model_name,
                split_index=split_index,
                validation_period=(split.validation_period.start_date, split.validation_period.end_date),
                metrics={},
                validation_success=False,
                error_message=str(e),
            )

    def _save_model_checkpoint(
        self,
        model: PortfolioModel,
        model_name: str,
        split: RollSplit,
        training_result: dict[str, Any],
        validation_result: ValidationResult,
    ) -> ModelCheckpointInfo:
        """Save model checkpoint with comprehensive metadata."""

        if not self.config.checkpoint_dir:
            raise ValueError("Checkpoint directory not configured")

        checkpoint_filename = f"{model_name}_split_{validation_result.split_index}_{split.train_period.start_date.strftime('%Y%m%d')}.pkl"
        checkpoint_path = self.config.checkpoint_dir / checkpoint_filename

        # Create checkpoint info
        checkpoint_info = ModelCheckpointInfo(
            checkpoint_path=checkpoint_path,
            model_name=model_name,
            split_info={
                "split_index": validation_result.split_index,
                "train_start": split.train_period.start_date.isoformat(),
                "train_end": split.train_period.end_date.isoformat(),
                "validation_start": split.validation_period.start_date.isoformat(),
                "validation_end": split.validation_period.end_date.isoformat(),
            },
            training_metrics=training_result,
            validation_metrics=validation_result.metrics,
            created_at=pd.Timestamp.now(),
            model_config={"model_type": self._detect_model_type(model)},
        )

        # Save checkpoint (simplified - would use pickle or model-specific serialization)
        try:
            import pickle
            with open(checkpoint_path, 'wb') as f:
                pickle.dump({
                    'model': model,
                    'checkpoint_info': checkpoint_info,
                }, f)

            # Register checkpoint
            if model_name not in self.checkpoint_registry:
                self.checkpoint_registry[model_name] = []
            self.checkpoint_registry[model_name].append(checkpoint_info)

            logger.debug(f"Saved checkpoint: {checkpoint_path}")

        except Exception as e:
            logger.error(f"Failed to save checkpoint: {e}")

        return checkpoint_info

    def _aggregate_validation_metrics(self, validation_results: list[ValidationResult]) -> dict[str, float]:
        """Aggregate validation metrics across splits."""

        successful_results = [r for r in validation_results if r.validation_success]

        if not successful_results:
            return {"error": "No successful validations"}

        # Extract metrics from each result
        all_metrics = {}
        for result in successful_results:
            for metric_name, metric_value in result.metrics.items():
                if metric_name not in all_metrics:
                    all_metrics[metric_name] = []
                all_metrics[metric_name].append(metric_value)

        # Calculate aggregated metrics
        aggregated = {}
        for metric_name, values in all_metrics.items():
            if values:  # Only process non-empty lists
                aggregated[f"avg_{metric_name}"] = np.mean(values)
                aggregated[f"std_{metric_name}"] = np.std(values)
                aggregated[f"min_{metric_name}"] = np.min(values)
                aggregated[f"max_{metric_name}"] = np.max(values)

        # Add summary statistics
        aggregated["successful_validations"] = len(successful_results)
        aggregated["total_validations"] = len(validation_results)
        aggregated["validation_success_rate"] = len(successful_results) / len(validation_results)

        return aggregated

    def _create_model_ensemble(
        self,
        models: dict[str, PortfolioModel],
        splits: list[RollSplit],
        data: dict[str, pd.DataFrame],
        model_results: dict[str, Any],
    ) -> dict[str, Any]:
        """Create ensemble of models based on validation performance."""

        logger.info("Creating model ensemble")

        # Determine ensemble weights based on validation performance
        ensemble_weights = {}
        total_performance = 0.0

        for model_name, results in model_results.items():
            if "overall_metrics" in results and "avg_sharpe_ratio" in results["overall_metrics"]:
                performance = results["overall_metrics"]["avg_sharpe_ratio"]
                ensemble_weights[model_name] = max(0, performance)  # Non-negative weights
                total_performance += ensemble_weights[model_name]

        # Normalize weights
        if total_performance > 0:
            ensemble_weights = {
                name: weight / total_performance
                for name, weight in ensemble_weights.items()
            }
        else:
            # Equal weights if no performance data
            ensemble_weights = {name: 1.0 / len(models) for name in models.keys()}

        logger.info(f"Ensemble weights: {ensemble_weights}")

        return {
            "ensemble_weights": ensemble_weights,
            "ensemble_method": "performance_weighted",
            "total_models": len(models),
        }

    def _calculate_integrated_performance(self, model_results: dict[str, Any]) -> dict[str, Any]:
        """Calculate integrated performance metrics across all models."""

        integrated_metrics = {
            "models_evaluated": len(model_results),
            "successful_models": 0,
            "best_model": None,
            "best_performance": float('-inf'),
            "performance_comparison": {},
        }

        for model_name, results in model_results.items():
            if "overall_metrics" in results and results["overall_metrics"]:
                integrated_metrics["successful_models"] += 1

                # Extract key performance metric (Sharpe ratio)
                performance = results["overall_metrics"].get("avg_sharpe_ratio", float('-inf'))
                integrated_metrics["performance_comparison"][model_name] = performance

                # Track best model
                if performance > integrated_metrics["best_performance"]:
                    integrated_metrics["best_performance"] = performance
                    integrated_metrics["best_model"] = model_name

        return integrated_metrics

    def get_checkpoint_info(self, model_name: str) -> list[ModelCheckpointInfo]:
        """Get checkpoint information for model."""
        return self.checkpoint_registry.get(model_name, [])

    def load_model_checkpoint(
        self,
        model_name: str,
        checkpoint_index: int = -1
    ) -> PortfolioModel | None:
        """Load model from checkpoint."""

        if model_name not in self.checkpoint_registry:
            return None

        checkpoints = self.checkpoint_registry[model_name]
        if not checkpoints or abs(checkpoint_index) > len(checkpoints):
            return None

        checkpoint_info = checkpoints[checkpoint_index]

        try:
            import pickle
            with open(checkpoint_info.checkpoint_path, 'rb') as f:
                checkpoint_data = pickle.load(f)
                return checkpoint_data['model']
        except Exception as e:
            logger.error(f"Failed to load checkpoint: {e}")
            return None

    def get_integration_summary(self) -> dict[str, Any]:
        """Get summary of integration status and performance."""

        return {
            "config": {
                "checkpointing_enabled": self.config.enable_checkpointing,
                "validation_metrics_enabled": self.config.enable_validation_metrics,
                "supported_models": self.config.supported_model_types,
                "ensemble_enabled": self.config.enable_model_ensemble,
            },
            "checkpoints": {
                "total_models_with_checkpoints": len(self.checkpoint_registry),
                "total_checkpoints": sum(len(checkpoints) for checkpoints in self.checkpoint_registry.values()),
            },
            "validation_cache_size": len(self.validation_cache),
        }
