assets_used:
- TEX
- HUBB
- BFH
- SMTC
- OVV
- EHC
- AMG
- TRIP
- MTZ
- FLG
- KSS
- AIT
- OMI
- PRGO
- TTWO
- WPC
- CNO
- GEO
- MTSI
- RBC
- VNO
- ALV
- TRMB
- FR
- PAG
- OMCL
- TTEK
- TOL
- LAD
- PTC
- NYT
- ICUI
- CAR
- SLM
- STWD
- SBRA
- FHN
- SYNA
- HNI
- CLC
- G
- SEIC
- KNX
- WOR
- FAF
- UBSI
- BURL
- COHR
- CBT
- BIO
- CELH
- ALGN
- SCOR
- ENSG
- IRDM
- OLED
- SMG
- COTY
- DDD
- FLEX
- WAL
- FCFS
- RS
- PENN
- CVG
- SNX
- VICR
- AMD
- ASH
- CFR
- SM
- LEG
- OIS
- GES
- NAN
- HR
- VYX
- GDOT
- IT
- WEN
- MAC
- NDSN
- MSA
- CBOE
- FNF
- GATX
- DLX
- BWXT
- KAR
- CYH
- WEX
- LSTR
- CIEN
- STRA
- SCI
- GMED
- TXRH
- ST
- HRB
- ATGE
- CGNX
- FICO
- FULT
- AOS
- OGS
- BR
- PPC
- URBN
- ARW
- ZD
- SWX
- WERN
- SXT
- PZZA
- GGG
- TTC
- H
- IART
- RMD
- WHR
- MORN
- SAIA
- CAKE
- BCO
- TNL
- QDEL
- TECH
- TYL
- RNR
- MDRX
- LNT
- ALB
- DNOW
- GHC
- HELE
- AMCX
- MUSA
- ACIW
- KEX
- INGR
- WGL
- TREX
- GVA
- GTLS
- NBIX
- CYTK
- WU
- IRT
- NXST
- JEF
- POWI
- ALE
- ONB
- TRN
- RGLD
- DAR
- UFPI
- CHDN
- UGI
- ACHC
- HWC
- WRB
- BMRN
- TREE
- ACM
- QLYS
- PCH
- LPNT
- OPCH
- R
- TRGP
- PEB
- EXP
- VLY
- RJF
- EEFT
- RLI
- MKTX
- JLL
- CDP
- VMI
- SABR
- KRG
- BDC
- KRC
- VOYA
- VSAT
- ERIE
- ENPH
- FHI
- AGCO
- CRL
- LOPE
- LGND
- COKE
- ANSS
- UAA
- NUS
- TPL
- KBH
device_name: NVIDIA GeForce RTX 5070 Ti Laptop GPU
gpu_available: true
hyperparameter_optimization:
  all_results:
    lstm_h128_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0005554792587645352
      config_name: lstm_h128_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.001492284315948685
      final_val_loss: 0.0005583949678111821
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - &id001 !!python/object/apply:numpy.dtype
        args:
        - f8
        - false
        - true
        state: !!python/tuple
        - 3
        - <
        - null
        - null
        - null
        - -1
        - -1
        - 0
      - !!binary |
        vw1OiHOxcT8=
      train_losses:
      - 0.1449782553439339
      - 0.020159236388280988
      - 0.006804639764595777
      - 0.0036499802372418344
      - 0.0027453268121462315
      - 0.0022258861863519996
      - 0.0019298425080099453
      - 0.0018533586408011615
      - 0.0015133391425479203
      - 0.0014121858015035589
      - 0.0013410030708958705
      - 0.001492284315948685
      val_losses:
      - 0.004156089737080038
      - 0.0005554792587645352
      - 0.0005567585758399218
      - 0.000557536433916539
      - 0.0005579774442594498
      - 0.0005582422309089452
      - 0.0005583931924775243
      - 0.0005584953760262579
      - 0.0005585085018537939
      - 0.0005584812024608254
      - 0.0005584390892181545
      - 0.0005583949678111821
    lstm_h128_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0005569193162955344
      config_name: lstm_h128_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0011142028912824269
      final_val_loss: 0.0005577049450948834
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        +hsq6cfrcj8=
      train_losses:
      - 0.19109607425828776
      - 0.016542304617663223
      - 0.004843450597642611
      - 0.0027231628676721207
      - 0.0018334621951604884
      - 0.0016513350419700146
      - 0.001398082512120406
      - 0.0012406764678113784
      - 0.0013037311388567712
      - 0.0010582357402502869
      - 0.0010765610059024766
      - 0.0011625627278893564
      - 0.0011142028912824269
      val_losses:
      - 0.003981448011472821
      - 0.0005589719221461564
      - 0.0005569193162955344
      - 0.000557666557142511
      - 0.0005580205470323563
      - 0.0005581460427492857
      - 0.0005581706645898521
      - 0.0005581165896728635
      - 0.0005580202850978822
      - 0.0005579553544521332
      - 0.0005578703421633691
      - 0.0005577874835580587
      - 0.0005577049450948834
    lstm_h128_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0005567077896557748
      config_name: lstm_h128_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.0037025036096262434
      final_val_loss: 0.0005657579167746007
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        HLmeKaE7Zz8=
      train_losses:
      - 0.23536017971734205
      - 0.05052314046770334
      - 0.02067963918671012
      - 0.012126108476271233
      - 0.008403865154832602
      - 0.006957453326322138
      - 0.0057945149795462685
      - 0.00486501931057622
      - 0.004360222800945242
      - 0.003791073007353892
      - 0.003727160432996849
      - 0.0037025036096262434
      val_losses:
      - 0.0030011649942025542
      - 0.0005567077896557748
      - 0.0005589788779616356
      - 0.0005608024075627327
      - 0.0005622512544505298
      - 0.000563381181564182
      - 0.0005642617761623114
      - 0.0005649256054311991
      - 0.0005651831743307412
      - 0.0005654136766679585
      - 0.0005655912682414055
      - 0.0005657579167746007
    lstm_h128_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0005562244332395494
      config_name: lstm_h128_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0030987883219495416
      final_val_loss: 0.0005631837411783636
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        xmF1xjBnYj8=
      train_losses:
      - 0.2776974570006132
      - 0.05122688304012021
      - 0.017390210336695116
      - 0.009977911366149783
      - 0.006563559950639804
      - 0.004601822137677421
      - 0.004532699337384353
      - 0.003856976303116729
      - 0.003162562798631067
      - 0.0033735320321284235
      - 0.002919678071824213
      - 0.0030987883219495416
      val_losses:
      - 0.008538750000298023
      - 0.0005562244332395494
      - 0.0005584956670645624
      - 0.0005601159064099193
      - 0.0005612015083897859
      - 0.0005618644063360989
      - 0.0005623544275294989
      - 0.0005627532082144171
      - 0.0005628950602840632
      - 0.0005630206142086536
      - 0.0005631159583572298
      - 0.0005631837411783636
    lstm_h128_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.0005560754216276109
      config_name: lstm_h128_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.0065293375713129835
      final_val_loss: 0.0005695658619515598
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        2RFzgS2LdT8=
      train_losses:
      - 0.41235995292663574
      - 0.091463103890419
      - 0.04121657429883877
      - 0.021550811749572556
      - 0.017070975465079147
      - 0.014663603234415254
      - 0.012273032761489352
      - 0.009535216726362705
      - 0.009068913292139769
      - 0.007675617700442672
      - 0.007865314953960478
      - 0.0065293375713129835
      val_losses:
      - 0.013466325588524342
      - 0.0005560754216276109
      - 0.0005589060310740024
      - 0.0005613172543235123
      - 0.0005631943640764803
      - 0.0005648600053973496
      - 0.0005663639749400318
      - 0.0005676281580235809
      - 0.0005681992333848029
      - 0.0005687267403118312
      - 0.000569173862459138
      - 0.0005695658619515598
    lstm_h128_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0005562150909099728
      config_name: lstm_h128_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0073775155857826276
      final_val_loss: 0.0005695898726116866
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        kf7pZ8V9dD8=
      train_losses:
      - 0.33127858117222786
      - 0.07801896271606286
      - 0.03559770559271177
      - 0.02223832719027996
      - 0.015698473745336134
      - 0.012971515534445643
      - 0.009834893979132175
      - 0.007752635050565004
      - 0.0072869318149363
      - 0.0069385139892498655
      - 0.0060602166534711914
      - 0.0073775155857826276
      val_losses:
      - 0.0018377536325715482
      - 0.0005562150909099728
      - 0.0005589296051766723
      - 0.0005614096589852124
      - 0.0005634585104417056
      - 0.0005651661776937544
      - 0.0005666329234372824
      - 0.0005678960878867656
      - 0.0005684018251486123
      - 0.0005688214732799679
      - 0.0005692301492672414
      - 0.0005695898726116866
    lstm_h128_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.000553386053070426
      config_name: lstm_h128_lr0.0005_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0006766751757822931
      final_val_loss: 0.0005533923394978046
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        TH9wX2cAhT8=
      train_losses:
      - 0.08811508976699163
      - 0.0028576844391257814
      - 0.0013095110771246254
      - 0.0009705330351910865
      - 0.0008472220700544616
      - 0.0008220576661794136
      - 0.0008439249747122327
      - 0.0008159875142155215
      - 0.0007468320933791498
      - 0.000735645832416291
      - 0.0007418392633553594
      - 0.0007370965079947686
      - 0.0007418257397754738
      - 0.0007357958723635724
      - 0.000761065054878903
      - 0.0007259944638159747
      - 0.0007164309257253384
      - 0.0007125547175140431
      - 0.0006844547121242309
      - 0.0006879946061720451
      - 0.0006891355927412709
      - 0.0006855107979693761
      - 0.0006960335304029286
      - 0.0006772245639391864
      - 0.000671834389019447
      - 0.0006744410299385587
      - 0.000694249373433801
      - 0.0006939195494245117
      - 0.0006750626732051993
      - 0.0006766751757822931
      val_losses:
      - 0.0005658555019181222
      - 0.000577369297388941
      - 0.0005790878494735807
      - 0.0005755342426709831
      - 0.0005705678777303547
      - 0.0005661171162500978
      - 0.0005626993661280721
      - 0.0005602522869594395
      - 0.0005584377213381231
      - 0.0005570585199166089
      - 0.000556092505576089
      - 0.0005554470408242196
      - 0.000554985279450193
      - 0.0005546011670958251
      - 0.0005542791914194822
      - 0.0005541032005567104
      - 0.000553954771021381
      - 0.0005538652767427266
      - 0.0005537505494430661
      - 0.0005536098615266383
      - 0.0005536526732612401
      - 0.0005535355012398213
      - 0.0005534648662433028
      - 0.000553432444576174
      - 0.0005534096853807569
      - 0.0005534430383704603
      - 0.0005533945222850889
      - 0.000553386053070426
      - 0.0005534043302759528
      - 0.0005533923394978046
    lstm_h128_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.0005532985960599035
      config_name: lstm_h128_lr0.0005_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0006479368021246046
      final_val_loss: 0.0005533222865778953
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 29
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        sEAPXSikhz8=
      train_losses:
      - 0.0572658353485167
      - 0.0017124446652208765
      - 0.0010513257536028202
      - 0.0008586735687761878
      - 0.0007919547739826763
      - 0.0007087123037005464
      - 0.0007740713287300119
      - 0.0007261889453123634
      - 0.0007323276659008116
      - 0.0007156181382015347
      - 0.0006915477715665475
      - 0.0006924767561334496
      - 0.0006907034548930824
      - 0.0006970005779294297
      - 0.0006828048111250004
      - 0.0006749963407249501
      - 0.0006636735148883114
      - 0.0006827313918620348
      - 0.0006666003124943624
      - 0.0006757676746929064
      - 0.0006572178293329974
      - 0.0006821785937063396
      - 0.0006663619860773906
      - 0.00069034167972859
      - 0.0006796737822393576
      - 0.0006629319541389123
      - 0.0006619509464750687
      - 0.0006602094508707523
      - 0.0006479368021246046
      val_losses:
      - 0.0005629240476991981
      - 0.0005708295211661607
      - 0.0005709457327611744
      - 0.0005673133709933609
      - 0.000563246023375541
      - 0.0005599665746558458
      - 0.0005576344847213477
      - 0.0005560980062000453
      - 0.0005550647911150008
      - 0.000554393365746364
      - 0.0005539744161069393
      - 0.000553672289242968
      - 0.0005535254022106528
      - 0.0005534599767997861
      - 0.0005533871008083224
      - 0.000553356803720817
      - 0.0005533316289074719
      - 0.0005533047369681299
      - 0.0005532985960599035
      - 0.0005533381481654942
      - 0.0005533113435376436
      - 0.0005533437361009419
      - 0.0005533291259780526
      - 0.0005533146904781461
      - 0.0005533234507311136
      - 0.0005533400981221348
      - 0.0005533405055757612
      - 0.0005533164658118039
      - 0.0005533222865778953
    lstm_h128_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0005536899552680552
      config_name: lstm_h128_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007630442560184747
      final_val_loss: 0.0005536899552680552
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        Uj5eTtV2dz8=
      train_losses:
      - 0.14929139303664365
      - 0.0059202998915376765
      - 0.002169547777157277
      - 0.001709123937568317
      - 0.0010931624565273523
      - 0.0012989624325806897
      - 0.0010588116468473647
      - 0.0010542861515811335
      - 0.0010397401977873717
      - 0.0009589389034469301
      - 0.000924192305925923
      - 0.000944676906025658
      - 0.0008551054925192147
      - 0.0009543849446345121
      - 0.000838285923236981
      - 0.0008578211030301949
      - 0.0008353143784916028
      - 0.0008214840636355802
      - 0.0008854162879288197
      - 0.0009307145228376612
      - 0.000845408498814019
      - 0.0008621110852497319
      - 0.0007800688375330841
      - 0.0007690290901033828
      - 0.0007344369902663553
      - 0.000752887106500566
      - 0.0008580549329053611
      - 0.000698618193079407
      - 0.000744031747065795
      - 0.0007630442560184747
      val_losses:
      - 0.0005675010033883154
      - 0.0005832320312038064
      - 0.0005874312773812562
      - 0.0005845322739332914
      - 0.000578951119678095
      - 0.0005733695288654417
      - 0.0005686664953827858
      - 0.0005667168006766587
      - 0.0005649576487485319
      - 0.0005633411055896431
      - 0.0005619268631562591
      - 0.0005606383201666176
      - 0.0005595666880253702
      - 0.0005586182815022767
      - 0.0005578525015152991
      - 0.0005571652727667242
      - 0.0005565879109781235
      - 0.0005560493736993521
      - 0.0005555910465773195
      - 0.000555268838070333
      - 0.0005549885390792042
      - 0.0005547761684283614
      - 0.0005545710737351328
      - 0.0005543363222386688
      - 0.0005541483114939183
      - 0.0005540221463888884
      - 0.0005539115809369832
      - 0.000553842110093683
      - 0.0005537580291274935
      - 0.0005536899552680552
    lstm_h128_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0005538433615583926
      config_name: lstm_h128_lr0.0005_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0007419780158670619
      final_val_loss: 0.0005538433615583926
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        TtHjc53Rgj8=
      train_losses:
      - 0.15266797776954869
      - 0.005513510938423376
      - 0.0021117913032261035
      - 0.0016360209556296468
      - 0.0011952860901753108
      - 0.0012487390874108921
      - 0.001026101890602149
      - 0.0009757432174713662
      - 0.0010363943729316816
      - 0.0008546818523124481
      - 0.0010025360097642988
      - 0.0009186666138703004
      - 0.0008745092054596171
      - 0.0008661398460390046
      - 0.0008313233847729862
      - 0.0009341492162396511
      - 0.0008393380800650144
      - 0.0008609769671844939
      - 0.0008313457704692458
      - 0.0008167459745891392
      - 0.0008892001496860757
      - 0.0007521381048718467
      - 0.0007399039119870091
      - 0.0007456844226301959
      - 0.0007618879608344287
      - 0.0008259613096015528
      - 0.0007927547606717175
      - 0.0007968178688315675
      - 0.0007995228709963461
      - 0.0007419780158670619
      val_losses:
      - 0.000566301605431363
      - 0.00058115812134929
      - 0.0005853550101164728
      - 0.0005826624110341072
      - 0.0005774469173047692
      - 0.0005723684444092214
      - 0.0005679157911799848
      - 0.0005660820170305669
      - 0.0005644631164614111
      - 0.0005629662482533604
      - 0.0005616050912067294
      - 0.0005605386104434729
      - 0.0005595013499259949
      - 0.0005585901963058859
      - 0.0005578321870416403
      - 0.0005571911460720003
      - 0.0005566068284679204
      - 0.0005561396246775985
      - 0.0005557330150622874
      - 0.0005554135714191943
      - 0.0005551290523726493
      - 0.0005548395856749266
      - 0.0005546239553950727
      - 0.0005544619634747505
      - 0.0005543181614484638
      - 0.0005541949649341404
      - 0.0005540888232644647
      - 0.0005540077982004732
      - 0.0005539068079087883
      - 0.0005538433615583926
    lstm_h128_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0005694380670320243
      config_name: lstm_h128_lr0.0005_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.0015936202107695863
      final_val_loss: 0.0005726727831643075
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        htGW6kRKYL8=
      train_losses:
      - 0.2019789001593987
      - 0.017140722135081887
      - 0.006922024845456083
      - 0.004037165277016659
      - 0.0027234513933459916
      - 0.0020792938788266233
      - 0.0022132475763404122
      - 0.0018152518508334954
      - 0.001974590112998461
      - 0.0016475140437250957
      - 0.0015936202107695863
      val_losses:
      - 0.0005694380670320243
      - 0.0005932164494879544
      - 0.0006049804796930403
      - 0.0006058430881239474
      - 0.0005996692925691605
      - 0.0005919287214055657
      - 0.0005846268613822758
      - 0.0005811768351122737
      - 0.0005781025975011289
      - 0.0005752786237280816
      - 0.0005726727831643075
    lstm_h128_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0005688669916708022
      config_name: lstm_h128_lr0.0005_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0014886128628859296
      final_val_loss: 0.0005702890048269182
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        pQTSavdgbz8=
      train_losses:
      - 0.22279288216183582
      - 0.014747969224117696
      - 0.005322617449564859
      - 0.0032753858055608966
      - 0.002364268012267227
      - 0.0021475307391180345
      - 0.0019186998057800035
      - 0.001465790905058384
      - 0.0015632686845492572
      - 0.0016736381270069007
      - 0.0014886128628859296
      val_losses:
      - 0.0005688669916708022
      - 0.0005916878872085363
      - 0.000601780804572627
      - 0.0006008675554767251
      - 0.0005946902674622834
      - 0.000587678950978443
      - 0.0005809848662465811
      - 0.0005779545463155955
      - 0.0005751135759055614
      - 0.0005726051167584956
      - 0.0005702890048269182
    lstm_h128_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0005532719369512051
      config_name: lstm_h128_lr0.001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0006582438509212807
      final_val_loss: 0.0005533825024031103
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        V0ZgtEduhD8=
      train_losses:
      - 0.07848513598825473
      - 0.0011212434280120458
      - 0.0007751376979285851
      - 0.0007017544703558087
      - 0.0006918858271092176
      - 0.000703749208090206
      - 0.0006642241642111912
      - 0.0006800683768233284
      - 0.0006760674538478876
      - 0.0006587961640131349
      - 0.0006570196953058863
      - 0.0006531000350757191
      - 0.000652479570514212
      - 0.0006562560447491705
      - 0.0006629676354350522
      - 0.0006596941530006006
      - 0.0006616754302134117
      - 0.0006655388230380291
      - 0.0006677536099838713
      - 0.0006586481370807936
      - 0.0006558884585198635
      - 0.0006521335405219967
      - 0.00065315605509871
      - 0.0006608616968151182
      - 0.0006542777021725973
      - 0.0006544763697699333
      - 0.0006464074976975098
      - 0.0006548409971098105
      - 0.0006495593794776747
      - 0.0006582438509212807
      val_losses:
      - 0.0005865137500222772
      - 0.0005999799177516252
      - 0.0005893276247661561
      - 0.0005747699178755283
      - 0.0005647013895213604
      - 0.0005589850479736924
      - 0.0005561273719649762
      - 0.0005547985783778131
      - 0.0005540995916817337
      - 0.0005537699616979808
      - 0.000553658464923501
      - 0.0005535856180358678
      - 0.0005534470546990633
      - 0.0005533825897146016
      - 0.0005533931544050574
      - 0.0005533749936148524
      - 0.0005533929797820747
      - 0.0005534311931114644
      - 0.0005533878575079143
      - 0.0005532719369512051
      - 0.0005533518269658089
      - 0.0005534286610782146
      - 0.0005534006631933153
      - 0.0005533775547519326
      - 0.0005534877418540418
      - 0.0005533553776331246
      - 0.0005533095973078161
      - 0.0005532915820367634
      - 0.0005533640214707702
      - 0.0005533825024031103
    lstm_h128_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0005533258954528719
      config_name: lstm_h128_lr0.001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0006579907737129057
      final_val_loss: 0.000553403893718496
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 24
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        2G1/CEPJhz8=
      train_losses:
      - 0.11794092676912744
      - 0.0011812505157043536
      - 0.0008397814041624466
      - 0.0007985362559945012
      - 0.0006905150997530048
      - 0.0006748554878868163
      - 0.0006719451921526343
      - 0.000663664778888536
      - 0.0006792427933154007
      - 0.0006645446992479265
      - 0.0006625406967941672
      - 0.0006735866869955013
      - 0.0006903605002056187
      - 0.0006591991210977236
      - 0.0006633098528254777
      - 0.0006647176148059467
      - 0.0006814327740964169
      - 0.0006595998953950281
      - 0.0006529294914798811
      - 0.0006444654888279425
      - 0.0006573373054076607
      - 0.0006517001456813887
      - 0.0006663133826805279
      - 0.0006579907737129057
      val_losses:
      - 0.0005856117641087621
      - 0.0006031006632838398
      - 0.000593084841966629
      - 0.0005776170291937888
      - 0.0005664906639140099
      - 0.0005601902084890753
      - 0.0005568758642766625
      - 0.0005551549256779253
      - 0.0005542614380829036
      - 0.000553833058802411
      - 0.0005536775570362806
      - 0.0005535022064577788
      - 0.0005534327938221395
      - 0.0005533258954528719
      - 0.0005533441435545683
      - 0.0005534771189559251
      - 0.000553462072275579
      - 0.0005534584051929414
      - 0.0005534400697797537
      - 0.000553434801986441
      - 0.0005534227238968015
      - 0.000553365534869954
      - 0.0005533298244699836
      - 0.000553403893718496
    lstm_h128_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.0005532767099794
      config_name: lstm_h128_lr0.001_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007003776457471153
      final_val_loss: 0.0005533539515454322
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        yekkVHtOhT8=
      train_losses:
      - 0.10874660627450794
      - 0.0023269986656183996
      - 0.0013399623324706529
      - 0.0009564316278556362
      - 0.0008668453131879991
      - 0.0007839046496277055
      - 0.0007484063535230234
      - 0.0007353761272194485
      - 0.0007398545033841705
      - 0.0007511771837016568
      - 0.0007002413428078095
      - 0.0007302183027301604
      - 0.0006836598331574351
      - 0.0006740311946487054
      - 0.0006933665960483874
      - 0.0006866036468030264
      - 0.0006909523217473179
      - 0.0006805221152414257
      - 0.000719862359498317
      - 0.000674811924303261
      - 0.0006745807865324119
      - 0.0006666545959888026
      - 0.0006633921972631166
      - 0.0006744527102758487
      - 0.0006687955659193298
      - 0.0006684138158258671
      - 0.0006630764401052147
      - 0.0006738984229741618
      - 0.0006782447550601015
      - 0.0007003776457471153
      val_losses:
      - 0.0005945323209743947
      - 0.0006197054171934724
      - 0.0006075949931982905
      - 0.0005871251341886818
      - 0.000572150805965066
      - 0.0005633514374494553
      - 0.000558673869818449
      - 0.0005562536243814975
      - 0.000554858852410689
      - 0.0005542156577575952
      - 0.0005538179830182344
      - 0.0005537295364774764
      - 0.0005535756936296821
      - 0.0005533974035643041
      - 0.0005533825606107712
      - 0.0005534932133741677
      - 0.0005535218515433371
      - 0.000553432444576174
      - 0.0005535600939765573
      - 0.0005534584342967719
      - 0.000553425430553034
      - 0.0005533578514587134
      - 0.0005533365765586495
      - 0.0005533708317670971
      - 0.0005534181836992502
      - 0.0005534157680813223
      - 0.0005534416704904288
      - 0.0005533629446290433
      - 0.0005532767099794
      - 0.0005533539515454322
    lstm_h128_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.000553316087462008
      config_name: lstm_h128_lr0.001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0006804539492198577
      final_val_loss: 0.0005533522344194353
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 23
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        Y/mgrpFQiT8=
      train_losses:
      - 0.09540772643716385
      - 0.0021742339983272054
      - 0.0009432086274803927
      - 0.0008573108886290962
      - 0.0007728191170220574
      - 0.0007864823225342358
      - 0.0007295340037671849
      - 0.000705600624011519
      - 0.0007027341731979201
      - 0.0006813049452224126
      - 0.0006993212640130272
      - 0.0006958585921286916
      - 0.0006699514973054951
      - 0.0006667074291423584
      - 0.0006733050880332788
      - 0.000675076114324232
      - 0.0006668307420720035
      - 0.0006634194141952321
      - 0.0006698436918668449
      - 0.0006953359843464568
      - 0.0006659122930917268
      - 0.0006795880957118546
      - 0.0006804539492198577
      val_losses:
      - 0.0005903714336454868
      - 0.0006081167666707188
      - 0.0005938433750998229
      - 0.0005756655591540039
      - 0.0005639366863761097
      - 0.0005582538724411279
      - 0.0005556493997573853
      - 0.0005542891449294984
      - 0.0005538225523196161
      - 0.0005536590178962797
      - 0.0005535404779948294
      - 0.0005533435032702982
      - 0.000553316087462008
      - 0.000553447607671842
      - 0.0005534093652386218
      - 0.0005533982766792178
      - 0.0005533614894375205
      - 0.0005533261282835156
      - 0.0005533736839424819
      - 0.0005533513613045216
      - 0.0005533562798518687
      - 0.0005533419025596231
      - 0.0005533522344194353
    lstm_h128_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0005532575014512986
      config_name: lstm_h128_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0006650489570650583
      final_val_loss: 0.0005532575014512986
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        WP9yCnVjjT8=
      train_losses:
      - 0.18251469281191626
      - 0.0063417846492181225
      - 0.0026015990297310054
      - 0.0015732498189512019
      - 0.0013441886888661732
      - 0.0010669255182923127
      - 0.0010400518416039024
      - 0.0009795827063499019
      - 0.0008859075120805452
      - 0.0008555007516406476
      - 0.0009367036254843697
      - 0.0007836873895333459
      - 0.0008322109885436172
      - 0.0009065024205483496
      - 0.0008245028681509817
      - 0.0007910564891062677
      - 0.000794693041825667
      - 0.0007446711727728447
      - 0.0007355209139253324
      - 0.0007695089540599535
      - 0.0007424452972675984
      - 0.0007027437968645245
      - 0.0007792211351140091
      - 0.0006947648119724666
      - 0.0007184524680875862
      - 0.0007028970818889017
      - 0.0006807358149671927
      - 0.0007028667556975657
      - 0.0006911619566380978
      - 0.0006650489570650583
      val_losses:
      - 0.0006002901936881244
      - 0.0006411215290427208
      - 0.0006310639437288046
      - 0.0006046935159247369
      - 0.000582789711188525
      - 0.0005699377506971359
      - 0.0005627803911920637
      - 0.0005588122294284403
      - 0.0005565665778703988
      - 0.0005551218637265265
      - 0.0005544874875340611
      - 0.0005540745332837105
      - 0.000553825666429475
      - 0.0005537514807656407
      - 0.0005536876560654491
      - 0.0005535878299269825
      - 0.0005535504315048456
      - 0.0005534004303626716
      - 0.000553412246517837
      - 0.0005535170203074813
      - 0.0005534465017262846
      - 0.0005535098316613585
      - 0.0005534315423574299
      - 0.0005534723750315607
      - 0.0005534767406061292
      - 0.0005534185329452157
      - 0.0005533928924705833
      - 0.0005533902731258422
      - 0.0005533519724849612
      - 0.0005532575014512986
    lstm_h128_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0005532908835448325
      config_name: lstm_h128_lr0.001_d0.4_wd1e-05
      converged: true
      final_train_loss: 0.000672269748368611
      final_val_loss: 0.0005534146912395954
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        mxlkXA+JhT8=
      train_losses:
      - 0.1266727625237157
      - 0.0051534294422405464
      - 0.001813728779476757
      - 0.0013780421286355704
      - 0.0011743715634414305
      - 0.0009221637689430887
      - 0.000872517940782321
      - 0.000863394801854156
      - 0.0007471472878629962
      - 0.0007687520458906268
      - 0.0008952246668438116
      - 0.0007812782860128209
      - 0.0007760281829784313
      - 0.0007240343499385441
      - 0.000737147192315509
      - 0.0007637474142635862
      - 0.0007390867976937443
      - 0.000735528917478708
      - 0.0007280448965805894
      - 0.0006930600017464409
      - 0.0007493578013963997
      - 0.0007095640321495011
      - 0.0006828863261034712
      - 0.0007088480924721807
      - 0.0007161047930518786
      - 0.0006635569152422249
      - 0.000697174109518528
      - 0.0006866447317103545
      - 0.0007026281139890974
      - 0.000672269748368611
      val_losses:
      - 0.0006062981847207993
      - 0.0006434958195313811
      - 0.0006291107565630227
      - 0.0005997165862936527
      - 0.0005769937124568969
      - 0.0005645230994559824
      - 0.0005583587335422635
      - 0.0005557557742577046
      - 0.0005544949090108275
      - 0.0005540446727536619
      - 0.0005538319237530231
      - 0.0005537484830711037
      - 0.0005535322998184711
      - 0.0005533748771995306
      - 0.0005533226358238608
      - 0.0005533699877560139
      - 0.0005534800293389708
      - 0.0005534175434149802
      - 0.0005534450174309313
      - 0.000553449586732313
      - 0.0005534181254915893
      - 0.0005533444345928729
      - 0.0005532908835448325
      - 0.0005533319781534374
      - 0.00055338503443636
      - 0.0005533653020393103
      - 0.0005533812800422311
      - 0.0005533678340725601
      - 0.0005534566298592836
      - 0.0005534146912395954
    lstm_h256_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0005550979403778911
      config_name: lstm_h256_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0010089046166588862
      final_val_loss: 0.0005561165744438767
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        ZahzLu0pRb8=
      train_losses:
      - 0.18181078492974242
      - 0.009883924426200489
      - 0.003093432441043357
      - 0.0018717932204405467
      - 0.0015483745082747191
      - 0.0011947131036625553
      - 0.0013069596122174214
      - 0.0011369702503240358
      - 0.0010319153661839664
      - 0.0011102406327457477
      - 0.0010357321880292147
      - 0.0010089046166588862
      val_losses:
      - 0.0005640013259835541
      - 0.0005550979403778911
      - 0.0005559003911912441
      - 0.0005562777223531157
      - 0.0005564125895034522
      - 0.0005564320599660277
      - 0.000556415761820972
      - 0.000556361221242696
      - 0.0005563116283155978
      - 0.0005562503938563168
      - 0.0005561816215049475
      - 0.0005561165744438767
    lstm_h256_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0005548852495849133
      config_name: lstm_h256_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0010214502738866333
      final_val_loss: 0.0005557483236771077
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        osooKWYRMT8=
      train_losses:
      - 0.1821984457783401
      - 0.010955948188590506
      - 0.0034500069838638106
      - 0.0019165532915697743
      - 0.0015456075974119206
      - 0.001346987400514384
      - 0.0011631651141215116
      - 0.0011883405192444723
      - 0.0011107848355701815
      - 0.0009936691155113901
      - 0.00097290286309241
      - 0.0010214502738866333
      val_losses:
      - 0.0006193351873662323
      - 0.0005548852495849133
      - 0.0005556156393140554
      - 0.0005559338023886085
      - 0.0005560379067901522
      - 0.0005560540885198861
      - 0.0005560198042076081
      - 0.0005559626442845911
      - 0.0005559168930631131
      - 0.0005558642442338169
      - 0.000555801932932809
      - 0.0005557483236771077
    lstm_h256_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0005552433140110224
      config_name: lstm_h256_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.001775986437375347
      final_val_loss: 0.0005583189486060292
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NDvokXHSYD8=
      train_losses:
      - 0.3498688442632556
      - 0.02557107767400642
      - 0.0083929644121478
      - 0.005501748489526411
      - 0.003669176968590667
      - 0.003277531398149828
      - 0.0027799993210161724
      - 0.00223165979453673
      - 0.0022452905929336944
      - 0.0019337115906334172
      - 0.001842734976283585
      - 0.001775986437375347
      val_losses:
      - 0.0008497259696014225
      - 0.0005552433140110224
      - 0.0005565063911490142
      - 0.0005573405651375651
      - 0.000557875056983903
      - 0.0005581971781793982
      - 0.0005583268939517438
      - 0.0005584098398685455
      - 0.0005584160389844328
      - 0.0005583960737567395
      - 0.0005583689780905843
      - 0.0005583189486060292
    lstm_h256_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0005551122303586453
      config_name: lstm_h256_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0016141795495059341
      final_val_loss: 0.0005580389115493745
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        w6sKtTA/fD8=
      train_losses:
      - 0.2675539382422964
      - 0.023144967931633193
      - 0.0092290542088449
      - 0.005136878986377269
      - 0.0039239538212617235
      - 0.0028788360068574548
      - 0.0024769285519141704
      - 0.0021786089540303997
      - 0.002171394075655068
      - 0.0018863289830430101
      - 0.0019040960275257628
      - 0.0016141795495059341
      val_losses:
      - 0.0005772497388534248
      - 0.0005551122303586453
      - 0.000556292972760275
      - 0.0005570861685555428
      - 0.000557567342184484
      - 0.000557847204618156
      - 0.0005580318393185735
      - 0.0005580934521276504
      - 0.0005581091390922666
      - 0.000558100757189095
      - 0.0005580762517638505
      - 0.0005580389115493745
    lstm_h256_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.000556104292627424
      config_name: lstm_h256_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.004968938592355698
      final_val_loss: 0.0005654988926835358
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        bkWIMz5Ldb8=
      train_losses:
      - 0.4734422278900941
      - 0.07381701872994502
      - 0.030566143803298473
      - 0.0179479227711757
      - 0.012779260364671549
      - 0.009568525827489793
      - 0.008044202191134294
      - 0.0072031803817177815
      - 0.006654395760657887
      - 0.0058551223094885545
      - 0.005063516922139873
      - 0.004968938592355698
      val_losses:
      - 0.0006173457077238709
      - 0.000556104292627424
      - 0.0005585983744822443
      - 0.0005605500773526728
      - 0.0005620292504318058
      - 0.0005632283573504537
      - 0.0005641015595756471
      - 0.0005647772049997002
      - 0.0005650362581945956
      - 0.0005652348336298019
      - 0.0005653980188071728
      - 0.0005654988926835358
    lstm_h256_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0005567008629441261
      config_name: lstm_h256_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.004230204057724525
      final_val_loss: 0.000565443595405668
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        TLSm+cuEYr8=
      train_losses:
      - 0.5552184854944547
      - 0.07640926291545232
      - 0.029438293538987637
      - 0.017425139822686713
      - 0.012234425715481242
      - 0.008219171160211166
      - 0.00775359357551982
      - 0.006336390661696593
      - 0.0058690821945977705
      - 0.004661188538496693
      - 0.004834185723060121
      - 0.004230204057724525
      val_losses:
      - 0.0006159830663818866
      - 0.0005567008629441261
      - 0.0005592489033006132
      - 0.0005612182430922985
      - 0.0005626832717098296
      - 0.0005637454451061785
      - 0.0005644849734380841
      - 0.0005650410312227905
      - 0.0005652419349644333
      - 0.0005653482512570918
      - 0.0005654101842083037
      - 0.000565443595405668
    lstm_h256_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.000553299265448004
      config_name: lstm_h256_lr0.0005_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.00065014956635423
      final_val_loss: 0.0005533572693821043
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        wYnxhdWIhj8=
      train_losses:
      - 0.12342159126031522
      - 0.0012997238178892683
      - 0.0007554668554803357
      - 0.0007185952563304454
      - 0.0006989717367105186
      - 0.0006860009937857588
      - 0.0006660895839255924
      - 0.0006820364457477505
      - 0.0006587372433083752
      - 0.0006693267544809108
      - 0.0006593621316521118
      - 0.0006748970966630926
      - 0.0006663424816603462
      - 0.0006520149278609703
      - 0.0006510582801032191
      - 0.0006624410840837905
      - 0.000650500413030386
      - 0.000653859480128934
      - 0.0006617335602641106
      - 0.0006552795433284094
      - 0.0006562847314247241
      - 0.0006544126129786795
      - 0.0006487975139558936
      - 0.000651472340299127
      - 0.0006488886040945848
      - 0.0006501177995232865
      - 0.0006511165605237087
      - 0.0006512042358129596
      - 0.0006546673636573056
      - 0.00065014956635423
      val_losses:
      - 0.0005606067134067416
      - 0.0005661046598106623
      - 0.0005655435961671174
      - 0.0005627264326903969
      - 0.0005598019051831216
      - 0.000557556573767215
      - 0.0005560166900977492
      - 0.0005550060886889696
      - 0.0005543790757656097
      - 0.0005540023848880082
      - 0.0005537036049645394
      - 0.0005535484524443746
      - 0.0005534961237572134
      - 0.0005534568626899272
      - 0.0005534497322514653
      - 0.0005533815128728747
      - 0.0005533598887268454
      - 0.000553359102923423
      - 0.0005533287767320871
      - 0.0005533148651011288
      - 0.000553299265448004
      - 0.0005533404182642698
      - 0.0005533727526199073
      - 0.000553356483578682
      - 0.000553315767319873
      - 0.0005533280782401562
      - 0.0005533401854336262
      - 0.0005533467046916485
      - 0.0005533350922632962
      - 0.0005533572693821043
    lstm_h256_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.0005533118674065918
      config_name: lstm_h256_lr0.0005_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0006463645161905637
      final_val_loss: 0.000553388090338558
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 29
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        ieny2D8jhD8=
      train_losses:
      - 0.0792984443930133
      - 0.0010255001980112866
      - 0.0007577643749148896
      - 0.0006931311509106308
      - 0.0006820188864367083
      - 0.0006769411120330915
      - 0.0006680252699879929
      - 0.0006562710817282399
      - 0.0006716865803658342
      - 0.0006536357541335747
      - 0.0006574858731861847
      - 0.000661190235405229
      - 0.000665687068249099
      - 0.0006561049861678233
      - 0.000651428165535132
      - 0.0006499752053059638
      - 0.0006574384218159443
      - 0.0006495460499233255
      - 0.0006546343890173981
      - 0.0006542829796671867
      - 0.0006527323809374744
      - 0.0006561139258944119
      - 0.0006542408518726006
      - 0.0006476754982334872
      - 0.0006555848328086237
      - 0.0006519538583233953
      - 0.000655558168849287
      - 0.000645654698018916
      - 0.0006463645161905637
      val_losses:
      - 0.000561898312298581
      - 0.0005663958145305514
      - 0.0005650114908348769
      - 0.0005617736314889044
      - 0.0005587905179709196
      - 0.0005566136096604168
      - 0.0005552586517296731
      - 0.000554421596461907
      - 0.0005539561971090734
      - 0.0005537232791539282
      - 0.000553621823200956
      - 0.0005534343945328146
      - 0.000553424411918968
      - 0.000553341320483014
      - 0.0005533633811865002
      - 0.0005533186194952577
      - 0.0005534277006518096
      - 0.0005533831135835499
      - 0.0005533118674065918
      - 0.0005534253432415426
      - 0.0005534050869755447
      - 0.0005533596267923713
      - 0.0005533328512683511
      - 0.0005533626535907388
      - 0.0005533728981390595
      - 0.0005533920484595001
      - 0.0005533987132366747
      - 0.0005533887015189976
      - 0.000553388090338558
    lstm_h256_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0005533400108106434
      config_name: lstm_h256_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0006494400683247173
      final_val_loss: 0.0005534101219382137
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        P5aca4HSgz8=
      train_losses:
      - 0.14017059438629076
      - 0.0020526553174325577
      - 0.0010402713329919304
      - 0.000866897118006212
      - 0.0008000257366802543
      - 0.0007308574228469903
      - 0.0006861186896761259
      - 0.0007148982889096563
      - 0.0006962920548782373
      - 0.0007010435219854116
      - 0.0006920373513518522
      - 0.0006983182781065503
      - 0.0007006521724785367
      - 0.0006901381614928445
      - 0.0006592107674805447
      - 0.000697363405682457
      - 0.000684672961748826
      - 0.0006590895839811614
      - 0.0006885575518632928
      - 0.0006661101651843637
      - 0.0006534142933863526
      - 0.0006682245342138534
      - 0.0006621951082100471
      - 0.0006868559721624479
      - 0.0006710208253934979
      - 0.0006740043560663859
      - 0.0006803941311469922
      - 0.0006720521196257323
      - 0.0006753917356642584
      - 0.0006494400683247173
      val_losses:
      - 0.0005626712809316814
      - 0.0005701250047422945
      - 0.0005691793048754334
      - 0.0005650943785440177
      - 0.0005610978987533599
      - 0.0005580662400461733
      - 0.0005561547004617751
      - 0.0005549890920519829
      - 0.000554282043594867
      - 0.0005538818950299174
      - 0.0005536797980312258
      - 0.0005535790405701846
      - 0.000553574733203277
      - 0.0005534154479391873
      - 0.0005533736548386514
      - 0.000553346995729953
      - 0.0005534318916033953
      - 0.0005533623625524342
      - 0.0005534362280741334
      - 0.0005534283409360796
      - 0.000553416321054101
      - 0.0005534729862120003
      - 0.0005533400108106434
      - 0.0005533632356673479
      - 0.0005533937946893275
      - 0.0005533959483727813
      - 0.0005533756630029529
      - 0.0005534618976525962
      - 0.0005534343654289842
      - 0.0005534101219382137
    lstm_h256_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0005533633229788393
      config_name: lstm_h256_lr0.0005_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0006594670412596315
      final_val_loss: 0.0005533633229788393
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        8HuVRdQkhj8=
      train_losses:
      - 0.16802241614398858
      - 0.0029980192387787006
      - 0.0013752931117778644
      - 0.001091843373918285
      - 0.0008587514651784053
      - 0.0008338349143741652
      - 0.0007747695781290531
      - 0.000768615422809186
      - 0.0007908486683542529
      - 0.0008092334416384498
      - 0.0007630880524326736
      - 0.0007376936118816957
      - 0.0007276508840732276
      - 0.0006890381337143481
      - 0.0006920910236658528
      - 0.0007439966672488177
      - 0.0006848380726296455
      - 0.0006969026677931348
      - 0.0007415515768419331
      - 0.000695731413240234
      - 0.0007117093337001279
      - 0.0006610164952386791
      - 0.0006764812036029374
      - 0.0007003545712602014
      - 0.0006681303639197722
      - 0.000670616621694838
      - 0.0006906516015684853
      - 0.0006769489070090154
      - 0.0006809706343725944
      - 0.0006594670412596315
      val_losses:
      - 0.0005646084027830511
      - 0.0005738802428822964
      - 0.0005741043714806437
      - 0.0005703660135623068
      - 0.0005658520676661283
      - 0.0005620724696200341
      - 0.0005592293746303767
      - 0.0005572237714659423
      - 0.0005558531556744128
      - 0.0005549426423385739
      - 0.0005544083542190492
      - 0.000554047612240538
      - 0.0005538135883398354
      - 0.0005536689423024654
      - 0.000553589517949149
      - 0.0005534706288017333
      - 0.0005534418451134115
      - 0.0005534639058168977
      - 0.000553425430553034
      - 0.0005534531373996288
      - 0.000553487247088924
      - 0.0005534522933885455
      - 0.0005534338997676969
      - 0.0005534200463443995
      - 0.0005534086376428604
      - 0.0005534001102205366
      - 0.0005533879448194057
      - 0.0005533827934414148
      - 0.0005533821822609752
      - 0.0005533633229788393
    lstm_h256_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0005533284565899521
      config_name: lstm_h256_lr0.0005_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0006908761279191822
      final_val_loss: 0.0005533920775633305
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        J0r72lTOhj8=
      train_losses:
      - 0.2105226358398795
      - 0.007552982327373077
      - 0.002337347784002001
      - 0.001391096612981831
      - 0.0010751372271139796
      - 0.0010593527161593859
      - 0.0009426011335259924
      - 0.0009186148915129403
      - 0.0009234703708595285
      - 0.0008909106254577637
      - 0.0009434892514642949
      - 0.0008631745586171746
      - 0.0008795410443174964
      - 0.0007291374252721047
      - 0.0008671435595412428
      - 0.0007436427016121646
      - 0.0008179673701912785
      - 0.0008145267832636213
      - 0.0007940065843285993
      - 0.0008363440380586932
      - 0.0007334675610763952
      - 0.0007020343473413959
      - 0.0007409593818010762
      - 0.00074213974585291
      - 0.0007074824097799137
      - 0.0007276420947164297
      - 0.0007010830595390871
      - 0.000693582730794636
      - 0.0007111800756926338
      - 0.0006908761279191822
      val_losses:
      - 0.0005650410894304514
      - 0.0005780924402642995
      - 0.0005793878517579287
      - 0.0005749348201788962
      - 0.0005691573896910995
      - 0.0005642181204166263
      - 0.0005606153863482177
      - 0.0005581896984949708
      - 0.0005565238825511187
      - 0.0005554339732043445
      - 0.000554839672986418
      - 0.0005543256120290607
      - 0.000554004916921258
      - 0.0005538494733627886
      - 0.0005537131219170988
      - 0.0005536229582503438
      - 0.000553531979676336
      - 0.0005534877709578723
      - 0.000553419697098434
      - 0.0005534444062504917
      - 0.0005534356459975243
      - 0.0005533284565899521
      - 0.0005534005758818239
      - 0.0005534251686185598
      - 0.0005533794756047428
      - 0.0005533634102903306
      - 0.0005533663206733763
      - 0.0005533692892640829
      - 0.0005534043593797833
      - 0.0005533920775633305
    lstm_h256_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0005533219664357603
      config_name: lstm_h256_lr0.0005_d0.4_wd1e-05
      converged: true
      final_train_loss: 0.0007271368182652319
      final_val_loss: 0.0005533590156119317
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        zQJwOmWDgz8=
      train_losses:
      - 0.4068187549710274
      - 0.009154581852878133
      - 0.002631017647217959
      - 0.001674174874400099
      - 0.001172292652578714
      - 0.0010819180752150714
      - 0.001072310488477039
      - 0.0008783434799018627
      - 0.0009785994188860059
      - 0.0009659899612112591
      - 0.0007984128266495342
      - 0.0008519523980794474
      - 0.0007486419684331244
      - 0.0007887189713073894
      - 0.0008140814898069948
      - 0.000835145401651971
      - 0.0006909522683902954
      - 0.0008813280146569014
      - 0.0007469672855222598
      - 0.000726048475674664
      - 0.0007242333376780152
      - 0.0007528731124087548
      - 0.0007417261513182893
      - 0.0007254113249170283
      - 0.0006797601575575148
      - 0.0006972627791886529
      - 0.0006695951500053828
      - 0.0007740333239780739
      - 0.0006923963713537281
      - 0.0007271368182652319
      val_losses:
      - 0.0005650836392305791
      - 0.0005798635247629136
      - 0.0005828132852911949
      - 0.0005786073743365705
      - 0.0005722403002437204
      - 0.0005665272183250636
      - 0.0005622629832942039
      - 0.0005592970410361886
      - 0.000557309394935146
      - 0.0005559845012612641
      - 0.0005550752102863044
      - 0.0005544545128941536
      - 0.0005540945276152343
      - 0.0005539064586628228
      - 0.0005537334072869271
      - 0.0005535465315915644
      - 0.0005535009258892387
      - 0.0005534290976356715
      - 0.0005535132659133524
      - 0.0005534706870093942
      - 0.0005534471711143851
      - 0.0005533840740099549
      - 0.0005534069787245244
      - 0.0005533613730221987
      - 0.0005533219664357603
      - 0.0005533739167731255
      - 0.0005533477233257145
      - 0.0005533754592761397
      - 0.0005534088995773345
      - 0.0005533590156119317
    lstm_h256_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0005533119838219136
      config_name: lstm_h256_lr0.001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0006424559202666084
      final_val_loss: 0.0005533153598662466
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 28
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        5bsh9gRDiD8=
      train_losses:
      - 0.10269222899902768
      - 0.0008009034305966148
      - 0.0007202995960445454
      - 0.0006774285769400498
      - 0.0006684205097068722
      - 0.0006553462493078163
      - 0.0006498752433496217
      - 0.0006485264602815732
      - 0.0006490446151777481
      - 0.0006528213658990959
      - 0.0006466799289531385
      - 0.000644094519278345
      - 0.0006449054053518921
      - 0.0006454108467247958
      - 0.0006509651284432039
      - 0.0006504346092697233
      - 0.0006446684419643134
      - 0.0006452248150405163
      - 0.0006480112933786586
      - 0.0006432343555691963
      - 0.0006425651784714622
      - 0.0006447748407178248
      - 0.0006451666607366254
      - 0.0006434833582413072
      - 0.0006470078321096177
      - 0.0006525116332340986
      - 0.0006480774221320947
      - 0.0006424559202666084
      val_losses:
      - 0.0005778017512056977
      - 0.0005798501661047339
      - 0.0005689496465492994
      - 0.0005599162541329861
      - 0.0005556123505812138
      - 0.0005541032296605408
      - 0.0005536283133551478
      - 0.0005533880612347275
      - 0.0005533667572308332
      - 0.0005534279043786228
      - 0.0005533855292014778
      - 0.0005533524381462485
      - 0.0005533442308660597
      - 0.0005533990042749792
      - 0.0005534176016226411
      - 0.0005533505172934383
      - 0.0005533302610274404
      - 0.0005533119838219136
      - 0.000553355785086751
      - 0.0005533618677873164
      - 0.0005534335505217314
      - 0.0005533868679776788
      - 0.0005533711228054017
      - 0.0005533494986593723
      - 0.0005533602670766413
      - 0.0005533165822271258
      - 0.0005533288058359176
      - 0.0005533153598662466
    lstm_h256_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0005532656505238265
      config_name: lstm_h256_lr0.001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0006468386224393422
      final_val_loss: 0.0005533238872885704
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 25
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        T9gcao7fiz8=
      train_losses:
      - 0.10007257166823062
      - 0.0007910192168007294
      - 0.0006912244474127268
      - 0.0006602194140820453
      - 0.0006498355602767939
      - 0.0006493544109010448
      - 0.0006470609563014781
      - 0.0006638926085239897
      - 0.0006484518138070902
      - 0.0006472665069547171
      - 0.0006445634692984944
      - 0.000647143645134444
      - 0.0006478158708584184
      - 0.000643850847457846
      - 0.0006456549696546668
      - 0.0006451939116232097
      - 0.0006455504675007736
      - 0.000652430123106266
      - 0.0006436848295076439
      - 0.0006462905342535427
      - 0.000645398031338118
      - 0.0006561384167677412
      - 0.0006458194548031315
      - 0.000646844933119913
      - 0.0006468386224393422
      val_losses:
      - 0.0005760320636909455
      - 0.000579607323743403
      - 0.0005701111222151667
      - 0.0005614310794044286
      - 0.0005568575579673052
      - 0.00055467858328484
      - 0.0005538489494938403
      - 0.0005536128010135144
      - 0.0005534241790883243
      - 0.0005533559306059033
      - 0.0005534537485800683
      - 0.0005534549418371171
      - 0.0005534575611818582
      - 0.0005534036899916828
      - 0.0005532656505238265
      - 0.0005533767107408494
      - 0.0005533031362574548
      - 0.0005533939984161407
      - 0.0005534588999580592
      - 0.0005534819792956114
      - 0.0005533666408155113
      - 0.0005533501098398119
      - 0.0005533636140171438
      - 0.0005533851799555123
      - 0.0005533238872885704
    lstm_h256_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.000553219928406179
      config_name: lstm_h256_lr0.001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.0006467487546615303
      final_val_loss: 0.0005533827061299235
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 27
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        W2zwUoZdhj8=
      train_losses:
      - 0.1557251036671611
      - 0.0011376464341689523
      - 0.000760852160359112
      - 0.000689841661369428
      - 0.0006748403490443403
      - 0.000662029788751776
      - 0.0006595051236217842
      - 0.0006602855234329278
      - 0.0006562186172232032
      - 0.0006463522537766645
      - 0.0006563132677304869
      - 0.000662211585828724
      - 0.0006544707236268247
      - 0.0006489855926095819
      - 0.0006459605598744625
      - 0.0006488296930911019
      - 0.0006520435417769477
      - 0.0006498342603057002
      - 0.0006467484054155648
      - 0.0006498766645866757
      - 0.0006443343542438621
      - 0.0006447395765765881
      - 0.0006449264474213123
      - 0.0006469552172347903
      - 0.0006436462305525007
      - 0.0006564245947326223
      - 0.0006467487546615303
      val_losses:
      - 0.0005832334281876683
      - 0.0005919232789892703
      - 0.0005787803384009749
      - 0.0005656910943798721
      - 0.0005585277103818953
      - 0.0005554514355026186
      - 0.0005544093146454543
      - 0.0005538855330087245
      - 0.0005535891395993531
      - 0.0005533915245905519
      - 0.0005534233641810715
      - 0.0005534914380405098
      - 0.0005533319781534374
      - 0.0005533257208298892
      - 0.0005533992953132838
      - 0.0005533655057661235
      - 0.000553219928406179
      - 0.0005533711228054017
      - 0.0005536196695175022
      - 0.0005533654184546322
      - 0.0005533289804589003
      - 0.0005533563380595297
      - 0.0005533714429475367
      - 0.0005533772055059671
      - 0.0005532984796445817
      - 0.0005534051160793751
      - 0.0005533827061299235
    lstm_h256_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.000553262623725459
      config_name: lstm_h256_lr0.001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.000658304556660975
      final_val_loss: 0.0005533459188882262
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 28
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        dkd4NKkShz8=
      train_losses:
      - 0.13658932549878955
      - 0.0009571415672932441
      - 0.0007255476472588877
      - 0.000698868699449425
      - 0.0006654074337954322
      - 0.000659299524462161
      - 0.0006604419904761016
      - 0.0006490563973784447
      - 0.0006502347969217226
      - 0.0006569803081220016
      - 0.0006497367673243085
      - 0.0006597493435644234
      - 0.0006518015191735079
      - 0.0006459892004689513
      - 0.0006470088022372996
      - 0.0006483700854005292
      - 0.0006490675538467864
      - 0.000642905302811414
      - 0.000660979674042513
      - 0.0006578535879574096
      - 0.0006549030610282595
      - 0.0006547544277661169
      - 0.0006583640351891518
      - 0.0006600997730856761
      - 0.0006594504229724407
      - 0.0006560994273362061
      - 0.0006464274095681807
      - 0.000658304556660975
      val_losses:
      - 0.0005793189338874072
      - 0.0005851630994584411
      - 0.0005735585000365973
      - 0.0005628163053188473
      - 0.0005572865484282374
      - 0.0005549027118831873
      - 0.000553950434550643
      - 0.0005535957461688668
      - 0.000553486926946789
      - 0.0005534329975489527
      - 0.0005534139345400035
      - 0.0005533406801987439
      - 0.0005533381190616637
      - 0.0005533409130293876
      - 0.0005533299699891359
      - 0.0005533732764888555
      - 0.0005533573857974261
      - 0.000553262623725459
      - 0.0005533287476282567
      - 0.0005534644587896764
      - 0.0005533916701097041
      - 0.000553354067960754
      - 0.0005534339288715273
      - 0.0005533366638701409
      - 0.0005533564544748515
      - 0.000553356803720817
      - 0.0005533549410756677
      - 0.0005533459188882262
    lstm_h256_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0005532705981750041
      config_name: lstm_h256_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0006571782966299603
      final_val_loss: 0.000553324818611145
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        B59YoogtiD8=
      train_losses:
      - 0.20282799059835574
      - 0.002026741043664515
      - 0.0009772493455481406
      - 0.0007884175089808801
      - 0.0007043837055486316
      - 0.0006937406093735868
      - 0.0006937509897397831
      - 0.0006756292277714238
      - 0.0007331507804337889
      - 0.0006826946240228912
      - 0.000709962002777805
      - 0.0006832802707018951
      - 0.0006810888250280792
      - 0.0006673534468670065
      - 0.0006827879357539738
      - 0.0006559015843473995
      - 0.0006929631150948504
      - 0.0006545013117526347
      - 0.0006801544756550962
      - 0.0007095451098090658
      - 0.0006711083163584893
      - 0.0006649718949726472
      - 0.0006928124833696833
      - 0.0006512526645868396
      - 0.0006661883526248857
      - 0.0006897815959140038
      - 0.0006601791683351621
      - 0.0006721189032153537
      - 0.0006468045078994086
      - 0.0006571782966299603
      val_losses:
      - 0.0005860512901563197
      - 0.0005992937367409468
      - 0.0005854125192854553
      - 0.00056956906337291
      - 0.0005600878212135285
      - 0.0005558611883316189
      - 0.0005542555300053209
      - 0.0005536692624446005
      - 0.0005535336385946721
      - 0.0005534593656193465
      - 0.0005533966177608818
      - 0.0005534001102205366
      - 0.0005534529918804765
      - 0.000553533056518063
      - 0.0005533551739063114
      - 0.0005533448420464993
      - 0.0005534631491173059
      - 0.0005533475778065622
      - 0.000553399178897962
      - 0.0005534067458938807
      - 0.0005533264484256506
      - 0.0005533404473681003
      - 0.0005532705981750041
      - 0.000553334248252213
      - 0.0005533704243134707
      - 0.0005533208313863724
      - 0.0005533869552891701
      - 0.000553338642930612
      - 0.0005533412040676922
      - 0.000553324818611145
    lstm_h256_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0005532648647204041
      config_name: lstm_h256_lr0.001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0006557839321127782
      final_val_loss: 0.0005533619842026383
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 27
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        3ZY1pv+NjD8=
      train_losses:
      - 0.3093705077189952
      - 0.002554199988177667
      - 0.0010392685896173741
      - 0.0008176443661795929
      - 0.0007411222128818432
      - 0.0007044079053836564
      - 0.0007065650812971095
      - 0.0006777106270116443
      - 0.0006881026735451693
      - 0.0007072374670921514
      - 0.0007076028899367278
      - 0.0006696336107173314
      - 0.0006661112662792826
      - 0.0006757335553023344
      - 0.0006504948881532376
      - 0.0006556404114235193
      - 0.0006573213031515479
      - 0.0006752689757073919
      - 0.0006464845452380056
      - 0.0006471631883565957
      - 0.0006452324936011186
      - 0.000677419695421122
      - 0.0006514201862349486
      - 0.0006532526264588038
      - 0.000701870225990812
      - 0.0006683488609269261
      - 0.0006557839321127782
      val_losses:
      - 0.0005816441844217479
      - 0.0005979478009976447
      - 0.0005841531092301011
      - 0.0005681950715370476
      - 0.000559543666895479
      - 0.0005558799603022635
      - 0.0005544562882278115
      - 0.0005537726392503828
      - 0.0005535304953809828
      - 0.0005535735690500587
      - 0.000553458376089111
      - 0.000553328834939748
      - 0.000553420017240569
      - 0.0005533654766622931
      - 0.0005534783995244652
      - 0.0005533468793146312
      - 0.0005532648647204041
      - 0.0005534041847568005
      - 0.0005534359661396593
      - 0.0005533390794880688
      - 0.00055336017976515
      - 0.0005534722586162388
      - 0.0005534579395316541
      - 0.0005534539523068815
      - 0.0005534659721888602
      - 0.0005533237126655877
      - 0.0005533619842026383
    lstm_h64_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0005655919085256755
      config_name: lstm_h64_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0019462549438079197
      final_val_loss: 0.0005697647284250706
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 17
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        mFclzxguez8=
      train_losses:
      - 0.07652020920068026
      - 0.025914579164236784
      - 0.014718461238468686
      - 0.009015325922518969
      - 0.006464255740866065
      - 0.00523334132352223
      - 0.003640706689717869
      - 0.003618093265686184
      - 0.0033149085162828365
      - 0.0030045006424188614
      - 0.002461829484673217
      - 0.002045837686940407
      - 0.002525946571646879
      - 0.00197250063259465
      - 0.002048929328642165
      - 0.0018877042748499662
      - 0.0019462549438079197
      val_losses:
      - 0.0072100290562957525
      - 0.0019827065407298505
      - 0.0007097993802744895
      - 0.00062446782249026
      - 0.000589351897360757
      - 0.0005656093417201191
      - 0.0005655919085256755
      - 0.0005666269280482084
      - 0.0005675487627740949
      - 0.0005684119823854417
      - 0.0005690664984285831
      - 0.0005694464198313653
      - 0.0005696167645510286
      - 0.0005697380111087114
      - 0.0005697723536286503
      - 0.0005697754677385092
      - 0.0005697647284250706
    lstm_h64_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0005594594695139676
      config_name: lstm_h64_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0017090251591677468
      final_val_loss: 0.0005638436414301395
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        mphNHhuHdD8=
      train_losses:
      - 0.1759100817143917
      - 0.03774121031165123
      - 0.013970302495484551
      - 0.0068999544794981675
      - 0.004673571519864102
      - 0.003566972620319575
      - 0.0028295503580011427
      - 0.0025082693900913
      - 0.002011418303785225
      - 0.0020043328016375503
      - 0.0018414312896008294
      - 0.0019033025794972975
      - 0.0017090251591677468
      val_losses:
      - 0.01832046266645193
      - 0.0017647049389779568
      - 0.0005594594695139676
      - 0.000560237531317398
      - 0.0005614770343527198
      - 0.0005622650496661663
      - 0.0005628335929941386
      - 0.0005632639222312719
      - 0.0005635495472233742
      - 0.000563658046303317
      - 0.0005637419235426933
      - 0.0005637973663397133
      - 0.0005638436414301395
    lstm_h64_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0005593040550593287
      config_name: lstm_h64_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.004662119531227897
      final_val_loss: 0.0005785244284197688
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        OO2z8jJBZL8=
      train_losses:
      - 0.2527684016774098
      - 0.04175254205862681
      - 0.02535282587632537
      - 0.015633388344819348
      - 0.012475528831904134
      - 0.009159614060384532
      - 0.007817639891679088
      - 0.00674076455955704
      - 0.005875236742819349
      - 0.005210096715018153
      - 0.005497542617376894
      - 0.005225255968980491
      - 0.004662119531227897
      val_losses:
      - 0.009308743756264448
      - 0.000816126266727224
      - 0.0005593040550593287
      - 0.0005624229088425636
      - 0.0005654800334013999
      - 0.0005682862247340381
      - 0.0005708335665985942
      - 0.0005730815755669028
      - 0.0005751471326220781
      - 0.0005760411731898785
      - 0.0005768804694525898
      - 0.0005777127807959914
      - 0.0005785244284197688
    lstm_h64_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0005624224431812763
      config_name: lstm_h64_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.005810844828374684
      final_val_loss: 0.0005745594680774957
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        OPWJ5ZFrUr8=
      train_losses:
      - 0.24766878162821135
      - 0.09323315043002367
      - 0.038548242300748825
      - 0.023766922298818827
      - 0.015693263461192448
      - 0.01185238741648694
      - 0.010876060153047243
      - 0.008641083802406987
      - 0.008214472870652875
      - 0.007158186635933816
      - 0.006643653692056735
      - 0.005799990613013506
      - 0.006078384642023593
      - 0.005810844828374684
      val_losses:
      - 0.02815366256982088
      - 0.004823409719392657
      - 0.0005978201807010919
      - 0.0005624224431812763
      - 0.0005646359932143241
      - 0.000566503731533885
      - 0.0005682095070369542
      - 0.0005696714797522873
      - 0.0005711349658668041
      - 0.000572510325582698
      - 0.0005730833217967302
      - 0.0005735861486755311
      - 0.0005740589986089617
      - 0.0005745594680774957
    lstm_h64_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.0005656358553096652
      config_name: lstm_h64_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.009833679030028483
      final_val_loss: 0.0005920731055084616
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        dePTQvEqcD8=
      train_losses:
      - 0.30077703421314556
      - 0.1049284723897775
      - 0.060190544463694096
      - 0.03894410297895471
      - 0.03175986356412371
      - 0.024191219670077164
      - 0.01927891094237566
      - 0.014773608651012182
      - 0.015150790878882011
      - 0.012073917081579566
      - 0.011741824060057601
      - 0.010379050392657518
      - 0.010585322820891937
      - 0.009833679030028483
      val_losses:
      - 0.019169828854501247
      - 0.0014380571665242314
      - 0.0007382013718597591
      - 0.0005656358553096652
      - 0.000569518277188763
      - 0.0005735352169722319
      - 0.0005772346921730787
      - 0.0005807404522784054
      - 0.0005839393415953964
      - 0.0005868597945664078
      - 0.0005882555851712823
      - 0.000589550007134676
      - 0.0005908579041715711
      - 0.0005920731055084616
    lstm_h64_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0005643281619995832
      config_name: lstm_h64_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.011447626166045666
      final_val_loss: 0.0005907497543375939
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        9FRyZPheLb8=
      train_losses:
      - 0.23054908340175947
      - 0.09211312234401703
      - 0.06211830706646045
      - 0.04259077397485574
      - 0.03287543418506781
      - 0.02557499830921491
      - 0.022095409221947193
      - 0.0182860071460406
      - 0.01585287367925048
      - 0.014829996740445495
      - 0.013973666587844491
      - 0.014000174046183625
      - 0.012879892097165188
      - 0.011447626166045666
      val_losses:
      - 0.011235019192099571
      - 0.0013829632080160081
      - 0.0006811542261857539
      - 0.0005643281619995832
      - 0.0005681994662154466
      - 0.0005718730099033564
      - 0.0005754876765422523
      - 0.0005788857815787196
      - 0.0005819976795464754
      - 0.00058502092724666
      - 0.000586499780183658
      - 0.0005879098025616258
      - 0.000589388539083302
      - 0.0005907497543375939
    lstm_h64_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.0005536894605029374
      config_name: lstm_h64_lr0.0005_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0007588847171670446
      final_val_loss: 0.0005536894605029374
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        L8BWmTT+iz8=
      train_losses:
      - 0.04354299767874181
      - 0.004237459856085479
      - 0.001863486007399236
      - 0.001372891429734106
      - 0.001108048595294046
      - 0.0010545357266285766
      - 0.0009696375006266559
      - 0.0009108556162876388
      - 0.0009518036677036434
      - 0.0008872338754978651
      - 0.0008343727192065368
      - 0.0008600095752626657
      - 0.000856844344525598
      - 0.0008674108976265416
      - 0.0008468155671531955
      - 0.000785297917900607
      - 0.0008996091734540338
      - 0.00076398225792218
      - 0.0007763389003230259
      - 0.0007865130804323902
      - 0.0007660456515926247
      - 0.0007957541383802891
      - 0.000808699105012541
      - 0.0007412519141022736
      - 0.0007084734097588807
      - 0.0007036580306400234
      - 0.000713685246106858
      - 0.0007244682832000157
      - 0.0007044505327939987
      - 0.0007588847171670446
      val_losses:
      - 0.0005656147841364145
      - 0.0005835624469909817
      - 0.0005916937079746276
      - 0.0005906517908442765
      - 0.0005851011665072292
      - 0.0005786071415059268
      - 0.000572992634261027
      - 0.0005705536459572613
      - 0.0005684526404365897
      - 0.0005664819036610425
      - 0.0005646392528433353
      - 0.0005629914521705359
      - 0.0005616106209345162
      - 0.0005603408790193498
      - 0.0005593349051196128
      - 0.0005583796591963619
      - 0.000557635969016701
      - 0.0005570222274400294
      - 0.0005564161401707679
      - 0.0005558686680160463
      - 0.0005555123498197645
      - 0.0005551488138735294
      - 0.0005549410998355597
      - 0.0005546481406781822
      - 0.0005544347513932735
      - 0.000554217491298914
      - 0.000554024096345529
      - 0.0005538806726690382
      - 0.0005538142286241055
      - 0.0005536894605029374
    lstm_h64_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.0005539917619898915
      config_name: lstm_h64_lr0.0005_d0.2_wd1e-05
      converged: true
      final_train_loss: 0.0007180790465402728
      final_val_loss: 0.0005539917619898915
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        H6oXJwIhgD8=
      train_losses:
      - 0.10797874377264331
      - 0.003466103497582177
      - 0.0015854727729068447
      - 0.001275482897957166
      - 0.0009279372510112202
      - 0.0009055602713488042
      - 0.0008465539091654742
      - 0.0008471111019995684
      - 0.0009268771633893872
      - 0.0008065290846085796
      - 0.0007489990966860205
      - 0.000778716896699431
      - 0.0007900366860364253
      - 0.0007525464898208156
      - 0.0007615882495883852
      - 0.0007785101867436121
      - 0.0007610873387117559
      - 0.0007159775365532065
      - 0.0007426921074511483
      - 0.0007317894196603447
      - 0.0007234385217695186
      - 0.0007024556107353419
      - 0.0007512819962964082
      - 0.0007062416116241366
      - 0.0007277614155706639
      - 0.0007603208602328474
      - 0.0007389496264901633
      - 0.000740280263319922
      - 0.0007180223765317351
      - 0.0007180790465402728
      val_losses:
      - 0.0005665728822350502
      - 0.0005845634150318801
      - 0.0005900964024476707
      - 0.0005871538305655122
      - 0.0005810144648421556
      - 0.0005746958777308464
      - 0.0005693572165910155
      - 0.0005671977705787867
      - 0.0005654882697854191
      - 0.0005638966686092317
      - 0.0005623757024295628
      - 0.0005610583757515997
      - 0.0005599238211289048
      - 0.000558997766347602
      - 0.0005582026788033545
      - 0.0005575358809437603
      - 0.0005569372733589262
      - 0.0005564202729146928
      - 0.0005560091522056609
      - 0.0005556131945922971
      - 0.0005553206137847155
      - 0.0005550478817895055
      - 0.0005548525077756494
      - 0.0005546332977246493
      - 0.0005544877785723656
      - 0.0005543779698200524
      - 0.0005542502040043473
      - 0.0005541548889596015
      - 0.0005540826823562384
      - 0.0005539917619898915
    lstm_h64_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0005537605029530823
      config_name: lstm_h64_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007765072659822181
      final_val_loss: 0.0005537605029530823
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        vVl8+8qXgT8=
      train_losses:
      - 0.04662863281555474
      - 0.005022046233837803
      - 0.0022230630080836513
      - 0.0016792183644914378
      - 0.0013035271064533542
      - 0.0013509053727223848
      - 0.001153473654994741
      - 0.001144595963220733
      - 0.0010682991899860401
      - 0.0010338651190977544
      - 0.0010704598971642554
      - 0.0010082010170056794
      - 0.0008965682694300389
      - 0.000964479879864181
      - 0.0009847638866631314
      - 0.0007662278464219222
      - 0.0009139158549563339
      - 0.0008280152008713534
      - 0.0007843637771050757
      - 0.00085406334255822
      - 0.0008365454074616233
      - 0.0007664455091192698
      - 0.0008512718923157081
      - 0.0007665940841737514
      - 0.0007771701057208702
      - 0.0008135031239362434
      - 0.000879646865845037
      - 0.0007219158336132144
      - 0.0007350661714250842
      - 0.0007765072659822181
      val_losses:
      - 0.0005665067001245916
      - 0.0005870633758604527
      - 0.0005951393977738917
      - 0.0005914251669310033
      - 0.0005832654132973403
      - 0.0005754827870987356
      - 0.0005694201099686325
      - 0.0005669773963745683
      - 0.0005647727521136403
      - 0.0005629796360153705
      - 0.0005613727553281933
      - 0.0005600782460533082
      - 0.0005589143838733435
      - 0.0005580902507063001
      - 0.0005573537782765925
      - 0.0005566728359553963
      - 0.0005560545541811734
      - 0.0005555497773457319
      - 0.0005551977956201881
      - 0.0005548175831791013
      - 0.0005546075699385256
      - 0.0005543831503018737
      - 0.0005542001163121313
      - 0.0005540528800338507
      - 0.0005539059056900442
      - 0.0005538709519896656
      - 0.0005538841942325234
      - 0.000553909019799903
      - 0.0005537938268389553
      - 0.0005537605029530823
    lstm_h64_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0005718789470847696
      config_name: lstm_h64_lr0.0005_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.001444666234116691
      final_val_loss: 0.0005790842114947736
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NSRNb2OgdT8=
      train_losses:
      - 0.10571574776743849
      - 0.010966380243189633
      - 0.004720957794537147
      - 0.003424514608923346
      - 0.0025460470448403307
      - 0.002134753546367089
      - 0.0017784218265054126
      - 0.0017816200755381335
      - 0.0015896932697311665
      - 0.0016035061077369999
      - 0.001444666234116691
      val_losses:
      - 0.0005718789470847696
      - 0.0005923058197367936
      - 0.0006040903681423515
      - 0.0006059940496925265
      - 0.0006027757481206208
      - 0.0005970859783701599
      - 0.000590907409787178
      - 0.0005877875373698771
      - 0.000584691995754838
      - 0.000581835862249136
      - 0.0005790842114947736
    lstm_h64_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0005951698985882103
      config_name: lstm_h64_lr0.0005_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.002093389843745778
      final_val_loss: 0.0006028646603226662
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        k8Otz2jPc78=
      train_losses:
      - 0.16089070697004595
      - 0.018665471812710166
      - 0.007863921656583747
      - 0.005446678434964269
      - 0.003968979911102603
      - 0.003636806310775379
      - 0.00335720885777846
      - 0.0028236512055930993
      - 0.0022954763165519885
      - 0.002158720288813735
      - 0.002093389843745778
      val_losses:
      - 0.0005951698985882103
      - 0.0006099505990277976
      - 0.0006383667059708387
      - 0.0006495978450402617
      - 0.0006464906909968704
      - 0.0006376237142831087
      - 0.0006274684274103492
      - 0.0006217059271875769
      - 0.0006153341964818537
      - 0.0006088696245569736
      - 0.0006028646603226662
    lstm_h64_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0005740210472140461
      config_name: lstm_h64_lr0.0005_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0026126836407153555
      final_val_loss: 0.0006064715271349996
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        1GnDP9K1fr8=
      train_losses:
      - 0.15316199123238525
      - 0.01966460848537584
      - 0.009775478664475182
      - 0.006908358365762979
      - 0.004391035821754485
      - 0.00400460676367705
      - 0.0039038690738379955
      - 0.0033079758674527207
      - 0.0028537754454494766
      - 0.002301285140371571
      - 0.0026126836407153555
      val_losses:
      - 0.0005740210472140461
      - 0.0006080815801396966
      - 0.0006328406452666968
      - 0.0006432419759221375
      - 0.0006411293579731137
      - 0.000634190218988806
      - 0.000626738736173138
      - 0.000622197927441448
      - 0.0006173281581141055
      - 0.0006116614094935358
      - 0.0006064715271349996
    lstm_h64_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0005532342765945941
      config_name: lstm_h64_lr0.001_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0006682657403871417
      final_val_loss: 0.0005533758667297661
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        yHzn20LThT8=
      train_losses:
      - 0.06340499092281486
      - 0.001392611763246047
      - 0.0009133243826606
      - 0.0008263363076063494
      - 0.0007689199943949158
      - 0.0007604057997620354
      - 0.0007247946584053958
      - 0.000674323610534581
      - 0.000706326095193314
      - 0.0007351741417854404
      - 0.0007134630965689818
      - 0.0007056729082250968
      - 0.0006779344839742407
      - 0.0007054956804495305
      - 0.0006686126483449092
      - 0.0006677860073978081
      - 0.0006631670888358107
      - 0.000664941825865147
      - 0.0006860013769861931
      - 0.0006844408975060409
      - 0.00066196079327104
      - 0.0006450304548100879
      - 0.0006675495290740704
      - 0.0006680190369176368
      - 0.0006554824067279696
      - 0.0006502934168869009
      - 0.0006627929736472046
      - 0.0006518393978088474
      - 0.0006700308197954049
      - 0.0006682657403871417
      val_losses:
      - 0.000590807234402746
      - 0.0006130672700237483
      - 0.0006041419692337513
      - 0.0005870648601558059
      - 0.0005733160651288927
      - 0.0005644816264975816
      - 0.0005595912807621062
      - 0.0005567751359194517
      - 0.0005552612419705838
      - 0.0005544485175050795
      - 0.0005540325364563614
      - 0.0005538234545383602
      - 0.0005536455428227782
      - 0.0005534972005989403
      - 0.0005536535754799843
      - 0.0005535120435524732
      - 0.0005533569492399693
      - 0.0005533858202397823
      - 0.0005533721414394677
      - 0.0005534989759325981
      - 0.0005535304080694914
      - 0.0005534149822779
      - 0.0005532342765945941
      - 0.0005534574447665364
      - 0.0005534054362215102
      - 0.00055337292724289
      - 0.0005533242656383663
      - 0.0005534299125429243
      - 0.0005534138472285122
      - 0.0005533758667297661
    lstm_h64_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0005533255753107369
      config_name: lstm_h64_lr0.001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0006553206233850991
      final_val_loss: 0.0005533952207770199
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 23
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        KQVSzH4rhT8=
      train_losses:
      - 0.028217241720994934
      - 0.0010900549871924643
      - 0.0007882264423339317
      - 0.0007058622237915794
      - 0.0007397323449064667
      - 0.00069307205073225
      - 0.0006762785196769983
      - 0.000682503562226581
      - 0.000678152641436706
      - 0.0006671991723123938
      - 0.0006676171954798823
      - 0.0006803751700014496
      - 0.0006726035693039497
      - 0.000664146626756216
      - 0.000668785156449303
      - 0.0006600691170509284
      - 0.0006627343779352183
      - 0.0006609553480908895
      - 0.0006562190149755528
      - 0.000649430953975146
      - 0.0006689562433166429
      - 0.0006488412036560476
      - 0.0006553206233850991
      val_losses:
      - 0.0005872194014955312
      - 0.0005984020826872438
      - 0.0005836817726958543
      - 0.0005682300252374262
      - 0.0005595301336143166
      - 0.000555818056454882
      - 0.0005543449660763144
      - 0.0005537680990528315
      - 0.0005535014206543565
      - 0.0005535148666240275
      - 0.0005533573275897652
      - 0.0005534481606446207
      - 0.0005533255753107369
      - 0.0005533751391340047
      - 0.0005535180098377168
      - 0.0005534261872526258
      - 0.0005534336960408837
      - 0.000553354766452685
      - 0.0005533282237593085
      - 0.0005534073861781508
      - 0.0005533469375222921
      - 0.0005533933581318706
      - 0.0005533952207770199
    lstm_h64_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.0005532827053684741
      config_name: lstm_h64_lr0.001_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0006818670177987466
      final_val_loss: 0.0005533939984161407
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        MjxjuryshD8=
      train_losses:
      - 0.04329840264593562
      - 0.0015502739164124553
      - 0.0009793885401450098
      - 0.0009227681875927374
      - 0.0007812579812404389
      - 0.0008616695946936185
      - 0.000893611654949685
      - 0.0007713174854870886
      - 0.000804013708451142
      - 0.0007141317813269173
      - 0.0007403917503931249
      - 0.0006934577007389938
      - 0.0006963097306046014
      - 0.0006948112956403444
      - 0.000713591017605116
      - 0.000680609048383
      - 0.0006944268679944798
      - 0.0006715853184383983
      - 0.0007101096998667344
      - 0.0006960672326385975
      - 0.0006704206704550112
      - 0.0006698256135374928
      - 0.0006909496247923622
      - 0.0006584856503953537
      - 0.0006986285928481569
      - 0.0006626688749141371
      - 0.000700129351268212
      - 0.0006568589402983586
      - 0.0006647511812237402
      - 0.0006818670177987466
      val_losses:
      - 0.0005903583660256118
      - 0.0006024684407748282
      - 0.0005879964446648955
      - 0.0005719243781641126
      - 0.0005621136224362999
      - 0.0005574110837187618
      - 0.0005555175303015858
      - 0.0005542702274397016
      - 0.0005537250835914165
      - 0.0005534767697099596
      - 0.000553545483853668
      - 0.0005534178926609457
      - 0.0005533472867682576
      - 0.0005534749652724713
      - 0.0005534566880669445
      - 0.000553510122699663
      - 0.0005534006922971457
      - 0.0005533155053853989
      - 0.0005534004303626716
      - 0.0005533607618417591
      - 0.0005532827053684741
      - 0.0005533704243134707
      - 0.0005533922812901437
      - 0.0005533280200324953
      - 0.0005533296789508313
      - 0.0005533005751203746
      - 0.000553484249394387
      - 0.0005534037481993437
      - 0.0005534181254915893
      - 0.0005533939984161407
    lstm_h64_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.0005532721988856792
      config_name: lstm_h64_lr0.001_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0006637140031671152
      final_val_loss: 0.000553336285520345
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        GKHeMctvkD8=
      train_losses:
      - 0.050000170400987066
      - 0.002195238407390813
      - 0.0013855427678208798
      - 0.0010283025621902198
      - 0.0009300605403647447
      - 0.0008731402388851469
      - 0.0007929227382798368
      - 0.000783849332947284
      - 0.000822878767697451
      - 0.0008301659108838066
      - 0.0007765061479100647
      - 0.0007381040049949661
      - 0.0007467479251014689
      - 0.0007364517902412141
      - 0.0007197822075492392
      - 0.0007571179012302309
      - 0.0007162517722463235
      - 0.0007066130783641711
      - 0.0007039421810380494
      - 0.0007114228598463038
      - 0.0006819309298104296
      - 0.0007058037930012991
      - 0.0006652730808127671
      - 0.0006562212317173058
      - 0.0007065532796938593
      - 0.0006787084760920455
      - 0.000680870643312422
      - 0.0006989492345989371
      - 0.0006687501057361563
      - 0.0006637140031671152
      val_losses:
      - 0.0005966869939584285
      - 0.0006241257360670716
      - 0.0006104066560510546
      - 0.0005874044436495751
      - 0.0005712470738217235
      - 0.0005624406912829727
      - 0.0005578445852734149
      - 0.000555526843527332
      - 0.0005543797160498798
      - 0.000553922465769574
      - 0.0005537857068702579
      - 0.0005537318938877434
      - 0.0005537789547815919
      - 0.0005534957745112479
      - 0.0005535653326660395
      - 0.0005534527590498328
      - 0.0005535649543162435
      - 0.000553583464352414
      - 0.0005535582604352385
      - 0.000553406192921102
      - 0.0005534090159926564
      - 0.000553480233065784
      - 0.0005535144591704011
      - 0.0005534226365853101
      - 0.0005532721988856792
      - 0.0005534888769034296
      - 0.000553418358322233
      - 0.0005534202209673822
      - 0.0005534134688787162
      - 0.000553336285520345
    lstm_h64_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0005532902432605624
      config_name: lstm_h64_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0007141185827398052
      final_val_loss: 0.0005534389638341963
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        SybKFhCFhD8=
      train_losses:
      - 0.10432181243474285
      - 0.005939969637741645
      - 0.0034060754211774715
      - 0.0018609832477523014
      - 0.0018586677421505253
      - 0.001338653465306076
      - 0.001462724578838485
      - 0.0011361864211115365
      - 0.0011806329857790843
      - 0.0009962321588924776
      - 0.000887741960468702
      - 0.0010094593453686684
      - 0.0008482872993530085
      - 0.0008919636311475188
      - 0.0009248977391204486
      - 0.0008303846310203274
      - 0.0008951912410945321
      - 0.0008262784200875709
      - 0.0007321332765665526
      - 0.0007970287842908874
      - 0.0007799185599045207
      - 0.0007186047247766206
      - 0.0007461634425756832
      - 0.0007756577688269317
      - 0.000741296069463715
      - 0.0007666607173935821
      - 0.0007990744343260303
      - 0.0006943038267005855
      - 0.0007561041954128692
      - 0.0007141185827398052
      val_losses:
      - 0.0006041945307515562
      - 0.0006536459259223193
      - 0.0006524761265609413
      - 0.0006250795559026301
      - 0.0005988530174363405
      - 0.0005812306189909577
      - 0.0005696864100173116
      - 0.0005635193665511906
      - 0.0005598080169875175
      - 0.000557513878447935
      - 0.0005559998098760843
      - 0.0005550463683903217
      - 0.0005547664186451584
      - 0.000554281665245071
      - 0.0005539116100408137
      - 0.0005537887045647949
      - 0.0005538274126593024
      - 0.0005538004916161299
      - 0.0005536745302379131
      - 0.000553672929527238
      - 0.0005537196702789515
      - 0.0005535730451811105
      - 0.0005536287499126047
      - 0.0005536049429792911
      - 0.0005536086100619286
      - 0.0005535743548534811
      - 0.0005534588708542287
      - 0.0005533884395845234
      - 0.0005532902432605624
      - 0.0005534389638341963
    lstm_h64_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0005533275252673775
      config_name: lstm_h64_lr0.001_d0.4_wd1e-05
      converged: true
      final_train_loss: 0.0007092333398759365
      final_val_loss: 0.0005535027594305575
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        cuMxZ8sahj8=
      train_losses:
      - 0.10212050921594103
      - 0.007430826857065161
      - 0.0031711082459272197
      - 0.0023780799917100617
      - 0.0016963205998763442
      - 0.0016137450099146615
      - 0.0013627021010809888
      - 0.0012956795932647462
      - 0.0011516365678592895
      - 0.0009171651909127831
      - 0.0010892035885869216
      - 0.0011941100674448535
      - 0.0010199186266011868
      - 0.0009202609653584659
      - 0.0008305777300847694
      - 0.0009059010917553678
      - 0.0008772897660188997
      - 0.0008315358233327667
      - 0.0008257734637785082
      - 0.0009635400104646882
      - 0.000744613993447274
      - 0.0008233232462468246
      - 0.0007549571989026541
      - 0.0007997061232648169
      - 0.0008621435117674991
      - 0.00073744547747386
      - 0.0007538379480441412
      - 0.0007336095150094479
      - 0.0007549424966176351
      - 0.0007092333398759365
      val_losses:
      - 0.0006143182690721005
      - 0.0006842183938715607
      - 0.0006920836167410016
      - 0.0006611827702727169
      - 0.0006251099111977965
      - 0.0005975051026325673
      - 0.0005800704238936305
      - 0.0005694636492989957
      - 0.0005630618834402412
      - 0.0005592008237726986
      - 0.0005569489439949393
      - 0.000555978185730055
      - 0.0005553002119995654
      - 0.0005547017499338835
      - 0.0005540759884752333
      - 0.0005538813711609691
      - 0.0005536376847885549
      - 0.0005536424578167498
      - 0.0005535676900763065
      - 0.0005536987155210227
      - 0.0005537555844057351
      - 0.0005535277305170894
      - 0.0005535371601581573
      - 0.0005534311640076339
      - 0.0005536719691008329
      - 0.0005535430682357401
      - 0.0005534414085559547
      - 0.0005533275252673775
      - 0.0005535737727768719
      - 0.0005535027594305575
  best_config: lstm_h256_lr0.001_d0.3_wd0.0001
  best_val_loss: 0.000553219928406179
  hyperparameter_analysis:
    dropout:
      '0.2':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5DiOK9AuQj8=
        num_configs: 18
      '0.3':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          VVVVzXE2Qj8=
        num_configs: 18
      '0.4':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5DiOk8VdQj8=
        num_configs: 18
    hidden_size:
      '128':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          chzHmfE4Qj8=
        num_configs: 18
      '256':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5DiO+50nQj8=
        num_configs: 18
      '64':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          x3Ec93diQj8=
        num_configs: 18
    learning_rate:
      '0.0001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          q6qq/rhDQj8=
        num_configs: 36
      '0.0005':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5DiOQ0pXQj8=
        num_configs: 18
      '0.001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          x3Ec/0IhQj8=
        num_configs: 18
    weight_decay:
      '0.0001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          q6qq/rhDQj8=
        num_configs: 36
      1e-05:
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          TGgvQRZAQj8=
        num_configs: 27
  num_configs_tested: 54
  successful_configs: 54
training_period:
  end: '2024-12-30'
  start: '2010-04-01'
training_summary:
  input_features: 1200
  num_assets: 200
  sequence_length: 60
  total_sequences: 3712
  training_sequences: 754
  validation_sequences: 251
