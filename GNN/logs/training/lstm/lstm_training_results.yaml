assets_used:
- SMTC
- RRX
- SABR
- REXR
- LII
- NVR
- FLO
- XPO
- EWBC
- ENS
- ALE
- OMI
- EGP
- KMPR
- WEN
- ILG
- BDC
- SON
- OHI
- TRMB
- PPC
- OZK
- MRCY
- LEA
- CSC
- PVH
- OLN
- CW
- SBRA
- BYD
- GHC
- HCSG
- KRG
- FRT
- LEG
- BBBY
- KMT
- TDS
- RLI
- FOSL
- GME
- MOH
- TDY
- DHC
- VC
- MAC
- MCY
- UDR
- MUR
- MAT
- VNO
- LNT
- WHR
- ENPH
- IRDM
- CRL
- ACHC
- BC
- JEF
- VSH
- POOL
- MTDR
- SR
- CNX
- AMD
- CG
- ARE
- AJG
- GEF
- FHN
- TTC
- FCN
- NYT
- SIGI
- GAP
- CALX
- CLB
- SMG
- OPCH
- LFUS
- ZD
- STRA
- NLY
- DKS
- XRAY
- STAG
- CIEN
- STLD
- SWX
- ATI
- WOLF
- SIG
- MAN
- APPS
- SFM
- M
- NAN
- AEO
- WGL
- RGLD
- EAT
- CUBE
- RGA
- DDS
- WPC
- AZTA
- OGS
- AAON
- RAMP
- WWD
- AMED
- TNL
- G
- MSCC
- CSL
- ATW
- AMH
- VLY
- RMD
- IBKR
- IBOC
- ICUI
- LKQ
- ANSS
- NBR
- CRI
- DY
- FBIN
- ESNT
- BRKR
- TCBI
- ILMN
- PTC
- GVA
- PENN
- NEU
- INGN
- WTRG
- RJF
- TTWO
- HOG
- SVU
- BURL
- GPN
- ALB
- FICO
- THC
- FLS
- SAM
- NOV
- BMRN
- ITT
- NWE
- CLC
- HIW
- NTCT
- GNW
- CDNS
- DLB
- PTEN
- BR
- MSCI
- FAF
- SNX
- BRO
- SCOR
- SNV
- ARW
- NNN
- WEX
- SSB
- FTNT
- SAIA
- KNX
- TGNA
- BLKB
- OII
- CAR
- TREE
- PEB
- FSLR
- WRB
- FLG
- THG
- EXEL
- LECO
- GPK
- AA
- ENOV
- TKR
- VICR
- CPT
- TTEK
- GEO
- TRGP
- DNOW
- SVC
- CAKE
- FIX
- RH
device_name: NVIDIA GeForce RTX 5070 Ti Laptop GPU
gpu_available: true
hyperparameter_optimization:
  all_results:
    lstm_h128_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0006225681863725185
      config_name: lstm_h128_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0013128389255143702
      final_val_loss: 0.0006239818758331239
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - &id001 !!python/object/apply:numpy.dtype
        args:
        - f8
        - false
        - true
        state: !!python/tuple
        - 3
        - <
        - null
        - null
        - null
        - -1
        - -1
        - 0
      - !!binary |
        /UqD9QQ0gT8=
      train_losses:
      - 0.08117602889736493
      - 0.012048683711327612
      - 0.005275480740237981
      - 0.00292722126080965
      - 0.0021834706130903214
      - 0.0021694421981616565
      - 0.0016820981109049171
      - 0.001587666153985386
      - 0.0014609715532666694
      - 0.001493977911498708
      - 0.0014213905766761552
      - 0.0013128389255143702
      val_losses:
      - 0.002064983331365511
      - 0.0006225681863725185
      - 0.0006227106496226043
      - 0.0006233672029338777
      - 0.0006237252382561564
      - 0.0006239405483938754
      - 0.0006240418588276953
      - 0.0006240705261006951
      - 0.0006240705552045256
      - 0.0006240531511139125
      - 0.0006240209040697664
      - 0.0006239818758331239
    lstm_h128_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0006238853675313294
      config_name: lstm_h128_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0014272405436107267
      final_val_loss: 0.0006265982810873538
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        V+AgAvzQcr8=
      train_losses:
      - 0.12236343075831731
      - 0.01934337429702282
      - 0.007862342017081877
      - 0.004082244083595772
      - 0.0033579669737567506
      - 0.002731375153719758
      - 0.0020086433990703276
      - 0.0021050686191301793
      - 0.0018108257694014658
      - 0.0018629966167888294
      - 0.0017810595163609833
      - 0.001665535431432848
      - 0.0014272405436107267
      val_losses:
      - 0.002159354626201093
      - 0.0006366285670083016
      - 0.0006238853675313294
      - 0.0006250838050618768
      - 0.0006258334033191204
      - 0.0006263009854592383
      - 0.0006265348929446191
      - 0.0006266754644457251
      - 0.0006267170538194478
      - 0.0006267073331400752
      - 0.000626695400569588
      - 0.0006266609416343272
      - 0.0006265982810873538
    lstm_h128_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0006224573298823088
      config_name: lstm_h128_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.0035510092275217175
      final_val_loss: 0.0006307248258963227
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        FKyezmWSWb8=
      train_losses:
      - 0.22353998882075152
      - 0.04723568959161639
      - 0.020057838487749297
      - 0.012019805454959473
      - 0.008421177277341485
      - 0.0062878392248724895
      - 0.0056143741045768065
      - 0.004420524191421767
      - 0.004414424513621877
      - 0.0037985002466787896
      - 0.0033102967233086624
      - 0.0035510092275217175
      val_losses:
      - 0.0027911406941711903
      - 0.0006224573298823088
      - 0.000624544860329479
      - 0.0006262398674152792
      - 0.000627531495410949
      - 0.0006285444833338261
      - 0.0006293620681390166
      - 0.0006300478999037296
      - 0.000630317983450368
      - 0.0006304940616246313
      - 0.0006306233408395201
      - 0.0006307248258963227
    lstm_h128_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0006225838442333043
      config_name: lstm_h128_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0037680226184117296
      final_val_loss: 0.0006341605330817401
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        awJG9tkzYj8=
      train_losses:
      - 0.21384993443886438
      - 0.04662083446358641
      - 0.020883437478914857
      - 0.013978569225097695
      - 0.009516889889103671
      - 0.008405131249067685
      - 0.006643113447353244
      - 0.005503144561468313
      - 0.005082211845243971
      - 0.004194490514540424
      - 0.0042254054569639266
      - 0.0037680226184117296
      val_losses:
      - 0.0032515262719243765
      - 0.0006225838442333043
      - 0.0006250154692679644
      - 0.0006272076570894569
      - 0.0006290332530625165
      - 0.0006305190618149936
      - 0.0006317866500467062
      - 0.0006328529852908105
      - 0.0006332736229524016
      - 0.0006336304359138012
      - 0.0006339274113997817
      - 0.0006341605330817401
    lstm_h128_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.0006228316924534738
      config_name: lstm_h128_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.00820237941419085
      final_val_loss: 0.0006387584726326168
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        wRysdxSEdz8=
      train_losses:
      - 0.40558622901638347
      - 0.09269409192105134
      - 0.04297443153336644
      - 0.02462012677763899
      - 0.01901007699780166
      - 0.016580626756573718
      - 0.012627234061559042
      - 0.011027999493914345
      - 0.00964008376467973
      - 0.008947942134303352
      - 0.008267514834490916
      - 0.00820237941419085
      val_losses:
      - 0.005566852632910013
      - 0.0006228316924534738
      - 0.0006257538625504822
      - 0.0006284055998548865
      - 0.0006306350405793637
      - 0.0006327117152977735
      - 0.0006345806759782135
      - 0.0006362478015944362
      - 0.0006369593902491033
      - 0.0006375774391926825
      - 0.0006381780549418181
      - 0.0006387584726326168
    lstm_h128_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0006257037748582661
      config_name: lstm_h128_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.010400557153237363
      final_val_loss: 0.0006426861800719053
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        pTUzuA+rUz8=
      train_losses:
      - 0.5091597636540731
      - 0.14270629361271858
      - 0.06614203626910846
      - 0.040006207612653576
      - 0.029637336420516174
      - 0.020580944139510393
      - 0.017899018091460068
      - 0.014319404882068435
      - 0.013517757023995122
      - 0.012506228794033328
      - 0.011736575979739428
      - 0.009883244521915913
      - 0.010400557153237363
      val_losses:
      - 0.02025412581861019
      - 0.0012784559512510896
      - 0.0006257037748582661
      - 0.0006288991717156023
      - 0.0006318669184111059
      - 0.0006342964188661426
      - 0.000636408367427066
      - 0.0006383031432051212
      - 0.0006399400008376688
      - 0.000640696263872087
      - 0.0006414195522665977
      - 0.0006421238649636507
      - 0.0006426861800719053
    lstm_h128_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.0006195671739988029
      config_name: lstm_h128_lr0.0005_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0007363730643798286
      final_val_loss: 0.0006196810572873801
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        8ZCxByYKmT8=
      train_losses:
      - 0.08089383846769731
      - 0.001812903065001592
      - 0.0011823337845271453
      - 0.0009741404404242834
      - 0.0009276694278620804
      - 0.0008498897465566794
      - 0.0008403045843200138
      - 0.0008101240770580868
      - 0.0007747027654356012
      - 0.0007983345761507129
      - 0.000754048009791101
      - 0.0007532038337861499
      - 0.0007563515100628138
      - 0.0007501299939273546
      - 0.0007529160454093168
      - 0.0007421170060600465
      - 0.000759922205664528
      - 0.0007493070685692752
      - 0.0007307994237635285
      - 0.0007317222819741195
      - 0.0007261282444233075
      - 0.0007551634383465474
      - 0.0007362801601023724
      - 0.0007569464020586262
      - 0.0007445797624920184
      - 0.0007316121821835017
      - 0.0007465981567899386
      - 0.0007335123906765754
      - 0.0007382082015586396
      - 0.0007363730643798286
      val_losses:
      - 0.0006325968133751303
      - 0.0006424735765904188
      - 0.0006426761101465672
      - 0.0006385969754774123
      - 0.0006338551756925881
      - 0.0006297686486504972
      - 0.0006266213895287365
      - 0.0006243788811843842
      - 0.0006228022102732211
      - 0.0006217385525815189
      - 0.0006210476858541369
      - 0.0006205352256074548
      - 0.0006202124059200287
      - 0.0006199727649800479
      - 0.0006198109476827085
      - 0.0006197449401952326
      - 0.0006197264592628926
      - 0.0006196796894073486
      - 0.0006196367321535945
      - 0.0006196247704792768
      - 0.0006196870817802846
      - 0.0006196381291374564
      - 0.0006195671739988029
      - 0.0006196413887664676
      - 0.000619720172835514
      - 0.0006196665635798126
      - 0.0006196351896505803
      - 0.0006197012262418866
      - 0.0006196074245963246
      - 0.0006196810572873801
    lstm_h128_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.0006196660397108644
      config_name: lstm_h128_lr0.0005_d0.2_wd1e-05
      converged: true
      final_train_loss: 0.0007379332188672075
      final_val_loss: 0.0006196863832883537
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        1/gdouTtmD8=
      train_losses:
      - 0.07317059572475652
      - 0.002247158467071131
      - 0.0011351080853880073
      - 0.0009610364795662463
      - 0.0008645516985173648
      - 0.0008871860773069784
      - 0.0008107934506066764
      - 0.0007874424870048339
      - 0.0008451752025090779
      - 0.0007578770455438644
      - 0.0008117565351615971
      - 0.0007675872087323418
      - 0.0008147117429568121
      - 0.0007843247779722636
      - 0.0007763868246305113
      - 0.0007726757952089732
      - 0.0007643912880060574
      - 0.0007603646760496
      - 0.0007719621886887277
      - 0.0007365518734635165
      - 0.000749277508778808
      - 0.0007460724300472066
      - 0.0007527599470146621
      - 0.0007348944103190055
      - 0.0007357074597772831
      - 0.0007258110805802668
      - 0.0007496308583843833
      - 0.000729532771705029
      - 0.0007225199175688127
      - 0.0007379332188672075
      val_losses:
      - 0.0006331218464765698
      - 0.0006442187295760959
      - 0.0006447481864597648
      - 0.0006403719016816467
      - 0.000635287316981703
      - 0.0006310099270194769
      - 0.0006277830398175865
      - 0.0006253593892324716
      - 0.0006236756453290582
      - 0.0006224830576684326
      - 0.0006217109621502459
      - 0.0006211352301761508
      - 0.0006206764082890004
      - 0.000620400212937966
      - 0.0006202089134603739
      - 0.0006200155767146498
      - 0.0006199827766977251
      - 0.0006198797200340778
      - 0.000619876169366762
      - 0.0006197688344400376
      - 0.0006198387127369642
      - 0.0006197411275934428
      - 0.0006196841131895781
      - 0.0006196764588821679
      - 0.0006197963957674801
      - 0.0006197390903253108
      - 0.0006197101902216673
      - 0.0006196660397108644
      - 0.0006197046022862196
      - 0.0006196863832883537
    lstm_h128_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0006195992755237967
      config_name: lstm_h128_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007528603018727154
      final_val_loss: 0.0006196428730618209
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        53UqPND7lz8=
      train_losses:
      - 0.09711007459554821
      - 0.00407438554490606
      - 0.0019406529609113932
      - 0.001433678320609033
      - 0.0013699666402923565
      - 0.0011045736318919808
      - 0.00110712999109334
      - 0.000973208894720301
      - 0.0009729512385092676
      - 0.0009067684101561705
      - 0.0008617032047671577
      - 0.0009197733258285249
      - 0.0009131635694454113
      - 0.0008506338926963508
      - 0.0008563077038464447
      - 0.0008653026258495325
      - 0.0008320902124978602
      - 0.0008139909235372519
      - 0.0008447706495644525
      - 0.0008066925753761703
      - 0.0007641678481983641
      - 0.0007921834718824053
      - 0.0007474430361374592
      - 0.0008017268070640663
      - 0.0007369752226319785
      - 0.0007824035759161537
      - 0.0007771859042501698
      - 0.0007494074816349894
      - 0.0007609798631165177
      - 0.0007528603018727154
      val_losses:
      - 0.0006312067271210253
      - 0.0006436172698158771
      - 0.0006458148709498346
      - 0.0006418894918169826
      - 0.0006364899745676666
      - 0.0006317553052213043
      - 0.0006281294336076826
      - 0.0006254537147469819
      - 0.0006234742177184671
      - 0.0006222044175956398
      - 0.0006214345339685678
      - 0.0006208194245118648
      - 0.000620463106315583
      - 0.0006203323719091713
      - 0.0006201391806825995
      - 0.0006199930212460458
      - 0.0006199324561748654
      - 0.0006197789043653756
      - 0.0006197501788847148
      - 0.0006197360344231129
      - 0.0006197683687787503
      - 0.0006197442708071321
      - 0.0006196828908286989
      - 0.0006197118491400033
      - 0.0006196593749336898
      - 0.0006196556205395609
      - 0.0006196970934979618
      - 0.0006195992755237967
      - 0.0006197065522428602
      - 0.0006196428730618209
    lstm_h128_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0006201988726388663
      config_name: lstm_h128_lr0.0005_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0008071444996555025
      final_val_loss: 0.0006201988726388663
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        acD1nRBCkT8=
      train_losses:
      - 0.11197005010520418
      - 0.005797365835557382
      - 0.0023523500033964715
      - 0.0016519072911857318
      - 0.0013607643001402419
      - 0.0012053220416419208
      - 0.0010336194391129538
      - 0.0011807979850952204
      - 0.0010629207827150822
      - 0.0009897641721181571
      - 0.001010181872212949
      - 0.0009332360205007717
      - 0.0009158744457333038
      - 0.0008860970362244794
      - 0.0009430330052661399
      - 0.0009316096742016574
      - 0.000994433003749388
      - 0.0008850237112104272
      - 0.0009212657265986005
      - 0.0008603133416424195
      - 0.0009085146108797441
      - 0.000869425906178852
      - 0.0008269551763078198
      - 0.0008545039017917588
      - 0.0008608498028479517
      - 0.000798039977477553
      - 0.0008435702232721572
      - 0.0009299372419870148
      - 0.0008391669204380984
      - 0.0008071444996555025
      val_losses:
      - 0.0006358100799843669
      - 0.000651462672976777
      - 0.0006561414047610015
      - 0.0006531989783979952
      - 0.0006472451495938003
      - 0.000641301361611113
      - 0.0006361018749885261
      - 0.0006339286628644913
      - 0.0006320526299532503
      - 0.0006303690315689892
      - 0.0006289011507760733
      - 0.0006276136264204979
      - 0.0006264732510317117
      - 0.0006254581967368722
      - 0.0006246308039408177
      - 0.0006239223293960094
      - 0.0006233380117919296
      - 0.0006228049169294536
      - 0.00062234525103122
      - 0.0006219378847163171
      - 0.0006216441106516868
      - 0.0006213819433469325
      - 0.0006211409927345812
      - 0.0006209171551745385
      - 0.0006207603146322072
      - 0.0006205760000739247
      - 0.0006204259116202593
      - 0.0006203578668646514
      - 0.0006203158118296415
      - 0.0006201988726388663
    lstm_h128_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0006201729702297598
      config_name: lstm_h128_lr0.0005_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0008166376064764336
      final_val_loss: 0.0006201729702297598
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        bsqFietkiT8=
      train_losses:
      - 0.14396683235342303
      - 0.007704828477775057
      - 0.0036276603544441364
      - 0.002733824910440793
      - 0.0018268699059262872
      - 0.0014720074250362813
      - 0.0015368829723835613
      - 0.0013758192120197539
      - 0.0014489985769614577
      - 0.0014876537024974823
      - 0.0013646519898126523
      - 0.0010350527688084792
      - 0.001207853274536319
      - 0.0010738790830752503
      - 0.0009881455722885828
      - 0.0011545966311435525
      - 0.001149656954415453
      - 0.0011217893528131147
      - 0.0010761357164786507
      - 0.0010427689461115126
      - 0.0009967400304352243
      - 0.0008688517118571326
      - 0.0009657628397690132
      - 0.0010465825665354107
      - 0.000861109341106688
      - 0.0008830250493095567
      - 0.0008595160616096109
      - 0.0008509193236629168
      - 0.0009577274274003381
      - 0.0008166376064764336
      val_losses:
      - 0.0006354315846692771
      - 0.0006536301807500422
      - 0.0006591799319721758
      - 0.0006558532186318189
      - 0.0006488602957688272
      - 0.0006414544186554849
      - 0.000635665055597201
      - 0.0006335038051474839
      - 0.000631710747256875
      - 0.000630183843895793
      - 0.0006287884607445449
      - 0.0006274821062106639
      - 0.0006263037794269621
      - 0.0006253047613427043
      - 0.0006244585674721748
      - 0.000623697938863188
      - 0.0006230992148630321
      - 0.0006227036355994642
      - 0.0006222643423825502
      - 0.0006220079667400569
      - 0.0006216747860889882
      - 0.0006213893939275295
      - 0.0006211580766830593
      - 0.000620977662038058
      - 0.0006207867118064314
      - 0.0006205785030033439
      - 0.0006203750090207905
      - 0.0006202814111020416
      - 0.0006202154909260571
      - 0.0006201729702297598
    lstm_h128_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0006361626437865198
      config_name: lstm_h128_lr0.0005_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.001519251469289884
      final_val_loss: 0.0006382531428243965
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        V+6lqX1Egr8=
      train_losses:
      - 0.2157419892027974
      - 0.017724150481323402
      - 0.006469984073191881
      - 0.004324246818820636
      - 0.002553668232091392
      - 0.0023075885934910425
      - 0.0017089046159526333
      - 0.0017570547837143142
      - 0.0017761917940030496
      - 0.0017337526272361476
      - 0.001519251469289884
      val_losses:
      - 0.0006361626437865198
      - 0.000661406316794455
      - 0.0006737622898072004
      - 0.0006740783283021301
      - 0.0006678109348285943
      - 0.0006594077276531607
      - 0.0006511359824799001
      - 0.0006472679087892175
      - 0.0006438717828132212
      - 0.0006409282796084881
      - 0.0006382531428243965
    lstm_h128_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0006196074828039855
      config_name: lstm_h128_lr0.001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.000717454012677384
      final_val_loss: 0.0006196923786774278
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 24
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        muZIvm1amD8=
      train_losses:
      - 0.05877591517249433
      - 0.0011231120394465204
      - 0.0008232729887822643
      - 0.0008202391763916239
      - 0.0007602377979007239
      - 0.0007420640128354231
      - 0.0007529433156984547
      - 0.0007285137592892473
      - 0.0007362898080221688
      - 0.0007366910673833141
      - 0.0007188070400540406
      - 0.0007347873518786704
      - 0.0007204303935092563
      - 0.0007334050072434669
      - 0.0007183782436186448
      - 0.000716309659765102
      - 0.0007227903358095015
      - 0.0007197174903315803
      - 0.0007194585584026451
      - 0.0007168508066873377
      - 0.0007180724545226743
      - 0.0007370842698340615
      - 0.0007170396905470019
      - 0.000717454012677384
      val_losses:
      - 0.0006442162557505071
      - 0.0006529809325002134
      - 0.0006433690723497421
      - 0.0006323724810499698
      - 0.0006256088672671467
      - 0.0006222512747626752
      - 0.0006207250989973545
      - 0.0006201063224580139
      - 0.0006198518094606698
      - 0.0006197623151820153
      - 0.0006197481125127524
      - 0.0006197518086992204
      - 0.0006197097827680409
      - 0.0006196074828039855
      - 0.0006196599861141294
      - 0.0006196677277330309
      - 0.0006196883041411638
      - 0.0006197495094966143
      - 0.0006198669434525073
      - 0.0006197027396410704
      - 0.0006196277681738138
      - 0.0006196528556756675
      - 0.0006196248868945986
      - 0.0006196923786774278
    lstm_h128_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0006196164467837662
      config_name: lstm_h128_lr0.001_d0.2_wd1e-05
      converged: true
      final_train_loss: 0.0007177677859241763
      final_val_loss: 0.0006197074544616044
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        jSJ+QpvHmD8=
      train_losses:
      - 0.08689560011650126
      - 0.0012575382958554353
      - 0.0008383624469085286
      - 0.000808958876101921
      - 0.0007621808957386141
      - 0.0007584163055677587
      - 0.0007462943807089081
      - 0.0007649001781828701
      - 0.0007329965010285378
      - 0.0007354800375954559
      - 0.0007360783977977311
      - 0.0007451428488517801
      - 0.0007319639262277633
      - 0.0007376279099844396
      - 0.0007474090283115705
      - 0.0007306039478862658
      - 0.0007365949859376997
      - 0.0007229447946883738
      - 0.0007262381016820049
      - 0.0007165315231153121
      - 0.0007236446690512821
      - 0.0007280080462805927
      - 0.0007199546622966105
      - 0.0007367297512246296
      - 0.0007152363541536033
      - 0.0007206912462910017
      - 0.0007190270019539943
      - 0.0007274910603882745
      - 0.0007187563217788314
      - 0.0007177677859241763
      val_losses:
      - 0.0006566851807292551
      - 0.0006725831772200763
      - 0.0006603214424103498
      - 0.0006437601405195892
      - 0.0006320761458482593
      - 0.0006257132918108255
      - 0.0006223878299351782
      - 0.0006209364510141313
      - 0.0006202657532412559
      - 0.000619981496129185
      - 0.0006198660703375936
      - 0.0006197096372488886
      - 0.0006197616166900843
      - 0.0006196584436111152
      - 0.0006197906332090497
      - 0.0006197511975187808
      - 0.000619690865278244
      - 0.0006196313770487905
      - 0.0006197034381330013
      - 0.0006197884504217654
      - 0.0006196541362442076
      - 0.0006198061164468527
      - 0.0006196164467837662
      - 0.0006196708709467202
      - 0.0006196809117682278
      - 0.0006197084730956703
      - 0.0006196975009515882
      - 0.0006196407193783671
      - 0.0006196402246132493
      - 0.0006197074544616044
    lstm_h128_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.0006196567555889487
      config_name: lstm_h128_lr0.001_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.000741906653274782
      final_val_loss: 0.0006196965114213526
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        1uRrM82WmT8=
      train_losses:
      - 0.1075914999237284
      - 0.0021650028453829386
      - 0.0012941014332075913
      - 0.000996083903980131
      - 0.0009162560115025068
      - 0.0008866427233442664
      - 0.0008141249757803356
      - 0.0007803553016856313
      - 0.0008095241889047126
      - 0.0007679514916768918
      - 0.0008139351023904359
      - 0.0007450466946465895
      - 0.0007790144494113823
      - 0.0007739177575179687
      - 0.0007480339409084991
      - 0.00076478005212266
      - 0.0007666890014661476
      - 0.000761402552598156
      - 0.0007429599791066721
      - 0.0007480754963277528
      - 0.0007383977936115116
      - 0.0007709407412524646
      - 0.0007702938552635411
      - 0.000743812658280755
      - 0.0007365003160278624
      - 0.00077883376313063
      - 0.000752145157700094
      - 0.0007315573117618138
      - 0.0007799026449598993
      - 0.000741906653274782
      val_losses:
      - 0.0006597971951123327
      - 0.0006807579193264246
      - 0.0006691575981676579
      - 0.0006496039277408272
      - 0.0006349844916258007
      - 0.000627044471912086
      - 0.0006232367595657706
      - 0.0006213430315256119
      - 0.0006206585385371
      - 0.0006203071679919958
      - 0.0006200283532962203
      - 0.0006199433119036257
      - 0.0006198452902026474
      - 0.0006197175534907728
      - 0.0006197475304361433
      - 0.0006197688053362072
      - 0.0006198624905664474
      - 0.000619783706497401
      - 0.0006197050970513374
      - 0.0006196686590556055
      - 0.0006196567555889487
      - 0.0006196754693519324
      - 0.000619757134700194
      - 0.0006197913899086416
      - 0.0006197697657626122
      - 0.0006197348702698946
      - 0.0006197251786943525
      - 0.0006197229959070683
      - 0.0006196894391905516
      - 0.0006196965114213526
    lstm_h128_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.0006196420581545681
      config_name: lstm_h128_lr0.001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0007228467244810114
      final_val_loss: 0.0006196882750373334
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 23
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        tvIHV1Wylz8=
      train_losses:
      - 0.09784946807970603
      - 0.0015628436910143744
      - 0.0009732915884039054
      - 0.0008544018104051551
      - 0.0008110384369501844
      - 0.0007918519986560568
      - 0.0007736735763804367
      - 0.0007589085338016351
      - 0.0007607333342699955
      - 0.0007554920885013416
      - 0.0007470382212583596
      - 0.0007386324771990379
      - 0.0007384552736766636
      - 0.0007543534981474901
      - 0.0007337743639557933
      - 0.0007271555853852382
      - 0.0007373251816413055
      - 0.0007459399397096907
      - 0.0007410559774143621
      - 0.0007494597521144897
      - 0.0007488034219325831
      - 0.0007289575927037125
      - 0.0007228467244810114
      val_losses:
      - 0.0006551880796905607
      - 0.0006713577022310346
      - 0.000655881391139701
      - 0.0006383217696566135
      - 0.000627828121650964
      - 0.0006231332663446665
      - 0.0006211854342836887
      - 0.0006204056553542614
      - 0.0006200040515977889
      - 0.0006198370829224586
      - 0.0006199092313181609
      - 0.0006198182527441531
      - 0.0006196420581545681
      - 0.0006197420880198479
      - 0.0006197092880029231
      - 0.0006197244510985911
      - 0.0006197887123562396
      - 0.0006197404582053423
      - 0.0006197228212840855
      - 0.0006197470065671951
      - 0.0006197451148182154
      - 0.0006196887406986207
      - 0.0006196882750373334
    lstm_h128_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0006195851310621947
      config_name: lstm_h128_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0007419410540023819
      final_val_loss: 0.0006197352195158601
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        XlR40mFLlz8=
      train_losses:
      - 0.19127749224814275
      - 0.005340087091705452
      - 0.0019392479540935408
      - 0.0014801533155453701
      - 0.0011646144218199577
      - 0.0011435088284391288
      - 0.0009346709412056953
      - 0.0010269038126959156
      - 0.000980333400851426
      - 0.0009279393561882898
      - 0.0008462370800164839
      - 0.0009276550069140891
      - 0.0008547942125005648
      - 0.0008965051820268854
      - 0.000890839466592297
      - 0.0008579251783279082
      - 0.0008726421413787951
      - 0.0008445485582342371
      - 0.0008505102208194634
      - 0.0007729459903202951
      - 0.0008249601184312875
      - 0.0007808402297087014
      - 0.0007950494036776945
      - 0.000783637612281988
      - 0.0007532591553172097
      - 0.0007458214240614325
      - 0.0007379926440383618
      - 0.0007452017598552629
      - 0.0008153750871618589
      - 0.0007419410540023819
      val_losses:
      - 0.0006628086848650128
      - 0.0006969661917537451
      - 0.0006882298330310732
      - 0.0006648083799518645
      - 0.0006453494424931705
      - 0.0006334165518637747
      - 0.0006265891715884209
      - 0.0006231859733816236
      - 0.0006215503381099552
      - 0.000620603037532419
      - 0.0006200610077939928
      - 0.0006198149349074811
      - 0.0006198730843607336
      - 0.0006197837356012315
      - 0.0006197286420501769
      - 0.0006198796909302473
      - 0.0006197327165864408
      - 0.0006197207258082926
      - 0.0006196329777594656
      - 0.0006196785252541304
      - 0.0006196950853336602
      - 0.0006197665061336011
      - 0.0006198319606482983
      - 0.0006197226757649332
      - 0.0006196353933773935
      - 0.0006196178146637976
      - 0.0006196182512212545
      - 0.0006195851310621947
      - 0.0006197009060997516
      - 0.0006197352195158601
    lstm_h128_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0006195965979713947
      config_name: lstm_h128_lr0.001_d0.4_wd1e-05
      converged: true
      final_train_loss: 0.0007380528938180456
      final_val_loss: 0.000619600381469354
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 5.7890625
        estimated_sample_memory_mb: 0.911865234375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 900872
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        UxPKsYlWmj8=
      train_losses:
      - 0.28949701219486695
      - 0.006079774543953438
      - 0.0020515911552744606
      - 0.001512347439226384
      - 0.001250233612760591
      - 0.0010485150560270995
      - 0.0010655652343605955
      - 0.001029273068221907
      - 0.000948912973399274
      - 0.0009588877922700098
      - 0.0008618048256418357
      - 0.0009197509934892878
      - 0.0008631608992194136
      - 0.0007947228926544388
      - 0.0008497845313589399
      - 0.0008796416271555548
      - 0.0009032899251906201
      - 0.0007730838406132534
      - 0.0008464764008143296
      - 0.0007957709021866322
      - 0.0007656625530216843
      - 0.0007659004234786456
      - 0.0007987519541832929
      - 0.000741825254711633
      - 0.0007771352732864519
      - 0.0007853776381428664
      - 0.0007797686945802221
      - 0.0007278796886870017
      - 0.0007544767674213896
      - 0.0007380528938180456
      val_losses:
      - 0.000657690572552383
      - 0.000695080787409097
      - 0.0006870181823614985
      - 0.0006621923530474305
      - 0.0006423003796953708
      - 0.0006306323921307921
      - 0.0006250396836549044
      - 0.0006225834367796779
      - 0.0006211223662830889
      - 0.0006202530057635158
      - 0.0006200511706992984
      - 0.0006198279443196952
      - 0.0006199095805641264
      - 0.0006197539332788438
      - 0.0006197149923536927
      - 0.0006198580958880484
      - 0.0006200622592587024
      - 0.0006198597839102149
      - 0.0006197615002747625
      - 0.0006196542526595294
      - 0.0006197291368152946
      - 0.0006198537885211408
      - 0.0006199830968398601
      - 0.00061967913643457
      - 0.0006195965979713947
      - 0.000619911152170971
      - 0.0006198630144353956
      - 0.0006197519542183727
      - 0.000619619560893625
      - 0.000619600381469354
    lstm_h256_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0006219156784936786
      config_name: lstm_h256_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0009482023936773961
      final_val_loss: 0.0006223037198651582
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        zhdU0rcUfr8=
      train_losses:
      - 0.19565029442310333
      - 0.0075586204378244775
      - 0.0023899156464419016
      - 0.0014583995604577165
      - 0.0012516916904132813
      - 0.0010383275609153013
      - 0.0011804887568966176
      - 0.001022096616604055
      - 0.0009763683968534073
      - 0.0010757453370994579
      - 0.0009968384207847218
      - 0.0009482023936773961
      val_losses:
      - 0.0006239916256163269
      - 0.0006219156784936786
      - 0.0006226749683264643
      - 0.0006229329446796328
      - 0.0006229525606613606
      - 0.0006228775600902736
      - 0.0006227589619811624
      - 0.0006226158875506371
      - 0.0006225429533515126
      - 0.0006224639946594834
      - 0.0006223835807759315
      - 0.0006223037198651582
    lstm_h256_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0006215725152287632
      config_name: lstm_h256_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0010670878352054085
      final_val_loss: 0.0006225864053703845
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        4yhFgYzwgT8=
      train_losses:
      - 0.1756624657039841
      - 0.011986272525973618
      - 0.0036901596661967537
      - 0.00224063180697461
      - 0.0015172793161279212
      - 0.001503586662389959
      - 0.001363266579573974
      - 0.0013726402539759874
      - 0.0011290590094480042
      - 0.001201073799165897
      - 0.0010903021078168724
      - 0.0010670878352054085
      val_losses:
      - 0.0007026107923593372
      - 0.0006215725152287632
      - 0.0006224288081284612
      - 0.0006228470301721245
      - 0.000622996041784063
      - 0.0006230202270671725
      - 0.0006229437422007322
      - 0.0006228640268091112
      - 0.0006228176644071937
      - 0.0006227476405911148
      - 0.0006226763071026653
      - 0.0006225864053703845
    lstm_h256_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0006217127374839038
      config_name: lstm_h256_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.0020622762385755777
      final_val_loss: 0.000624795415205881
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        qWRIu7wyjT8=
      train_losses:
      - 0.3198311484108369
      - 0.029884875752031803
      - 0.010749745493133863
      - 0.005874374454530577
      - 0.004500029666814953
      - 0.0033290234277956188
      - 0.0031771837190414467
      - 0.002540553561023747
      - 0.002402909556015705
      - 0.002624321360296259
      - 0.002233785198768601
      - 0.0020622762385755777
      val_losses:
      - 0.0007771874952595681
      - 0.0006217127374839038
      - 0.0006229529099073261
      - 0.0006237791385501623
      - 0.0006243105162866414
      - 0.0006245959084481001
      - 0.0006247511773835868
      - 0.0006248246936593205
      - 0.0006248367426451296
      - 0.0006248317076824605
      - 0.0006248156423680484
      - 0.000624795415205881
    lstm_h256_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0006222842202987522
      config_name: lstm_h256_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0020560865329268077
      final_val_loss: 0.0006252772000152618
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        e23hBlX8dT8=
      train_losses:
      - 0.30843686995406944
      - 0.03573948952058951
      - 0.013022896212836107
      - 0.0068110045200834675
      - 0.004663384868763387
      - 0.004077023147450139
      - 0.0031529033828216293
      - 0.0027113721589557827
      - 0.0024007240523739406
      - 0.0022979234830321125
      - 0.00250396221720924
      - 0.0020560865329268077
      val_losses:
      - 0.0011477919179014862
      - 0.0006222842202987522
      - 0.0006236825720407069
      - 0.0006245166005101055
      - 0.0006249528378248215
      - 0.0006251998129300773
      - 0.000625316723017022
      - 0.0006253698084037751
      - 0.0006253651808947325
      - 0.0006253375904634595
      - 0.0006253149767871946
      - 0.0006252772000152618
    lstm_h256_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.0006218496710062027
      config_name: lstm_h256_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.004497592182209094
      final_val_loss: 0.0006283938710112125
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        KWEeoYhGjD8=
      train_losses:
      - 0.3834992554038763
      - 0.0531222652643919
      - 0.023551813947657745
      - 0.014594163668031493
      - 0.009546169972357651
      - 0.007775199444343646
      - 0.007142616164249678
      - 0.0056348564103245735
      - 0.0054398122398803634
      - 0.0051022113766521215
      - 0.004727848068190117
      - 0.004497592182209094
      val_losses:
      - 0.0006848273915238678
      - 0.0006218496710062027
      - 0.0006236188346520066
      - 0.0006249985308386385
      - 0.0006260190566536039
      - 0.0006267452554311603
      - 0.0006273622275330126
      - 0.0006278241635300219
      - 0.000628018140560016
      - 0.0006281652022153139
      - 0.0006282903486862779
      - 0.0006283938710112125
    lstm_h256_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0006224008684512228
      config_name: lstm_h256_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.004897240859766801
      final_val_loss: 0.0006299812230281532
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 12
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        mP7QZlKUfz8=
      train_losses:
      - 0.5137367260952791
      - 0.07420424309869607
      - 0.029743549103538196
      - 0.01771128294058144
      - 0.013050431928907832
      - 0.009452515553372601
      - 0.00827430976399531
      - 0.006272259051911533
      - 0.0056850198889151216
      - 0.0057868084210592015
      - 0.005264794298758109
      - 0.004897240859766801
      val_losses:
      - 0.0006603580550290644
      - 0.0006224008684512228
      - 0.0006244225660338998
      - 0.0006261126545723528
      - 0.00062742450973019
      - 0.0006283047841861844
      - 0.000628978421445936
      - 0.0006295281636994332
      - 0.000629693444352597
      - 0.0006298407970461994
      - 0.0006299299420788884
      - 0.0006299812230281532
    lstm_h256_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.0006196409231051803
      config_name: lstm_h256_lr0.0005_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0007237092213472351
      final_val_loss: 0.0006196769827511162
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        GxAh8GxtmT8=
      train_losses:
      - 0.10356787649410155
      - 0.0012254005608459313
      - 0.0008741487690713257
      - 0.0008090513486725589
      - 0.0007654507353436202
      - 0.0007536791575451692
      - 0.0007486689428333193
      - 0.0007782560229922334
      - 0.0007281693203064302
      - 0.000733073364244774
      - 0.0007431492607186859
      - 0.0007248667437427988
      - 0.0007318933542895442
      - 0.0007292093214346096
      - 0.0007348278013523668
      - 0.0007299979770323262
      - 0.0007354286402308693
      - 0.0007337445276789367
      - 0.0007267394442654526
      - 0.0007205452517761538
      - 0.0007261795787295947
      - 0.0007246222424631318
      - 0.0007211376456931854
      - 0.0007268283370649442
      - 0.0007161996860910828
      - 0.0007183649092136571
      - 0.0007257956021931022
      - 0.0007172665937105194
      - 0.0007134296708197022
      - 0.0007237092213472351
      val_losses:
      - 0.0006278018408920616
      - 0.0006333068595267832
      - 0.0006326083966996521
      - 0.0006296725186984986
      - 0.0006267160642892122
      - 0.000624400592641905
      - 0.0006227779958862811
      - 0.0006216704787220806
      - 0.0006209343555383384
      - 0.0006204207893460989
      - 0.0006201346404850483
      - 0.000619990547420457
      - 0.0006198878691066056
      - 0.0006198121991474181
      - 0.000619785743765533
      - 0.0006196970934979618
      - 0.0006197535840328783
      - 0.000619825761532411
      - 0.00061971036484465
      - 0.0006197216280270368
      - 0.0006196409231051803
      - 0.0006197268376126885
      - 0.0006197004404384643
      - 0.0006196885660756379
      - 0.0006196624017320573
      - 0.0006196761096362025
      - 0.000619660277152434
      - 0.0006196842587087303
      - 0.0006196665635798126
      - 0.0006196769827511162
    lstm_h256_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.0006196238682605326
      config_name: lstm_h256_lr0.0005_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0007279830169863999
      final_val_loss: 0.0006196842005010694
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 25
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        iqqx9U5VmT8=
      train_losses:
      - 0.11886873553157784
      - 0.0013974794613507886
      - 0.0008746106032049283
      - 0.0007963575723503405
      - 0.0007955065763477857
      - 0.0007461742692006131
      - 0.0007459976283522943
      - 0.0007380580645985901
      - 0.0007342406946312016
      - 0.0007357696983187149
      - 0.0007328229742900779
      - 0.0007453044236171991
      - 0.000732789320560793
      - 0.0007372929054933289
      - 0.000739502560463734
      - 0.00072826254957666
      - 0.0007275883447922146
      - 0.0007261408997389177
      - 0.0007178054220275953
      - 0.0007214851551301157
      - 0.0007173615692105765
      - 0.0007221052219392732
      - 0.0007226275289819265
      - 0.0007213905094734704
      - 0.0007279830169863999
      val_losses:
      - 0.0006278591463342309
      - 0.000633286836091429
      - 0.0006324686983134598
      - 0.0006294097693171352
      - 0.000626358378212899
      - 0.0006240169459488243
      - 0.0006223988602869213
      - 0.0006213456799741834
      - 0.0006206372345332056
      - 0.0006202253280207515
      - 0.0006199715426191688
      - 0.0006197873153723776
      - 0.0006197361799422652
      - 0.0006196718895807862
      - 0.0006196238682605326
      - 0.0006196352478582412
      - 0.0006196439207997173
      - 0.0006196289323270321
      - 0.0006196822214405984
      - 0.0006196359172463417
      - 0.0006196530302986503
      - 0.0006196605681907386
      - 0.000619634723989293
      - 0.0006196512840688229
      - 0.0006196842005010694
    lstm_h256_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0006196339963935316
      config_name: lstm_h256_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007283606731410449
      final_val_loss: 0.0006196509348228574
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        nfVVMkT9mT8=
      train_losses:
      - 0.17646392132155597
      - 0.002358100920294722
      - 0.0012385222798911855
      - 0.0008941039074367533
      - 0.0009366059481787185
      - 0.0008322920718152697
      - 0.0008099080684284369
      - 0.0007885622568816567
      - 0.0008275213428229714
      - 0.0007644699605104203
      - 0.0007784551368483031
      - 0.0007808887215408807
      - 0.0007814779334391156
      - 0.0007443095382768661
      - 0.0007601545221405104
      - 0.000738746147059525
      - 0.0007637192514569809
      - 0.0007595437152000765
      - 0.0007575526591002321
      - 0.0007379671102777744
      - 0.0007748067776750153
      - 0.000788399891462177
      - 0.0007458264347709095
      - 0.0007521275304801142
      - 0.0007442218193318695
      - 0.0007398351008305326
      - 0.0007448543716842929
      - 0.0007411793291491146
      - 0.000729389381983007
      - 0.0007283606731410449
      val_losses:
      - 0.000630931172054261
      - 0.0006392177601810545
      - 0.0006388415058609098
      - 0.0006349488394334912
      - 0.0006306729046627879
      - 0.0006272426689974964
      - 0.0006247964920476079
      - 0.0006229989230632782
      - 0.0006218911439646035
      - 0.0006211328436620533
      - 0.0006206320249475539
      - 0.0006202883378136903
      - 0.0006201570504345
      - 0.0006199909257702529
      - 0.0006197988986968994
      - 0.0006197910115588456
      - 0.0006197999464347959
      - 0.0006197068723849952
      - 0.0006198104820214212
      - 0.0006197878101374954
      - 0.0006197518378030509
      - 0.0006196747126523405
      - 0.0006196339963935316
      - 0.0006197004986461252
      - 0.0006197633047122508
      - 0.0006197203765623271
      - 0.0006197219190653414
      - 0.0006196923204697669
      - 0.0006196469475980848
      - 0.0006196509348228574
    lstm_h256_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0006196379836183041
      config_name: lstm_h256_lr0.0005_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0007296062588769322
      final_val_loss: 0.0006196926406119019
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        bezlNt2kmD8=
      train_losses:
      - 0.11101987631991506
      - 0.0024532347839946547
      - 0.0011227143453046058
      - 0.0009759605551759402
      - 0.0008549540701399868
      - 0.0008289774317139139
      - 0.0007865614898037165
      - 0.0008022364830443015
      - 0.0007776330409493918
      - 0.0007658185447022939
      - 0.0007736305124126375
      - 0.0007622846751473844
      - 0.0007498044772849729
      - 0.0007729037267078335
      - 0.0007610789374060308
      - 0.0007704744008757795
      - 0.0007728484585337961
      - 0.0007420237573872631
      - 0.0007529731907804186
      - 0.0007393225435710823
      - 0.0007509561692131683
      - 0.0007675293745705858
      - 0.0007529065090542039
      - 0.0007170685760987302
      - 0.0007827618343677992
      - 0.0007516453430677453
      - 0.0007272907484245176
      - 0.0007499192191365486
      - 0.0007337204539605106
      - 0.0007296062588769322
      val_losses:
      - 0.0006289810698945075
      - 0.0006354385695885867
      - 0.0006348951428662986
      - 0.0006315048958640546
      - 0.0006280944508034736
      - 0.0006253314495552331
      - 0.0006233891181182116
      - 0.0006220412324182689
      - 0.0006211987056303769
      - 0.0006206799298524857
      - 0.0006203585071489215
      - 0.0006201335054356605
      - 0.0006199482886586338
      - 0.0006199562049005181
      - 0.0006198038172442466
      - 0.0006197219190653414
      - 0.0006196971226017922
      - 0.0006197101029101759
      - 0.0006197321927174926
      - 0.00061975177959539
      - 0.0006196587346494198
      - 0.0006196379836183041
      - 0.0006196751492097974
      - 0.0006197122565936297
      - 0.000619773956714198
      - 0.0006197344337124377
      - 0.0006197547190822661
      - 0.0006197429029271007
      - 0.000619742990238592
      - 0.0006196926406119019
    lstm_h256_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0006196389440447092
      config_name: lstm_h256_lr0.0005_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0008339569273327166
      final_val_loss: 0.0006197388574946672
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        ERADMRqmmD8=
      train_losses:
      - 0.37605800448606413
      - 0.00880100455833599
      - 0.0027861445754145584
      - 0.0014406345532430957
      - 0.0015457466458125662
      - 0.0010920464313433815
      - 0.0010405754001112655
      - 0.0009928544701930757
      - 0.0010269533328634377
      - 0.0009412174937703336
      - 0.0008557640442935129
      - 0.0008355689302940542
      - 0.0008996607745454336
      - 0.0008630411418077225
      - 0.0008862640728087475
      - 0.0008396206200510884
      - 0.0008392047748202458
      - 0.0008171988689961532
      - 0.0008284059877041727
      - 0.000846536941632318
      - 0.0008313058303125823
      - 0.0008392161932230616
      - 0.0007849619869375601
      - 0.0007671149748299891
      - 0.0007473085715901107
      - 0.0007710455392953008
      - 0.0007450563959234083
      - 0.0007823572377674282
      - 0.0007817799923941493
      - 0.0008339569273327166
      val_losses:
      - 0.0006298307853285223
      - 0.0006441004225052893
      - 0.0006473216635640711
      - 0.0006432533264160156
      - 0.0006373763899318874
      - 0.0006320587708614767
      - 0.0006281250971369445
      - 0.0006253723695408553
      - 0.0006235202308744192
      - 0.0006222281081136316
      - 0.0006214730092324317
      - 0.0006209451239556074
      - 0.0006206085672602057
      - 0.0006203519005794078
      - 0.0006200941279530525
      - 0.0006200055358931422
      - 0.0006199250055942684
      - 0.0006198910705279559
      - 0.0006199149065651
      - 0.0006199070485308766
      - 0.0006198562332428992
      - 0.0006198922055773437
      - 0.0006197938346303999
      - 0.000619663274846971
      - 0.0006196395261213183
      - 0.0006197005859576166
      - 0.0006197043985594064
      - 0.0006196389440447092
      - 0.0006196801841724664
      - 0.0006197388574946672
    lstm_h256_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0006195787282194942
      config_name: lstm_h256_lr0.0005_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0007678565064755579
      final_val_loss: 0.0006196275062393397
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        Tez1c2JgmT8=
      train_losses:
      - 0.25625671974072856
      - 0.008911545504815876
      - 0.003274937131209299
      - 0.0016104779303229104
      - 0.001541659701615572
      - 0.0011196071427548304
      - 0.0011933418524373944
      - 0.0011500065059711535
      - 0.0010329712628542136
      - 0.0009888780283896874
      - 0.0009647705640721446
      - 0.00093386590500207
      - 0.0008942485243702928
      - 0.000848365646864598
      - 0.0008017729026808714
      - 0.0008538826756800214
      - 0.000867761181628642
      - 0.0008363786231105527
      - 0.000812759889716593
      - 0.0007951948791742325
      - 0.000840733113970297
      - 0.0007677534061561649
      - 0.0007919637573650107
      - 0.0008273755180804679
      - 0.0007835181459086016
      - 0.0007721384220834201
      - 0.0007829444075468928
      - 0.000791217379931671
      - 0.0007613930938532576
      - 0.0007678565064755579
      val_losses:
      - 0.0006305649585556239
      - 0.0006439057469833642
      - 0.0006465641199611127
      - 0.0006424409803003073
      - 0.0006366365705616772
      - 0.0006314064376056194
      - 0.0006274260231293738
      - 0.0006246323173400015
      - 0.0006228283746168017
      - 0.0006216065085027367
      - 0.00062079873168841
      - 0.0006203354569151998
      - 0.0006200828647706658
      - 0.0006199034396559
      - 0.0006198258779477328
      - 0.0006197139155119658
      - 0.0006196454633027315
      - 0.0006196924368850887
      - 0.0006197122274897993
      - 0.0006195787282194942
      - 0.0006196793401613832
      - 0.0006196688918862492
      - 0.0006196633621584624
      - 0.000619683472905308
      - 0.0006197199400048703
      - 0.0006196990434546024
      - 0.0006196676404215395
      - 0.0006196593458298594
      - 0.0006196351896505803
      - 0.0006196275062393397
    lstm_h256_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0006196134199853987
      config_name: lstm_h256_lr0.001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0007197216958350813
      final_val_loss: 0.000619674421614036
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 27
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        3NWo/aHLmT8=
      train_losses:
      - 0.11223389366447616
      - 0.0008817660176039984
      - 0.0007822174666216597
      - 0.0007331318047363311
      - 0.0007282282701150203
      - 0.0007372091107148057
      - 0.0007358595127395043
      - 0.0007147194652740533
      - 0.0007267107963950062
      - 0.0007210008673913156
      - 0.000721675047922569
      - 0.0007154231764919435
      - 0.0007158853356183196
      - 0.000713386262456576
      - 0.0007143576804082841
      - 0.0007128095118484149
      - 0.0007145777635741979
      - 0.0007141877916486313
      - 0.0007178889888261134
      - 0.0007139613201919323
      - 0.0007176747409782062
      - 0.0007129952282411978
      - 0.0007131181434184933
      - 0.0007224883981204281
      - 0.0007128788371725628
      - 0.0007155486382544041
      - 0.0007197216958350813
      val_losses:
      - 0.0006401240825653076
      - 0.0006449019419960678
      - 0.0006366591260302812
      - 0.0006282642716541886
      - 0.0006233551248442382
      - 0.0006211402651388198
      - 0.0006202626100275666
      - 0.0006199329800438136
      - 0.000619825441390276
      - 0.0006196745380293578
      - 0.0006196578033268452
      - 0.0006197253824211657
      - 0.000619702012045309
      - 0.0006197635666467249
      - 0.0006196813774295151
      - 0.0006196519534569234
      - 0.0006196134199853987
      - 0.0006196993635967374
      - 0.0006196237227413803
      - 0.0006196293688844889
      - 0.0006196776521392167
      - 0.0006196358299348503
      - 0.0006196493632160127
      - 0.0006196346075739712
      - 0.0006196691538207233
      - 0.0006196678732521832
      - 0.000619674421614036
    lstm_h256_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0006195550959091634
      config_name: lstm_h256_lr0.001_d0.2_wd1e-05
      converged: true
      final_train_loss: 0.0007193693212078264
      final_val_loss: 0.0006196514004841447
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        ReOdkxwTmj8=
      train_losses:
      - 0.0996223251131596
      - 0.0008701481759392967
      - 0.0007604283164255321
      - 0.0007417786788816253
      - 0.0007244234778530275
      - 0.0007206814577026913
      - 0.0007234420772874728
      - 0.0007185342595524465
      - 0.000713554885199604
      - 0.0007168284403936317
      - 0.0007217300250583018
      - 0.0007166640765111273
      - 0.0007191415303774799
      - 0.0007160161815894147
      - 0.0007127183198463172
      - 0.0007127421995392069
      - 0.000710933740871648
      - 0.0007147531044514229
      - 0.000720869114350838
      - 0.0007137581269489601
      - 0.0007171124210193133
      - 0.0007218437094707042
      - 0.0007207898064128434
      - 0.0007184702990343794
      - 0.0007162686815718189
      - 0.0007124049297999591
      - 0.0007131121480294192
      - 0.0007145620232525592
      - 0.000713392447020548
      - 0.0007193693212078264
      val_losses:
      - 0.0006434628448914737
      - 0.0006488531653303653
      - 0.0006394298980012536
      - 0.0006298809312283993
      - 0.0006243537063710392
      - 0.0006217514455784112
      - 0.0006205214594956487
      - 0.0006200034986250103
      - 0.0006197882466949522
      - 0.000619693601038307
      - 0.0006196695903781801
      - 0.0006196780886966735
      - 0.0006197649636305869
      - 0.0006197071634232998
      - 0.0006195880123414099
      - 0.0006196332687977701
      - 0.0006197344046086073
      - 0.0006197829789016396
      - 0.0006198027695063502
      - 0.0006196371105033904
      - 0.0006195845198817551
      - 0.0006195550959091634
      - 0.0006197166221681982
      - 0.0006197332113515586
      - 0.0006196469767019153
      - 0.000619615224422887
      - 0.0006196569884195924
      - 0.0006196846370585263
      - 0.0006196546019054949
      - 0.0006196514004841447
    lstm_h256_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.0006196247995831072
      config_name: lstm_h256_lr0.001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.0007162852950083712
      final_val_loss: 0.0006196578324306756
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 27
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        g2Ps2LLVmT8=
      train_losses:
      - 0.1775662845854337
      - 0.001159971405286342
      - 0.0008340458492360389
      - 0.0007733201297620932
      - 0.0007517664441062758
      - 0.0007550999871455133
      - 0.000730473436609221
      - 0.0007232438802020624
      - 0.000721075445956861
      - 0.0007292241789400578
      - 0.0007215045091773694
      - 0.0007311452063731849
      - 0.0007356377900578082
      - 0.0007219065591925755
      - 0.0007358631119132042
      - 0.0007310250803129748
      - 0.0007137755892472342
      - 0.0007231352550055211
      - 0.0007279176303806404
      - 0.0007324991408192242
      - 0.000727134033998785
      - 0.0007218748214654624
      - 0.0007158709340728819
      - 0.000719229302679499
      - 0.0007455995946656913
      - 0.0007166442568025863
      - 0.0007162852950083712
      val_losses:
      - 0.0006462543387897313
      - 0.0006524297059513628
      - 0.0006406250759027898
      - 0.0006293918995652348
      - 0.0006234519532881677
      - 0.0006210645660758018
      - 0.0006201702926773578
      - 0.0006199486961122602
      - 0.0006197429029271007
      - 0.000619708705926314
      - 0.000619754457147792
      - 0.0006197495677042753
      - 0.0006196696194820106
      - 0.0006196401081979275
      - 0.000619767204625532
      - 0.0006196699687279761
      - 0.0006196247995831072
      - 0.0006197281763888896
      - 0.0006198740447871387
      - 0.0006196976464707404
      - 0.0006196821632329375
      - 0.0006196279136929661
      - 0.0006196842587087303
      - 0.0006196606555022299
      - 0.0006196328904479742
      - 0.0006196791946422309
      - 0.0006196578324306756
    lstm_h256_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.0006196350150275975
      config_name: lstm_h256_lr0.001_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.000715698697604239
      final_val_loss: 0.000619725527940318
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        2srepKpylz8=
      train_losses:
      - 0.14401995084093264
      - 0.001161698882545655
      - 0.0008031385805225
      - 0.000810224810265936
      - 0.0007314430646753559
      - 0.0007384112201786289
      - 0.0007211076590465382
      - 0.0007350069499807432
      - 0.0007618583040311933
      - 0.0007246690608250598
      - 0.0007265653645542139
      - 0.0007243976918592429
      - 0.0007179192956148958
      - 0.0007316752744372934
      - 0.0007218144310172647
      - 0.0007207864158165952
      - 0.000717655251113077
      - 0.0007339085277635604
      - 0.0007139100149894754
      - 0.000717502111607852
      - 0.0007223676317759479
      - 0.0007208382497386386
      - 0.0007247155978499601
      - 0.0007347882540974145
      - 0.0007214247937857484
      - 0.0007243574703655516
      - 0.0007134253294983258
      - 0.0007144629295604924
      - 0.0007128409876410539
      - 0.000715698697604239
      val_losses:
      - 0.0006515917484648526
      - 0.000657508906442672
      - 0.0006441242876462638
      - 0.0006318589439615607
      - 0.0006255103216972202
      - 0.000622410123469308
      - 0.0006208844715729356
      - 0.000620303035248071
      - 0.000619988568359986
      - 0.0006197336479090154
      - 0.0006197700276970863
      - 0.000619700294919312
      - 0.0006197409529704601
      - 0.0006196761096362025
      - 0.0006196786998771131
      - 0.0006196487811394036
      - 0.0006197700568009168
      - 0.0006197546317707747
      - 0.0006196631875354797
      - 0.0006197254988364875
      - 0.0006197442999109626
      - 0.0006196350150275975
      - 0.0006197557668201625
      - 0.0006197374896146357
      - 0.0006196773611009121
      - 0.0006197749171406031
      - 0.000619651167653501
      - 0.0006196611793711782
      - 0.0006196690665092319
      - 0.000619725527940318
    lstm_h256_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0006195681926328689
      config_name: lstm_h256_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0007125366343340526
      final_val_loss: 0.0006196876347530633
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        OeAtVs97mT8=
      train_losses:
      - 0.17666208398683617
      - 0.002018485130975023
      - 0.0011043588795776789
      - 0.0008760806716357669
      - 0.0008089637364416072
      - 0.000798011688554349
      - 0.0007708576886216179
      - 0.0007682643287504712
      - 0.0007643999415449798
      - 0.0007472182963586723
      - 0.0007836526492610574
      - 0.0007572846467761943
      - 0.0007275537063833326
      - 0.0007808289519743994
      - 0.000756786398900052
      - 0.0007601297838846222
      - 0.0007215680864950021
      - 0.0007225561760909235
      - 0.0007278942406022301
      - 0.0007276073689960564
      - 0.000730639808656027
      - 0.0007286639302037656
      - 0.0007271077677917978
      - 0.0007329431731098642
      - 0.0007245720965632548
      - 0.000752791078411974
      - 0.0007223063633621981
      - 0.0007280753488885239
      - 0.0007237927444900075
      - 0.0007125366343340526
      val_losses:
      - 0.0006490080850198865
      - 0.0006593731231987476
      - 0.0006453146925196052
      - 0.0006313022749964148
      - 0.0006239201466087252
      - 0.0006211437575984746
      - 0.0006201200012583286
      - 0.0006199508788995445
      - 0.0006198149058036506
      - 0.000619671045569703
      - 0.0006196522153913975
      - 0.000619646452832967
      - 0.0006195794849190861
      - 0.0006198162736836821
      - 0.0006197114707902074
      - 0.0006197039620019495
      - 0.0006199845229275525
      - 0.0006196170288603753
      - 0.0006196866743266582
      - 0.000619676400674507
      - 0.0006197056209202856
      - 0.0006196847534738481
      - 0.0006195681926328689
      - 0.0006196065805852413
      - 0.0006195885071065277
      - 0.0006195887690410018
      - 0.0006196356262080371
      - 0.0006196561444085091
      - 0.0006196644098963588
      - 0.0006196876347530633
    lstm_h256_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0006196115573402494
      config_name: lstm_h256_lr0.001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.000722890006727539
      final_val_loss: 0.0006197180482558906
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 13.578125
        estimated_sample_memory_mb: 0.999755859375
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 2342216
      num_epochs_trained: 24
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        CTXFTmuQmD8=
      train_losses:
      - 0.2654283939239879
      - 0.0020019197933531054
      - 0.0011248228305097048
      - 0.0008232508892736708
      - 0.0008385633846046403
      - 0.0007597794271229456
      - 0.0007628624734934419
      - 0.000740876950051946
      - 0.0007611829302428911
      - 0.0007264440985939776
      - 0.0007370216480921954
      - 0.0007264375453814864
      - 0.0007510717841796577
      - 0.000728003607946448
      - 0.0007530682972477128
      - 0.0007282068982021883
      - 0.0007375972588003302
      - 0.0007568188448203728
      - 0.0007594705239171162
      - 0.0007419832691084594
      - 0.0007351321352568144
      - 0.0007215461955638602
      - 0.0007307296715832005
      - 0.000722890006727539
      val_losses:
      - 0.0006538746820297092
      - 0.0006668962305411696
      - 0.0006529432430397719
      - 0.0006375431548804045
      - 0.0006280879024416208
      - 0.0006234736065380275
      - 0.0006213451852090657
      - 0.0006203592056408525
      - 0.0006199710769578815
      - 0.000619862083112821
      - 0.0006198601040523499
      - 0.0006197552138473839
      - 0.0006197059701662511
      - 0.0006196115573402494
      - 0.0006196337344590575
      - 0.0006197734037414193
      - 0.0006197797192726284
      - 0.0006198241608217359
      - 0.0006196960457600653
      - 0.0006198134797159582
      - 0.0006198094051796943
      - 0.0006197556213010103
      - 0.0006196235190145671
      - 0.0006197180482558906
    lstm_h64_lr0.0001_d0.2_wd0.0001:
      best_val_loss: 0.0006296377978287637
      config_name: lstm_h64_lr0.0001_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0016647670903087903
      final_val_loss: 0.0006316968356259167
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 15
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        Z6OyHQhPVb8=
      train_losses:
      - 0.12148335296660662
      - 0.03334152031069001
      - 0.013747088649931053
      - 0.006898147752508521
      - 0.004672511131502688
      - 0.0033227612148039043
      - 0.002655469754245132
      - 0.0026040599429203817
      - 0.002317028479107345
      - 0.0018260539121304948
      - 0.001908384767981867
      - 0.0017659405420999974
      - 0.0017424638111454744
      - 0.0016684622532920912
      - 0.0016647670903087903
      val_losses:
      - 0.017724941950291395
      - 0.0031087218085303903
      - 0.0006836767424829304
      - 0.0006296414358075708
      - 0.0006296377978287637
      - 0.0006305189453996718
      - 0.0006310841999948025
      - 0.0006314400234259665
      - 0.000631719158263877
      - 0.0006318454979918897
      - 0.0006318723317235708
      - 0.0006318544619716704
      - 0.0006318193627521396
      - 0.0006317728839349002
      - 0.0006316968356259167
    lstm_h64_lr0.0001_d0.2_wd1e-05:
      best_val_loss: 0.0006248654099181294
      config_name: lstm_h64_lr0.0001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0012760730751324445
      final_val_loss: 0.0006282952381297946
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        kjaO/853bL8=
      train_losses:
      - 0.14945808922251066
      - 0.01997466788937648
      - 0.007563111488707364
      - 0.004133534695332249
      - 0.002873420133255422
      - 0.002390548266703263
      - 0.0021149297341859588
      - 0.0017391265913223226
      - 0.0016379555066426594
      - 0.001806332467822358
      - 0.0018096852872986346
      - 0.0013828153702585648
      - 0.0012760730751324445
      val_losses:
      - 0.011063091922551394
      - 0.0007985268894117326
      - 0.0006248654099181294
      - 0.0006262657989282161
      - 0.0006271209276746958
      - 0.0006276502972468734
      - 0.0006280167435761541
      - 0.0006282376125454903
      - 0.0006283612456172705
      - 0.0006283798720687628
      - 0.000628407986368984
      - 0.0006283755064941943
      - 0.0006282952381297946
    lstm_h64_lr0.0001_d0.3_wd0.0001:
      best_val_loss: 0.0006287558062467724
      config_name: lstm_h64_lr0.0001_d0.3_wd0.0001
      converged: false
      final_train_loss: 0.003530758355433742
      final_val_loss: 0.0006392566428985447
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NVolosFkZb8=
      train_losses:
      - 0.23664814978837967
      - 0.062473093159496784
      - 0.025692152325063944
      - 0.014912844713156423
      - 0.011344394646584988
      - 0.008177983302933475
      - 0.007164008216932416
      - 0.006245721325588723
      - 0.00530326758356144
      - 0.004700670154610028
      - 0.004291019363639255
      - 0.004376970309143265
      - 0.0036519510904327035
      - 0.003530758355433742
      val_losses:
      - 0.019111979752779007
      - 0.003466992056928575
      - 0.0007745794137008488
      - 0.0006287558062467724
      - 0.0006307913863565773
      - 0.0006326541188172996
      - 0.0006342661508824676
      - 0.0006357140664476901
      - 0.0006369256298057735
      - 0.0006378384423442185
      - 0.0006382516876328737
      - 0.0006386520399246365
      - 0.000638996425550431
      - 0.0006392566428985447
    lstm_h64_lr0.0001_d0.3_wd1e-05:
      best_val_loss: 0.0006282521353568882
      config_name: lstm_h64_lr0.0001_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0037625169303889074
      final_val_loss: 0.0006367229507304728
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        R3ozI+u1cr8=
      train_losses:
      - 0.19427559524774551
      - 0.0580470715649426
      - 0.020536528977875907
      - 0.012853425617019335
      - 0.009267442238827547
      - 0.007315769830408196
      - 0.006529114441946149
      - 0.0053578400596355396
      - 0.005018876807298511
      - 0.00428977688231195
      - 0.0036953092785552144
      - 0.004141653946135193
      - 0.004078509596486886
      - 0.0037625169303889074
      val_losses:
      - 0.02854391187429428
      - 0.0019890129915438592
      - 0.0006318403466138989
      - 0.0006282521353568882
      - 0.0006299926899373531
      - 0.0006314474740065634
      - 0.0006327058072201908
      - 0.0006337283411994576
      - 0.0006346437730826437
      - 0.0006354966608341783
      - 0.0006357904640026391
      - 0.0006360732077155262
      - 0.0006363819702528417
      - 0.0006367229507304728
    lstm_h64_lr0.0001_d0.4_wd0.0001:
      best_val_loss: 0.0006296743231359869
      config_name: lstm_h64_lr0.0001_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.007133532005051772
      final_val_loss: 0.0006453346868511289
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 14
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        YAFvb7YNbD8=
      train_losses:
      - 0.22294909755388895
      - 0.09340208396315575
      - 0.04643624477709333
      - 0.028399603596578043
      - 0.018639225823183853
      - 0.01532630699997147
      - 0.012280921529357633
      - 0.010069403680972755
      - 0.00950944311140726
      - 0.008617668994702399
      - 0.0076202243799343705
      - 0.007422909295807282
      - 0.00792112220854809
      - 0.007133532005051772
      val_losses:
      - 0.015134094748646021
      - 0.00188968371367082
      - 0.0006363331922329962
      - 0.0006296743231359869
      - 0.000632575451163575
      - 0.000635170639725402
      - 0.0006374157674144953
      - 0.0006393574876710773
      - 0.0006410825299099088
      - 0.000642598868580535
      - 0.0006433773378375918
      - 0.0006440406141337007
      - 0.0006447169871535152
      - 0.0006453346868511289
    lstm_h64_lr0.0001_d0.4_wd1e-05:
      best_val_loss: 0.0006269223231356591
      config_name: lstm_h64_lr0.0001_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.008937811013311148
      final_val_loss: 0.0006466668855864555
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 13
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        gncSp3tTU78=
      train_losses:
      - 0.28158720831076306
      - 0.0900339437648654
      - 0.04749057193597158
      - 0.031396158350010715
      - 0.024760685317839187
      - 0.01982406231885155
      - 0.014552417133624354
      - 0.013999877652774254
      - 0.011568401164064804
      - 0.01124532016304632
      - 0.010838683849821487
      - 0.009030033446227511
      - 0.008937811013311148
      val_losses:
      - 0.013838144950568676
      - 0.0009226759138982743
      - 0.0006269223231356591
      - 0.000629851856501773
      - 0.0006329517927952111
      - 0.0006358327809721231
      - 0.0006383728177752346
      - 0.0006407074688468128
      - 0.000642914412310347
      - 0.0006439367425628006
      - 0.0006449615466408432
      - 0.0006458236020989716
      - 0.0006466668855864555
    lstm_h64_lr0.0005_d0.2_wd0.0001:
      best_val_loss: 0.000619684491539374
      config_name: lstm_h64_lr0.0005_d0.2_wd0.0001
      converged: false
      final_train_loss: 0.0007189881483403345
      final_val_loss: 0.0006196965405251831
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 29
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        xk4pGRcqmj8=
      train_losses:
      - 0.019363744494815666
      - 0.001821858643476541
      - 0.0012396332022035494
      - 0.0010924186596336465
      - 0.0008891221708230054
      - 0.0009153701491110647
      - 0.0008920589607441798
      - 0.0008464785108420377
      - 0.0008493735319158683
      - 0.0008367845293832943
      - 0.0007986304893468817
      - 0.0007746996173712736
      - 0.0007699007401242852
      - 0.0007595333736389875
      - 0.0007786962281291684
      - 0.0007582221636160588
      - 0.0007777918654028326
      - 0.0007640819142883023
      - 0.0007770470838295296
      - 0.0007332886937850466
      - 0.0007601737646230807
      - 0.0007446543459082022
      - 0.0007433430097686747
      - 0.0007355483248829842
      - 0.0007381029766596233
      - 0.0007221319635088245
      - 0.0007442723096270735
      - 0.0007451490479676673
      - 0.0007189881483403345
      val_losses:
      - 0.0006344188295770437
      - 0.0006469353393185884
      - 0.0006462568999268115
      - 0.0006402473081834614
      - 0.00063414650503546
      - 0.000629391783149913
      - 0.0006262641400098801
      - 0.000624022854026407
      - 0.0006225687684491277
      - 0.0006218129419721663
      - 0.000621141167357564
      - 0.0006207261176314205
      - 0.0006204632227309048
      - 0.0006201573123689741
      - 0.0006199866475071758
      - 0.0006199359777383506
      - 0.0006198996852617711
      - 0.0006197853945195675
      - 0.000619684491539374
      - 0.0006197597831487656
      - 0.0006198094924911857
      - 0.0006197643815539777
      - 0.0006197107722982764
      - 0.0006197541661094874
      - 0.0006197387119755149
      - 0.000619713042397052
      - 0.0006196887115947902
      - 0.0006197388865984976
      - 0.0006196965405251831
    lstm_h64_lr0.0005_d0.2_wd1e-05:
      best_val_loss: 0.000620200065895915
      config_name: lstm_h64_lr0.0005_d0.2_wd1e-05
      converged: true
      final_train_loss: 0.00077239181943393
      final_val_loss: 0.000620200065895915
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        7VO6r9kvkD8=
      train_losses:
      - 0.04784302394061039
      - 0.003743708696371565
      - 0.0018109513136247795
      - 0.0012014629658854876
      - 0.0011643760323446866
      - 0.0010765240779922654
      - 0.0009511442573663468
      - 0.0009727552387630567
      - 0.0009521134197711945
      - 0.0008973840934534868
      - 0.0008734134656454747
      - 0.0009655068424763158
      - 0.0008852147536041836
      - 0.0009112293240226185
      - 0.0008487030281685293
      - 0.0008226816620056828
      - 0.0009861073097757374
      - 0.0008266804070444778
      - 0.0008035343053052202
      - 0.0008359037407596285
      - 0.0008049899770412594
      - 0.0008243745930182437
      - 0.000827425285630549
      - 0.0008289133693324402
      - 0.0007765756114774073
      - 0.000798680130780364
      - 0.0008094101406944295
      - 0.0008307179377879947
      - 0.0007633607601746917
      - 0.00077239181943393
      val_losses:
      - 0.0006332632037810981
      - 0.0006473778048530221
      - 0.0006524436757899821
      - 0.0006500464805867523
      - 0.0006448008352890611
      - 0.0006393354560714215
      - 0.0006345840229187161
      - 0.0006326144502963871
      - 0.0006308629235718399
      - 0.0006292969919741154
      - 0.0006279529770836234
      - 0.0006268941797316074
      - 0.0006259517977014184
      - 0.0006250902370084077
      - 0.0006243298412300646
      - 0.0006235444161575288
      - 0.0006230387953110039
      - 0.0006225906254258007
      - 0.000622135994490236
      - 0.000621775456238538
      - 0.0006214572349563241
      - 0.0006212121807038784
      - 0.0006209769053384662
      - 0.0006208362465258688
      - 0.0006206404941622168
      - 0.000620505161350593
      - 0.0006204270175658166
      - 0.0006203072553034872
      - 0.0006202491058502346
      - 0.000620200065895915
    lstm_h64_lr0.0005_d0.3_wd0.0001:
      best_val_loss: 0.0006205473619047552
      config_name: lstm_h64_lr0.0005_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0008773346295735488
      final_val_loss: 0.0006205473619047552
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        cjkmUcqhkT8=
      train_losses:
      - 0.08515326368312041
      - 0.006735962349921465
      - 0.0031684962062475583
      - 0.0021709795497978726
      - 0.001880904387993117
      - 0.001607646350748837
      - 0.0014330046348428975
      - 0.001272847002837807
      - 0.0014777934314527859
      - 0.0013095041989193608
      - 0.001139897882239893
      - 0.0012336475144062813
      - 0.0011286745478476707
      - 0.0010781620658235624
      - 0.0011059027019655332
      - 0.0011025725786263745
      - 0.0010782646810791145
      - 0.0010804863171263908
      - 0.0010208351401767384
      - 0.000964846103064095
      - 0.0009659629431553185
      - 0.0009225354830656821
      - 0.0008589942396307985
      - 0.0008544030376166726
      - 0.0008803637222930168
      - 0.0009323720829949403
      - 0.000993452000936183
      - 0.0009361317594690869
      - 0.0008619753934908658
      - 0.0008773346295735488
      val_losses:
      - 0.0006358059181366116
      - 0.0006558276945725083
      - 0.0006644399836659431
      - 0.0006631600554101169
      - 0.0006570422847289592
      - 0.0006500299787148833
      - 0.0006435898831114173
      - 0.0006407387554645538
      - 0.0006382014835253358
      - 0.0006359041144605726
      - 0.0006337118102237582
      - 0.0006318738451227546
      - 0.00063022252288647
      - 0.0006287646538112313
      - 0.0006275050691328943
      - 0.0006264766852837056
      - 0.0006254970794543624
      - 0.0006247730052564293
      - 0.0006241028313525021
      - 0.000623461528448388
      - 0.000622878986177966
      - 0.0006224269745871425
      - 0.0006219830829650164
      - 0.0006215657922439277
      - 0.0006212741718627512
      - 0.0006210744322743267
      - 0.000620904000243172
      - 0.0006207224505487829
      - 0.0006206182588357478
      - 0.0006205473619047552
    lstm_h64_lr0.0005_d0.3_wd1e-05:
      best_val_loss: 0.0006352102791424841
      config_name: lstm_h64_lr0.0005_d0.3_wd1e-05
      converged: false
      final_train_loss: 0.0011835732342054446
      final_val_loss: 0.000635936128674075
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        fWS77qxkfT8=
      train_losses:
      - 0.06935469627690811
      - 0.006768717915595819
      - 0.003378417391407614
      - 0.002353654863933722
      - 0.0018414221121929586
      - 0.0013563631800934672
      - 0.0013620803753534954
      - 0.0013981526377998914
      - 0.001240261835240138
      - 0.0011648772973179196
      - 0.0011835732342054446
      val_losses:
      - 0.0006352102791424841
      - 0.000655642943456769
      - 0.0006645839894190431
      - 0.0006639412022195756
      - 0.0006586879026144743
      - 0.0006521758914459497
      - 0.0006457978452090174
      - 0.0006428964552469552
      - 0.0006404163432307541
      - 0.0006380322156473994
      - 0.000635936128674075
    lstm_h64_lr0.0005_d0.4_wd0.0001:
      best_val_loss: 0.0006701766396872699
      config_name: lstm_h64_lr0.0005_d0.4_wd0.0001
      converged: false
      final_train_loss: 0.002549769609080007
      final_val_loss: 0.0006851441867183894
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        9QiHlCxOiT8=
      train_losses:
      - 0.1070797039816777
      - 0.022441439951459568
      - 0.010802020318806171
      - 0.007617201733713348
      - 0.0056046432970712585
      - 0.004462638151987146
      - 0.003382254818764826
      - 0.0035374214057810605
      - 0.003227315687884887
      - 0.0029390742905282727
      - 0.002549769609080007
      val_losses:
      - 0.0006701766396872699
      - 0.0006740621465723962
      - 0.0007015517330728471
      - 0.000715478410711512
      - 0.0007193638011813164
      - 0.0007151775935199112
      - 0.0007070013380143791
      - 0.0007018005417194217
      - 0.0006959103338886052
      - 0.0006907359347678721
      - 0.0006851441867183894
    lstm_h64_lr0.0005_d0.4_wd1e-05:
      best_val_loss: 0.0006358449172694236
      config_name: lstm_h64_lr0.0005_d0.4_wd1e-05
      converged: false
      final_train_loss: 0.0020924684261747948
      final_val_loss: 0.0006549192767124623
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 11
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        NH6vQ+jtjT8=
      train_losses:
      - 0.09290920958543818
      - 0.016216194063114624
      - 0.008094432530924678
      - 0.005256429935495059
      - 0.0041744213667698205
      - 0.0037654566889007888
      - 0.0030925474711693823
      - 0.0028289540205150843
      - 0.002871063188649714
      - 0.0022777613679257533
      - 0.0020924684261747948
      val_losses:
      - 0.0006358449172694236
      - 0.0006623434310313314
      - 0.0006788169266656041
      - 0.0006849669152870774
      - 0.0006827279576100409
      - 0.0006770566978957504
      - 0.0006702597020193934
      - 0.0006664629036094993
      - 0.00066279066959396
      - 0.0006588507094420493
      - 0.0006549192767124623
    lstm_h64_lr0.001_d0.2_wd0.0001:
      best_val_loss: 0.0006196192698553205
      config_name: lstm_h64_lr0.001_d0.2_wd0.0001
      converged: true
      final_train_loss: 0.0007462590777625641
      final_val_loss: 0.0006196371978148818
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        V84UnAwLmz8=
      train_losses:
      - 0.04382989683654159
      - 0.001602336866199039
      - 0.0009983779358056684
      - 0.0009144086167604352
      - 0.0008191525800308833
      - 0.0007983166384898747
      - 0.000767964952198478
      - 0.0007854430490018179
      - 0.0007927580008981749
      - 0.0007894918720315521
      - 0.0007476977189071476
      - 0.0007760191462390745
      - 0.0007534553539395953
      - 0.000730819379289945
      - 0.0007440202073970189
      - 0.0007422792114084587
      - 0.0007502390265775224
      - 0.0007246123617126917
      - 0.0007505969454844793
      - 0.0007397780767253911
      - 0.0007316942452841128
      - 0.0007564430707134306
      - 0.0007389471284113824
      - 0.0007657868166764578
      - 0.0007400351557104538
      - 0.0007250964893804243
      - 0.0007256669390092915
      - 0.000733301256938527
      - 0.0007371483564687272
      - 0.0007462590777625641
      val_losses:
      - 0.0006627671245951205
      - 0.0006887976778671145
      - 0.0006738044030498713
      - 0.0006511019892059267
      - 0.0006349647301249206
      - 0.0006263784307520837
      - 0.0006226423720363528
      - 0.0006209214625414461
      - 0.0006201897631399333
      - 0.0006198786140885204
      - 0.0006198032351676375
      - 0.0006198229966685176
      - 0.0006197952025104314
      - 0.0006196304748300463
      - 0.0006196470931172371
      - 0.0006197693292051554
      - 0.0006196928443387151
      - 0.000619801547145471
      - 0.0006197246839292347
      - 0.000619644793914631
      - 0.0006196275935508311
      - 0.0006196598696988076
      - 0.0006196820468176156
      - 0.000619665632257238
      - 0.0006196950562298298
      - 0.0006196192698553205
      - 0.0006196658068802208
      - 0.0006196491303853691
      - 0.0006196521571837366
      - 0.0006196371978148818
    lstm_h64_lr0.001_d0.2_wd1e-05:
      best_val_loss: 0.0006195188616402447
      config_name: lstm_h64_lr0.001_d0.2_wd1e-05
      converged: false
      final_train_loss: 0.0007318950423117107
      final_val_loss: 0.0006196039321366698
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 25
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        6N9W4hNEmj8=
      train_losses:
      - 0.052665483012485005
      - 0.002217206968149791
      - 0.0011246293724980205
      - 0.0009310482322083166
      - 0.0008866935725867128
      - 0.0008156728824057305
      - 0.0007863666590613624
      - 0.0007919787021819502
      - 0.0008259124442702159
      - 0.0007737062939365084
      - 0.0007580538852683579
      - 0.000754748568094025
      - 0.000792297170846723
      - 0.0007662498925734932
      - 0.0007563237886643037
      - 0.0007417777766628811
      - 0.0007675424179372688
      - 0.0007866276040052375
      - 0.0007410880061797798
      - 0.0007438686734531075
      - 0.0007448449856989706
      - 0.0007339986914303154
      - 0.0007596817401160175
      - 0.0007426843997867157
      - 0.0007318950423117107
      val_losses:
      - 0.0006625187816098332
      - 0.0006887377239763737
      - 0.0006770823092665523
      - 0.00065498540061526
      - 0.0006384198786690831
      - 0.0006289657612796873
      - 0.0006241266673896462
      - 0.0006217742629814893
      - 0.0006206689868122339
      - 0.0006200060888659209
      - 0.0006198534683790058
      - 0.0006197057664394379
      - 0.0006196170870680362
      - 0.0006196096073836088
      - 0.0006195188616402447
      - 0.0006197552720550448
      - 0.0006197540496941656
      - 0.0006196895847097039
      - 0.0006195584719534963
      - 0.0006196557078510523
      - 0.0006196371687110513
      - 0.0006195878959260881
      - 0.0006195614696480334
      - 0.0006195869937073439
      - 0.0006196039321366698
    lstm_h64_lr0.001_d0.3_wd0.0001:
      best_val_loss: 0.0006196696194820106
      config_name: lstm_h64_lr0.001_d0.3_wd0.0001
      converged: true
      final_train_loss: 0.0007751421944703907
      final_val_loss: 0.0006197717157192528
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        b00TaVNVlT8=
      train_losses:
      - 0.09056561455751459
      - 0.0050042340299114585
      - 0.0021143938987127817
      - 0.0014865362512258191
      - 0.001247787110817929
      - 0.0011916567649071415
      - 0.0010607994239156444
      - 0.0009841799328569323
      - 0.0009881339744121458
      - 0.0010644529344669233
      - 0.0008715111519753312
      - 0.0008526489788588757
      - 0.0008964921968678633
      - 0.000783236202551052
      - 0.0008606359818562245
      - 0.0007966071668003375
      - 0.000778222776716575
      - 0.0007753783089962477
      - 0.0008210006053559482
      - 0.0007837668090360239
      - 0.000764920114306733
      - 0.0007674370378178234
      - 0.0008049105672398582
      - 0.0007518001269393911
      - 0.0007695634267292917
      - 0.000736182254816716
      - 0.0007724738776839027
      - 0.0007707827026024461
      - 0.0007571714086225256
      - 0.0007751421944703907
      val_losses:
      - 0.0006740026292391121
      - 0.0007200247200671583
      - 0.00071481813210994
      - 0.0006885757902637124
      - 0.0006640793872065842
      - 0.0006471611559391022
      - 0.0006364933797158301
      - 0.0006299229571595788
      - 0.0006259868969209492
      - 0.0006236746266949922
      - 0.000622350285993889
      - 0.0006214325258042663
      - 0.0006209464627318084
      - 0.0006205434910953045
      - 0.0006202180229593068
      - 0.0006201431388035417
      - 0.0006200486968737096
      - 0.0006198020710144192
      - 0.0006198154878802598
      - 0.000619922298938036
      - 0.0006198806222528219
      - 0.000619826721958816
      - 0.0006197851034812629
      - 0.0006196696194820106
      - 0.0006197754119057208
      - 0.0006198494520504028
      - 0.0006196820759214461
      - 0.000619838509010151
      - 0.0006197271286509931
      - 0.0006197717157192528
    lstm_h64_lr0.001_d0.3_wd1e-05:
      best_val_loss: 0.0006195423775352538
      config_name: lstm_h64_lr0.001_d0.3_wd1e-05
      converged: true
      final_train_loss: 0.0007714749808656052
      final_val_loss: 0.0006196983449626714
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        2vc+nPXelj8=
      train_losses:
      - 0.037510723360658936
      - 0.0024663743873437247
      - 0.001288513871259056
      - 0.0011440116407660146
      - 0.0010108493297593668
      - 0.0008634790816965202
      - 0.0008745153027120978
      - 0.0009052259168432405
      - 0.0008948279040244719
      - 0.0008458938973490149
      - 0.0007868600660003722
      - 0.000802926219572934
      - 0.0008072832085114593
      - 0.000833118882534715
      - 0.0008126657436757038
      - 0.000769481203557613
      - 0.0007464228935229281
      - 0.0007428418175550178
      - 0.0007624169181023414
      - 0.0007882910431362689
      - 0.000760695727270407
      - 0.0007541731514114266
      - 0.0007882464706199244
      - 0.0007436082426769038
      - 0.000730449731539314
      - 0.0007802265172358602
      - 0.000750921016636615
      - 0.0007443983437648664
      - 0.0007296173280337825
      - 0.0007714749808656052
      val_losses:
      - 0.0006630142452195287
      - 0.0006885081820655614
      - 0.0006718625081703067
      - 0.0006483308388851583
      - 0.0006328477757051587
      - 0.0006253417232073843
      - 0.0006218853814061731
      - 0.0006205891259014606
      - 0.00062030233675614
      - 0.0006201878422871232
      - 0.0006199686322361231
      - 0.0006199001218192279
      - 0.0006198517512530088
      - 0.0006198653136380017
      - 0.0006197203474584967
      - 0.0006196506728883833
      - 0.0006196301546879113
      - 0.0006198763148859143
      - 0.0006199437193572521
      - 0.0006196403701324016
      - 0.0006197128968778998
      - 0.0006196036993060261
      - 0.0006196607719175518
      - 0.0006195423775352538
      - 0.0006196697650011629
      - 0.000619636062765494
      - 0.0006196153699420393
      - 0.0006196608301252127
      - 0.0006197281763888896
      - 0.0006196983449626714
    lstm_h64_lr0.001_d0.4_wd0.0001:
      best_val_loss: 0.0006197718612384051
      config_name: lstm_h64_lr0.001_d0.4_wd0.0001
      converged: true
      final_train_loss: 0.0008370921035141995
      final_val_loss: 0.0006198290502652526
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        dYPefWvAlz8=
      train_losses:
      - 0.1187218305033942
      - 0.008735046450359127
      - 0.004397611405390005
      - 0.0025489356194157153
      - 0.0019345703185535967
      - 0.0017551625011644016
      - 0.0014097205954991903
      - 0.0012776649285418291
      - 0.0013990702621716384
      - 0.0012630913843167946
      - 0.0010844977126301576
      - 0.0010757546939809497
      - 0.0009664612055833762
      - 0.0009032179514179006
      - 0.0009722139075165614
      - 0.0009721853566588834
      - 0.0009165989370861402
      - 0.0009342450600039834
      - 0.0009216944696769739
      - 0.0007894742302596569
      - 0.0008417852950515226
      - 0.0008485921522757659
      - 0.0009608007579421004
      - 0.000832157107652165
      - 0.0008365789205223942
      - 0.000853434787131846
      - 0.0007926803470278779
      - 0.0007948031998239458
      - 0.0007945017277961597
      - 0.0008370921035141995
      val_losses:
      - 0.0006742316763848066
      - 0.000733607477741316
      - 0.000740473362384364
      - 0.0007161133398767561
      - 0.0006874066311866045
      - 0.0006637881451752037
      - 0.0006467367638833821
      - 0.0006358017853926867
      - 0.0006294729537330568
      - 0.0006262332026381046
      - 0.0006240496004465967
      - 0.0006224404496606439
      - 0.0006214948953129351
      - 0.0006208948325365782
      - 0.0006204408127814531
      - 0.0006203697412274778
      - 0.0006204528617672622
      - 0.0006203365919645876
      - 0.0006200310890562832
      - 0.000619901780737564
      - 0.0006197973852977157
      - 0.0006197890907060355
      - 0.000619828439084813
      - 0.0006199956114869565
      - 0.0006197718612384051
      - 0.00061990407994017
      - 0.0006199397321324795
      - 0.0006199470954015851
      - 0.0006199225608725101
      - 0.0006198290502652526
    lstm_h64_lr0.001_d0.4_wd1e-05:
      best_val_loss: 0.0006197609181981534
      config_name: lstm_h64_lr0.001_d0.4_wd1e-05
      converged: true
      final_train_loss: 0.0008126002406546226
      final_val_loss: 0.0006198684277478606
      memory_settings:
        batch_size: 64
        estimated_model_memory_mb: 2.64453125
        estimated_sample_memory_mb: 0.867919921875
        gradient_accumulation_steps: 1
        max_memory_mb: 11264
        use_mixed_precision: true
      model_parameters: 382952
      num_epochs_trained: 30
      prediction_correlation: !!python/object/apply:numpy._core.multiarray.scalar
      - *id001
      - !!binary |
        QvYGvekZlD8=
      train_losses:
      - 0.13812748497972885
      - 0.01137116194392244
      - 0.005309254222083837
      - 0.0031693547304409244
      - 0.0024713069918410233
      - 0.001985781806676338
      - 0.0017416320333722979
      - 0.0016004210871566709
      - 0.0015531534979042287
      - 0.001302761840634048
      - 0.0011276472456908475
      - 0.0011474535228141274
      - 0.0010025949983779963
      - 0.0009934862318914384
      - 0.0009763928974280134
      - 0.0009481995415020114
      - 0.001022038971617197
      - 0.0008998781850095838
      - 0.0009678907954366878
      - 0.0010503929806873202
      - 0.0009810183886050556
      - 0.0008659794451280808
      - 0.0008631645663020512
      - 0.0008302881275691713
      - 0.0007890730194048956
      - 0.0008469676637711624
      - 0.0008765792056995755
      - 0.0008569204364903271
      - 0.0008111051429295912
      - 0.0008126002406546226
      val_losses:
      - 0.0006790802581235766
      - 0.000744330434827134
      - 0.0007545843254774809
      - 0.0007299605058506131
      - 0.0006967421795707196
      - 0.0006698172073811293
      - 0.0006507267244160175
      - 0.0006388869078364223
      - 0.000631659961072728
      - 0.0006273655162658542
      - 0.0006244765245355666
      - 0.0006227682169992477
      - 0.0006218088383320719
      - 0.0006212007428985089
      - 0.0006207499536685646
      - 0.0006202672084327787
      - 0.0006200753268785775
      - 0.0006201161595527083
      - 0.0006200149946380407
      - 0.0006199553899932653
      - 0.0006200516072567552
      - 0.0006200032075867057
      - 0.0006197609181981534
      - 0.0006197715119924396
      - 0.0006199197378009558
      - 0.0006199599010869861
      - 0.0006197933689691126
      - 0.0006199158378876746
      - 0.0006199925555847585
      - 0.0006198684277478606
  best_config: lstm_h64_lr0.001_d0.2_wd1e-05
  best_val_loss: 0.0006195188616402447
  hyperparameter_analysis:
    dropout:
      '0.2':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          VVVVrWxaRD8=
        num_configs: 18
      '0.3':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          5DiOU/JiRD8=
        num_configs: 18
      '0.4':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          chzHYaqDRD8=
        num_configs: 18
    hidden_size:
      '128':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          q6qqCl1gRD8=
        num_configs: 18
      '256':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          juM4tj5URD8=
        num_configs: 18
      '64':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          chzHoW2MRD8=
        num_configs: 18
    learning_rate:
      '0.0001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          x3Ecbx9uRD8=
        num_configs: 36
      '0.0005':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          HMdxLBx9RD8=
        num_configs: 18
      '0.001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          HMdxDMRNRD8=
        num_configs: 18
    weight_decay:
      '0.0001':
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          x3Ecbx9uRD8=
        num_configs: 36
      1e-05:
        avg_val_loss: !!python/object/apply:numpy._core.multiarray.scalar
        - *id001
        - !!binary |
          hfYSGiBqRD8=
        num_configs: 27
  num_configs_tested: 54
  successful_configs: 54
training_period:
  end: '2024-12-30'
  start: '2010-04-01'
training_summary:
  input_features: 1200
  num_assets: 200
  sequence_length: 60
  total_sequences: 3712
  training_sequences: 754
  validation_sequences: 251
